"use strict";(self.webpackChunkdocs_site=self.webpackChunkdocs_site||[]).push([[6396],{4145:e=>{e.exports=JSON.parse('{"permalink":"/fullstack-dev/blog/rag-ecommerce-recommendations-part2-data-pipeline-embeddings","editUrl":"https://github.com/tamnk74/fullstack-dev/tree/main/docs-site/blog/2025-10-08-rag-ecommerce-recommendations-part2-data-pipeline-embeddings.md","source":"@site/blog/2025-10-08-rag-ecommerce-recommendations-part2-data-pipeline-embeddings.md","title":"Building Intelligent E-commerce Recommendations with RAG: Part 2 - Data Pipeline and Vector Embeddings","description":"Welcome back to our comprehensive series on building RAG-powered e-commerce recommendation systems! In Part 1, we explored the architecture and core concepts. Now, we\'ll get hands-on with the foundation of any RAG system: building robust data pipelines and generating high-quality vector embeddings that capture the semantic essence of products and user behavior.","date":"2025-10-08T00:00:00.000Z","tags":[{"inline":false,"label":"RAG","permalink":"/fullstack-dev/blog/tags/rag","description":"Retrieval-Augmented Generation"},{"inline":false,"label":"E-commerce","permalink":"/fullstack-dev/blog/tags/ecommerce","description":"E-commerce development and strategies"},{"inline":false,"label":"Recommendations","permalink":"/fullstack-dev/blog/tags/recommendations","description":"Recommendation systems and algorithms"},{"inline":false,"label":"Vector Embeddings","permalink":"/fullstack-dev/blog/tags/vector-embeddings","description":"Vector embeddings and semantic search"},{"inline":false,"label":"Data Pipeline","permalink":"/fullstack-dev/blog/tags/data-pipeline","description":"Data processing and pipeline architectures"},{"inline":false,"label":"Machine Learning","permalink":"/fullstack-dev/blog/tags/machine-learning","description":"Machine learning and AI applications"},{"inline":false,"label":"Natural Language Processing","permalink":"/fullstack-dev/blog/tags/nlp","description":"Natural language processing techniques"},{"inline":false,"label":"Python","permalink":"/fullstack-dev/blog/tags/python","description":"Python programming language"}],"readingTime":20.6,"hasTruncateMarker":true,"authors":[{"name":"Tam Nguyen","title":"Full Stack Developer, Next.js for Production Creator","url":"https://github.com/tamnk74","page":{"permalink":"/fullstack-dev/blog/authors/tam"},"socials":{"github":"https://github.com/tamnk74"},"imageURL":"https://github.com/tamnk74.png","key":"tam"}],"frontMatter":{"slug":"rag-ecommerce-recommendations-part2-data-pipeline-embeddings","title":"Building Intelligent E-commerce Recommendations with RAG: Part 2 - Data Pipeline and Vector Embeddings","authors":["tam"],"tags":["rag","ecommerce","recommendations","vector-embeddings","data-pipeline","machine-learning","nlp","python"],"date":"2025-10-08T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Building Intelligent E-commerce Recommendations with RAG: Part 1 - Introduction and Architecture Overview","permalink":"/fullstack-dev/blog/rag-ecommerce-recommendations-part1-introduction"},"nextItem":{"title":"Building Intelligent E-commerce Recommendations with RAG: Part 3 - Retrieval System Implementation","permalink":"/fullstack-dev/blog/rag-ecommerce-recommendations-part3-retrieval-system"}}')},5741:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var i=t(9729);const r={},s=i.createContext(r);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(s.Provider,{value:n},e.children)}},9482:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>c});var i=t(4145),r=t(5813),s=t(5741);const a={slug:"rag-ecommerce-recommendations-part2-data-pipeline-embeddings",title:"Building Intelligent E-commerce Recommendations with RAG: Part 2 - Data Pipeline and Vector Embeddings",authors:["tam"],tags:["rag","ecommerce","recommendations","vector-embeddings","data-pipeline","machine-learning","nlp","python"],date:new Date("2025-10-08T00:00:00.000Z")},o="Building Intelligent E-commerce Recommendations with RAG: Part 2 - Data Pipeline and Vector Embeddings",d={authorsImageUrls:[void 0]},c=[{value:"Data Pipeline Architecture Overview",id:"data-pipeline-architecture-overview",level:2},{value:"Setting Up the Development Environment",id:"setting-up-the-development-environment",level:2},{value:"Product Data Schema and Processing",id:"product-data-schema-and-processing",level:2},{value:"Product Schema Definition",id:"product-schema-definition",level:3},{value:"Product Data Processor",id:"product-data-processor",level:3},{value:"User Behavior Data Processing",id:"user-behavior-data-processing",level:2},{value:"User Models and Schemas",id:"user-models-and-schemas",level:3},{value:"User Behavior Processor",id:"user-behavior-processor",level:3},{value:"Vector Embedding Generation",id:"vector-embedding-generation",level:2},{value:"Embedding Quality Validation",id:"embedding-quality-validation",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"<strong>What We&#39;ve Accomplished</strong>",id:"what-weve-accomplished",level:3},{value:"<strong>Before Part 3</strong>",id:"before-part-3",level:3}];function l(e){const n={br:"br",code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Welcome back to our comprehensive series on building RAG-powered e-commerce recommendation systems! In Part 1, we explored the architecture and core concepts. Now, we'll get hands-on with the foundation of any RAG system: building robust data pipelines and generating high-quality vector embeddings that capture the semantic essence of products and user behavior."}),"\n",(0,r.jsx)(n.p,{children:"Vector embeddings are the secret sauce that enables our recommendation system to understand the relationships between products, users, and their preferences at a semantic level. Let's dive into building a production-ready data processing pipeline that transforms raw e-commerce data into meaningful vector representations."}),"\n",(0,r.jsx)(n.h2,{id:"data-pipeline-architecture-overview",children:"Data Pipeline Architecture Overview"}),"\n",(0,r.jsx)(n.p,{children:"Our data pipeline follows the Extract, Transform, Load (ETL) pattern, specifically designed for vector embedding generation:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Raw Data      \u2502    \u2502  Processing     \u2502    \u2502   Vector        \u2502\n\u2502   Sources       \u2502\u2500\u2500\u2500\u25b6\u2502  Pipeline       \u2502\u2500\u2500\u2500\u25b6\u2502   Storage       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502                      \u2502                      \u2502                 \u2502\n\u251c\u2500 Product Catalog     \u251c\u2500 Data Cleaning       \u251c\u2500 Product Vectors\u2502\n\u251c\u2500 User Profiles       \u251c\u2500 Feature Extraction  \u251c\u2500 User Vectors   \u2502\n\u251c\u2500 Interaction Logs    \u251c\u2500 Embedding Generation\u251c\u2500 Query Vectors  \u2502\n\u251c\u2500 Reviews/Ratings     \u251c\u2500 Quality Validation  \u251c\u2500 Metadata Index \u2502\n\u2514\u2500 Search Queries      \u2514\u2500 Batch Processing    \u2514\u2500 Search Index   \u2502\n"})}),"\n",(0,r.jsx)(n.h2,{id:"setting-up-the-development-environment",children:"Setting Up the Development Environment"}),"\n",(0,r.jsx)(n.p,{children:"First, let's set up our development environment with the necessary dependencies:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Create virtual environment\npython -m venv rag-ecommerce\nsource rag-ecommerce/bin/activate  # On Windows: rag-ecommerce\\Scripts\\activate\n\n# Install core dependencies\npip install -r requirements.txt\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# requirements.txt\n# Data processing\npandas==2.1.0\nnumpy==1.24.3\nsqlalchemy==2.0.20\npsycopg2-binary==2.9.7\n\n# Machine learning and embeddings\nsentence-transformers==2.2.2\ntransformers==4.33.2\ntorch==2.0.1\nscikit-learn==1.3.0\n\n# Vector databases\nqdrant-client==1.4.0\npinecone-client==2.2.4\nweaviate-client==3.23.1\n\n# API and LLM integration\nopenai==0.28.0\nanthropic==0.3.11\nlangchain==0.0.292\n\n# Data validation and monitoring\npydantic==2.3.0\ngreat-expectations==0.17.15\nmlflow==2.6.0\n\n# Utilities\npython-dotenv==1.0.0\ntqdm==4.66.1\nredis==4.6.0\ncelery==5.3.1\n"})}),"\n",(0,r.jsx)(n.h2,{id:"product-data-schema-and-processing",children:"Product Data Schema and Processing"}),"\n",(0,r.jsx)(n.h3,{id:"product-schema-definition",children:"Product Schema Definition"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# models/product.py\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nfrom enum import Enum\n\nclass ProductCategory(str, Enum):\n    ELECTRONICS = "electronics"\n    CLOTHING = "clothing"\n    HOME_GARDEN = "home_garden"\n    BOOKS = "books"\n    SPORTS = "sports"\n    BEAUTY = "beauty"\n    AUTOMOTIVE = "automotive"\n\nclass Product(BaseModel):\n    id: str = Field(..., description="Unique product identifier")\n    title: str = Field(..., description="Product title")\n    description: str = Field(..., description="Detailed product description")\n    category: ProductCategory = Field(..., description="Primary product category")\n    subcategory: Optional[str] = Field(None, description="Product subcategory")\n    brand: Optional[str] = Field(None, description="Product brand")\n    price: float = Field(..., gt=0, description="Product price")\n    currency: str = Field(default="USD", description="Price currency")\n    \n    # Product attributes\n    attributes: Dict[str, Any] = Field(default_factory=dict, description="Product attributes")\n    tags: List[str] = Field(default_factory=list, description="Product tags")\n    images: List[str] = Field(default_factory=list, description="Product image URLs")\n    \n    # Metrics\n    rating: Optional[float] = Field(None, ge=0, le=5, description="Average rating")\n    review_count: int = Field(default=0, ge=0, description="Number of reviews")\n    popularity_score: float = Field(default=0.0, description="Popularity metric")\n    \n    # Inventory and availability\n    stock_quantity: int = Field(default=0, ge=0, description="Available stock")\n    is_available: bool = Field(default=True, description="Product availability")\n    \n    # Temporal data\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n    updated_at: datetime = Field(default_factory=datetime.utcnow)\n    \n    # SEO and discoverability\n    keywords: List[str] = Field(default_factory=list, description="SEO keywords")\n    search_terms: List[str] = Field(default_factory=list, description="Common search terms")\n\n    class Config:\n        json_encoders = {\n            datetime: lambda v: v.isoformat()\n        }\n\nclass ProductReview(BaseModel):\n    id: str\n    product_id: str\n    user_id: str\n    rating: float = Field(..., ge=1, le=5)\n    title: Optional[str] = None\n    content: str\n    helpful_votes: int = Field(default=0, ge=0)\n    verified_purchase: bool = Field(default=False)\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"product-data-processor",children:"Product Data Processor"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# processors/product_processor.py\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Tuple\nimport re\nfrom collections import Counter\nimport logging\nfrom models.product import Product, ProductReview\n\nlogger = logging.getLogger(__name__)\n\nclass ProductDataProcessor:\n    def __init__(self):\n        self.stop_words = self._load_stop_words()\n        self.category_keywords = self._load_category_keywords()\n    \n    def clean_product_data(self, products_df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Clean and standardize product data\"\"\"\n        logger.info(f\"Cleaning {len(products_df)} products\")\n        \n        # Remove duplicates\n        products_df = products_df.drop_duplicates(subset=['id'])\n        \n        # Standardize text fields\n        products_df['title'] = products_df['title'].apply(self._clean_text)\n        products_df['description'] = products_df['description'].apply(self._clean_text)\n        \n        # Normalize prices\n        products_df['price'] = pd.to_numeric(products_df['price'], errors='coerce')\n        products_df = products_df.dropna(subset=['price'])\n        \n        # Fill missing values\n        products_df['description'] = products_df['description'].fillna('')\n        products_df['brand'] = products_df['brand'].fillna('Unknown')\n        products_df['rating'] = products_df['rating'].fillna(0.0)\n        products_df['review_count'] = products_df['review_count'].fillna(0)\n        \n        # Calculate derived features\n        products_df['title_length'] = products_df['title'].str.len()\n        products_df['description_length'] = products_df['description'].str.len()\n        products_df['price_category'] = pd.cut(\n            products_df['price'], \n            bins=[0, 25, 100, 500, np.inf], \n            labels=['budget', 'mid-range', 'premium', 'luxury']\n        )\n        \n        logger.info(f\"Cleaned data: {len(products_df)} products remaining\")\n        return products_df\n    \n    def extract_product_features(self, product: Product) -> Dict[str, Any]:\n        \"\"\"Extract meaningful features from product data\"\"\"\n        features = {\n            # Basic features\n            'title': product.title,\n            'description': product.description,\n            'category': product.category.value,\n            'subcategory': product.subcategory or '',\n            'brand': product.brand or '',\n            \n            # Numeric features\n            'price': product.price,\n            'rating': product.rating or 0.0,\n            'review_count': product.review_count,\n            'popularity_score': product.popularity_score,\n            \n            # Text processing\n            'combined_text': self._create_combined_text(product),\n            'keywords': product.keywords,\n            'search_terms': product.search_terms,\n            \n            # Categorical features\n            'price_range': self._categorize_price(product.price),\n            'rating_tier': self._categorize_rating(product.rating or 0.0),\n            'popularity_tier': self._categorize_popularity(product.popularity_score),\n            \n            # Availability features\n            'is_available': product.is_available,\n            'stock_level': self._categorize_stock(product.stock_quantity),\n        }\n        \n        # Add attribute-based features\n        features.update(self._process_attributes(product.attributes))\n        \n        return features\n    \n    def _clean_text(self, text: str) -> str:\n        \"\"\"Clean and normalize text data\"\"\"\n        if pd.isna(text) or not isinstance(text, str):\n            return \"\"\n        \n        # Convert to lowercase\n        text = text.lower()\n        \n        # Remove special characters but keep alphanumeric and spaces\n        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n        \n        # Remove extra whitespaces\n        text = re.sub(r'\\s+', ' ', text).strip()\n        \n        return text\n    \n    def _create_combined_text(self, product: Product) -> str:\n        \"\"\"Create combined text for embedding generation\"\"\"\n        components = [\n            product.title,\n            product.description,\n            product.brand or '',\n            product.category.value,\n            product.subcategory or '',\n            ' '.join(product.keywords),\n            ' '.join(product.search_terms)\n        ]\n        \n        # Filter out empty components\n        components = [comp for comp in components if comp and comp.strip()]\n        \n        return ' '.join(components)\n    \n    def _categorize_price(self, price: float) -> str:\n        \"\"\"Categorize price into ranges\"\"\"\n        if price < 25:\n            return 'budget'\n        elif price < 100:\n            return 'mid-range'\n        elif price < 500:\n            return 'premium'\n        else:\n            return 'luxury'\n    \n    def _categorize_rating(self, rating: float) -> str:\n        \"\"\"Categorize rating into tiers\"\"\"\n        if rating >= 4.5:\n            return 'excellent'\n        elif rating >= 4.0:\n            return 'very_good'\n        elif rating >= 3.5:\n            return 'good'\n        elif rating >= 3.0:\n            return 'average'\n        else:\n            return 'poor'\n    \n    def _categorize_popularity(self, score: float) -> str:\n        \"\"\"Categorize popularity score\"\"\"\n        if score >= 0.8:\n            return 'viral'\n        elif score >= 0.6:\n            return 'popular'\n        elif score >= 0.4:\n            return 'moderate'\n        elif score >= 0.2:\n            return 'low'\n        else:\n            return 'niche'\n    \n    def _categorize_stock(self, quantity: int) -> str:\n        \"\"\"Categorize stock levels\"\"\"\n        if quantity == 0:\n            return 'out_of_stock'\n        elif quantity < 10:\n            return 'low_stock'\n        elif quantity < 100:\n            return 'medium_stock'\n        else:\n            return 'high_stock'\n    \n    def _process_attributes(self, attributes: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"Process product attributes into features\"\"\"\n        processed = {}\n        \n        for key, value in attributes.items():\n            # Normalize key\n            clean_key = f\"attr_{re.sub(r'[^a-z0-9]', '_', key.lower())}\"\n            \n            # Convert value to string representation\n            if isinstance(value, (list, tuple)):\n                clean_value = ', '.join(str(v) for v in value)\n            else:\n                clean_value = str(value)\n            \n            processed[clean_key] = self._clean_text(clean_value)\n        \n        return processed\n    \n    def _load_stop_words(self) -> set:\n        \"\"\"Load stop words for text processing\"\"\"\n        # Basic stop words - in production, use NLTK or spaCy\n        return {\n            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n            'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during'\n        }\n    \n    def _load_category_keywords(self) -> Dict[str, List[str]]:\n        \"\"\"Load category-specific keywords\"\"\"\n        return {\n            'electronics': ['smartphone', 'laptop', 'tablet', 'tv', 'camera', 'headphones'],\n            'clothing': ['shirt', 'pants', 'dress', 'shoes', 'jacket', 'accessories'],\n            'home_garden': ['furniture', 'decor', 'kitchen', 'garden', 'tools', 'appliances'],\n            'books': ['novel', 'textbook', 'magazine', 'ebook', 'audiobook', 'reference'],\n            'sports': ['equipment', 'apparel', 'fitness', 'outdoor', 'team', 'individual'],\n            'beauty': ['skincare', 'makeup', 'hair', 'fragrance', 'tools', 'accessories']\n        }\n"})}),"\n",(0,r.jsx)(n.h2,{id:"user-behavior-data-processing",children:"User Behavior Data Processing"}),"\n",(0,r.jsx)(n.h3,{id:"user-models-and-schemas",children:"User Models and Schemas"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# models/user.py\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nfrom enum import Enum\n\nclass UserDemographic(BaseModel):\n    age_range: Optional[str] = Field(None, description="Age range (e.g., \'25-34\')")\n    gender: Optional[str] = Field(None, description="User gender")\n    location: Optional[str] = Field(None, description="User location")\n    income_range: Optional[str] = Field(None, description="Income bracket")\n\nclass UserPreferences(BaseModel):\n    preferred_categories: List[str] = Field(default_factory=list)\n    preferred_brands: List[str] = Field(default_factory=list)\n    price_sensitivity: float = Field(default=0.5, ge=0, le=1)\n    quality_preference: float = Field(default=0.5, ge=0, le=1)\n    style_preferences: List[str] = Field(default_factory=list)\n\nclass InteractionType(str, Enum):\n    VIEW = "view"\n    CLICK = "click"\n    ADD_TO_CART = "add_to_cart"\n    PURCHASE = "purchase"\n    REVIEW = "review"\n    SEARCH = "search"\n    WISHLIST = "wishlist"\n\nclass UserInteraction(BaseModel):\n    id: str\n    user_id: str\n    product_id: Optional[str] = None\n    interaction_type: InteractionType\n    timestamp: datetime = Field(default_factory=datetime.utcnow)\n    session_id: str\n    \n    # Context data\n    page_url: Optional[str] = None\n    referrer: Optional[str] = None\n    device_type: Optional[str] = None\n    search_query: Optional[str] = None\n    \n    # Interaction specifics\n    duration: Optional[float] = None  # Time spent (seconds)\n    scroll_depth: Optional[float] = None  # Page scroll percentage\n    click_position: Optional[Dict[str, float]] = None  # x, y coordinates\n    \n    # Purchase specific\n    quantity: Optional[int] = None\n    price_paid: Optional[float] = None\n    discount_applied: Optional[float] = None\n\nclass User(BaseModel):\n    id: str\n    email: Optional[str] = None\n    demographic: Optional[UserDemographic] = None\n    preferences: UserPreferences = Field(default_factory=UserPreferences)\n    \n    # Behavioral metrics\n    total_purchases: int = Field(default=0, ge=0)\n    total_spent: float = Field(default=0.0, ge=0)\n    avg_order_value: float = Field(default=0.0, ge=0)\n    lifetime_value: float = Field(default=0.0, ge=0)\n    \n    # Engagement metrics\n    session_count: int = Field(default=0, ge=0)\n    page_views: int = Field(default=0, ge=0)\n    avg_session_duration: float = Field(default=0.0, ge=0)\n    \n    # Temporal data\n    first_seen: datetime = Field(default_factory=datetime.utcnow)\n    last_seen: datetime = Field(default_factory=datetime.utcnow)\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n    updated_at: datetime = Field(default_factory=datetime.utcnow)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"user-behavior-processor",children:"User Behavior Processor"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# processors/user_processor.py\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Tuple\nfrom datetime import datetime, timedelta\nfrom collections import defaultdict, Counter\nimport logging\nfrom models.user import User, UserInteraction, InteractionType\n\nlogger = logging.getLogger(__name__)\n\nclass UserBehaviorProcessor:\n    def __init__(self):\n        self.interaction_weights = {\n            InteractionType.VIEW: 1.0,\n            InteractionType.CLICK: 2.0,\n            InteractionType.ADD_TO_CART: 5.0,\n            InteractionType.PURCHASE: 10.0,\n            InteractionType.REVIEW: 8.0,\n            InteractionType.SEARCH: 3.0,\n            InteractionType.WISHLIST: 4.0,\n        }\n    \n    def process_user_interactions(self, interactions_df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Process and enrich user interaction data\"\"\"\n        logger.info(f\"Processing {len(interactions_df)} user interactions\")\n        \n        # Convert timestamp to datetime\n        interactions_df['timestamp'] = pd.to_datetime(interactions_df['timestamp'])\n        \n        # Add temporal features\n        interactions_df['hour'] = interactions_df['timestamp'].dt.hour\n        interactions_df['day_of_week'] = interactions_df['timestamp'].dt.dayofweek\n        interactions_df['is_weekend'] = interactions_df['day_of_week'].isin([5, 6])\n        \n        # Calculate interaction weights\n        interactions_df['interaction_weight'] = interactions_df['interaction_type'].map(\n            lambda x: self.interaction_weights.get(InteractionType(x), 1.0)\n        )\n        \n        # Add session-based features\n        interactions_df = self._add_session_features(interactions_df)\n        \n        return interactions_df\n    \n    def create_user_profiles(self, users_df: pd.DataFrame, interactions_df: pd.DataFrame) -> Dict[str, Dict]:\n        \"\"\"Create comprehensive user profiles from interaction data\"\"\"\n        user_profiles = {}\n        \n        for user_id in users_df['id'].unique():\n            user_interactions = interactions_df[interactions_df['user_id'] == user_id]\n            \n            profile = {\n                'user_id': user_id,\n                'behavioral_features': self._extract_behavioral_features(user_interactions),\n                'preference_features': self._extract_preference_features(user_interactions),\n                'temporal_features': self._extract_temporal_features(user_interactions),\n                'engagement_features': self._extract_engagement_features(user_interactions),\n            }\n            \n            user_profiles[user_id] = profile\n        \n        logger.info(f\"Created profiles for {len(user_profiles)} users\")\n        return user_profiles\n    \n    def _add_session_features(self, interactions_df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Add session-based features to interactions\"\"\"\n        # Sort by user and timestamp\n        interactions_df = interactions_df.sort_values(['user_id', 'timestamp'])\n        \n        # Calculate time since previous interaction\n        interactions_df['time_since_prev'] = interactions_df.groupby('user_id')['timestamp'].diff()\n        interactions_df['time_since_prev_seconds'] = interactions_df['time_since_prev'].dt.total_seconds()\n        \n        # Identify session boundaries (gap > 30 minutes)\n        session_boundary = interactions_df['time_since_prev_seconds'] > 1800  # 30 minutes\n        interactions_df['session_number'] = session_boundary.groupby(interactions_df['user_id']).cumsum()\n        \n        return interactions_df\n    \n    def _extract_behavioral_features(self, user_interactions: pd.DataFrame) -> Dict[str, float]:\n        \"\"\"Extract behavioral patterns from user interactions\"\"\"\n        if len(user_interactions) == 0:\n            return self._get_default_behavioral_features()\n        \n        features = {}\n        \n        # Interaction type distribution\n        interaction_counts = user_interactions['interaction_type'].value_counts()\n        total_interactions = len(user_interactions)\n        \n        for interaction_type in InteractionType:\n            ratio = interaction_counts.get(interaction_type.value, 0) / total_interactions\n            features[f'{interaction_type.value}_ratio'] = ratio\n        \n        # Purchase behavior\n        purchases = user_interactions[user_interactions['interaction_type'] == InteractionType.PURCHASE.value]\n        features['purchase_frequency'] = len(purchases)\n        features['avg_time_to_purchase'] = self._calculate_avg_time_to_purchase(user_interactions)\n        \n        # Browsing behavior\n        views = user_interactions[user_interactions['interaction_type'] == InteractionType.VIEW.value]\n        features['avg_session_length'] = user_interactions.groupby('session_id').size().mean()\n        features['avg_products_per_session'] = user_interactions.groupby('session_id')['product_id'].nunique().mean()\n        \n        # Engagement depth\n        features['bounce_rate'] = self._calculate_bounce_rate(user_interactions)\n        features['repeat_visit_rate'] = self._calculate_repeat_visit_rate(user_interactions)\n        \n        return features\n    \n    def _extract_preference_features(self, user_interactions: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"Extract user preferences from interaction patterns\"\"\"\n        if len(user_interactions) == 0:\n            return {}\n        \n        # This would typically join with product data to get categories, brands, etc.\n        # For now, we'll create placeholder logic\n        features = {\n            'preferred_categories': [],\n            'preferred_brands': [],\n            'price_sensitivity': 0.5,\n            'quality_preference': 0.5,\n        }\n        \n        # Calculate preferences based on weighted interactions\n        weighted_interactions = user_interactions.copy()\n        weighted_interactions['weight'] = weighted_interactions['interaction_type'].map(\n            lambda x: self.interaction_weights.get(InteractionType(x), 1.0)\n        )\n        \n        # In a real implementation, you'd join with product data here\n        # features['preferred_categories'] = self._get_top_categories(weighted_interactions)\n        # features['preferred_brands'] = self._get_top_brands(weighted_interactions)\n        \n        return features\n    \n    def _extract_temporal_features(self, user_interactions: pd.DataFrame) -> Dict[str, float]:\n        \"\"\"Extract temporal behavior patterns\"\"\"\n        if len(user_interactions) == 0:\n            return {}\n        \n        features = {}\n        \n        # Time-based patterns\n        hour_distribution = user_interactions['hour'].value_counts(normalize=True)\n        features['peak_hour'] = hour_distribution.idxmax()\n        features['activity_variance'] = hour_distribution.var()\n        \n        # Day-of-week patterns\n        dow_distribution = user_interactions['day_of_week'].value_counts(normalize=True)\n        features['weekend_activity_ratio'] = user_interactions['is_weekend'].mean()\n        \n        # Recency and frequency\n        features['days_since_last_interaction'] = (\n            datetime.utcnow() - user_interactions['timestamp'].max()\n        ).days\n        \n        features['interaction_frequency'] = len(user_interactions) / max(\n            (user_interactions['timestamp'].max() - user_interactions['timestamp'].min()).days, 1\n        )\n        \n        return features\n    \n    def _extract_engagement_features(self, user_interactions: pd.DataFrame) -> Dict[str, float]:\n        \"\"\"Extract user engagement metrics\"\"\"\n        if len(user_interactions) == 0:\n            return {}\n        \n        features = {}\n        \n        # Session-based metrics\n        session_stats = user_interactions.groupby('session_id').agg({\n            'timestamp': ['min', 'max'],\n            'product_id': 'nunique',\n            'interaction_type': 'count'\n        }).reset_index()\n        \n        # Calculate session durations\n        session_stats['duration'] = (\n            session_stats[('timestamp', 'max')] - session_stats[('timestamp', 'min')]\n        ).dt.total_seconds()\n        \n        features['avg_session_duration'] = session_stats['duration'].mean()\n        features['total_sessions'] = len(session_stats)\n        features['avg_actions_per_session'] = session_stats[('interaction_type', 'count')].mean()\n        \n        # Conversion metrics\n        sessions_with_purchase = user_interactions[\n            user_interactions['interaction_type'] == InteractionType.PURCHASE.value\n        ]['session_id'].nunique()\n        \n        features['session_conversion_rate'] = sessions_with_purchase / len(session_stats) if len(session_stats) > 0 else 0\n        \n        return features\n    \n    def _calculate_avg_time_to_purchase(self, user_interactions: pd.DataFrame) -> float:\n        \"\"\"Calculate average time from first interaction to purchase\"\"\"\n        purchases = user_interactions[user_interactions['interaction_type'] == InteractionType.PURCHASE.value]\n        \n        if len(purchases) == 0:\n            return 0.0\n        \n        times_to_purchase = []\n        \n        for _, purchase in purchases.iterrows():\n            session_interactions = user_interactions[\n                user_interactions['session_id'] == purchase['session_id']\n            ].sort_values('timestamp')\n            \n            first_interaction = session_interactions.iloc[0]['timestamp']\n            time_to_purchase = (purchase['timestamp'] - first_interaction).total_seconds()\n            times_to_purchase.append(time_to_purchase)\n        \n        return np.mean(times_to_purchase) if times_to_purchase else 0.0\n    \n    def _calculate_bounce_rate(self, user_interactions: pd.DataFrame) -> float:\n        \"\"\"Calculate bounce rate (single-interaction sessions)\"\"\"\n        session_sizes = user_interactions.groupby('session_id').size()\n        single_interaction_sessions = (session_sizes == 1).sum()\n        return single_interaction_sessions / len(session_sizes) if len(session_sizes) > 0 else 0\n    \n    def _calculate_repeat_visit_rate(self, user_interactions: pd.DataFrame) -> float:\n        \"\"\"Calculate repeat visit rate\"\"\"\n        unique_days = user_interactions['timestamp'].dt.date.nunique()\n        total_days = (\n            user_interactions['timestamp'].max() - user_interactions['timestamp'].min()\n        ).days + 1\n        return unique_days / total_days if total_days > 0 else 0\n    \n    def _get_default_behavioral_features(self) -> Dict[str, float]:\n        \"\"\"Return default behavioral features for users with no interactions\"\"\"\n        features = {}\n        for interaction_type in InteractionType:\n            features[f'{interaction_type.value}_ratio'] = 0.0\n        \n        features.update({\n            'purchase_frequency': 0.0,\n            'avg_time_to_purchase': 0.0,\n            'avg_session_length': 0.0,\n            'avg_products_per_session': 0.0,\n            'bounce_rate': 1.0,\n            'repeat_visit_rate': 0.0,\n        })\n        \n        return features\n"})}),"\n",(0,r.jsx)(n.h2,{id:"vector-embedding-generation",children:"Vector Embedding Generation"}),"\n",(0,r.jsx)(n.p,{children:"Now let's implement the core embedding generation system:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# embeddings/embedding_generator.py\nimport numpy as np\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModel\nfrom typing import List, Dict, Any, Optional, Union\nimport logging\nfrom abc import ABC, abstractmethod\nimport asyncio\nimport aiohttp\nimport openai\nfrom tqdm import tqdm\n\nlogger = logging.getLogger(__name__)\n\nclass BaseEmbeddingGenerator(ABC):\n    """Abstract base class for embedding generators"""\n    \n    @abstractmethod\n    async def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n        pass\n    \n    @abstractmethod\n    def get_embedding_dimension(self) -> int:\n        pass\n\nclass SentenceTransformerEmbedding(BaseEmbeddingGenerator):\n    """Local sentence transformer embedding generator"""\n    \n    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):\n        self.model_name = model_name\n        self.model = SentenceTransformer(model_name)\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        self.model.to(self.device)\n        \n        logger.info(f"Loaded SentenceTransformer model: {model_name}")\n    \n    async def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n        """Generate embeddings for a list of texts"""\n        if not texts:\n            return np.array([])\n        \n        # Process in batches to manage memory\n        batch_size = 32\n        all_embeddings = []\n        \n        for i in tqdm(range(0, len(texts), batch_size), desc="Generating embeddings"):\n            batch = texts[i:i + batch_size]\n            embeddings = self.model.encode(\n                batch,\n                convert_to_numpy=True,\n                show_progress_bar=False,\n                batch_size=batch_size\n            )\n            all_embeddings.append(embeddings)\n        \n        return np.vstack(all_embeddings)\n    \n    def get_embedding_dimension(self) -> int:\n        return self.model.get_sentence_embedding_dimension()\n\nclass OpenAIEmbedding(BaseEmbeddingGenerator):\n    """OpenAI API embedding generator"""\n    \n    def __init__(self, model_name: str = "text-embedding-ada-002", api_key: Optional[str] = None):\n        self.model_name = model_name\n        openai.api_key = api_key or os.getenv("OPENAI_API_KEY")\n        self.dimensions = {\n            "text-embedding-ada-002": 1536,\n            "text-embedding-3-small": 1536,\n            "text-embedding-3-large": 3072,\n        }\n        \n        logger.info(f"Initialized OpenAI embedding model: {model_name}")\n    \n    async def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n        """Generate embeddings using OpenAI API"""\n        if not texts:\n            return np.array([])\n        \n        # OpenAI API has rate limits, so we batch requests\n        batch_size = 100  # OpenAI recommends up to 2048 texts per request\n        all_embeddings = []\n        \n        for i in tqdm(range(0, len(texts), batch_size), desc="Generating OpenAI embeddings"):\n            batch = texts[i:i + batch_size]\n            \n            try:\n                response = await openai.Embedding.acreate(\n                    model=self.model_name,\n                    input=batch\n                )\n                \n                batch_embeddings = [data.embedding for data in response.data]\n                all_embeddings.extend(batch_embeddings)\n                \n                # Rate limiting\n                await asyncio.sleep(0.1)\n                \n            except Exception as e:\n                logger.error(f"Error generating embeddings for batch {i}: {e}")\n                # Fill with zeros for failed batches\n                zero_embedding = [0.0] * self.get_embedding_dimension()\n                all_embeddings.extend([zero_embedding] * len(batch))\n        \n        return np.array(all_embeddings)\n    \n    def get_embedding_dimension(self) -> int:\n        return self.dimensions.get(self.model_name, 1536)\n\nclass ProductEmbeddingGenerator:\n    """Specialized embedding generator for products"""\n    \n    def __init__(self, embedding_generator: BaseEmbeddingGenerator):\n        self.embedding_generator = embedding_generator\n        self.dimension = embedding_generator.get_embedding_dimension()\n    \n    async def generate_product_embeddings(self, products: List[Dict[str, Any]]) -> Dict[str, np.ndarray]:\n        """Generate embeddings for products"""\n        logger.info(f"Generating embeddings for {len(products)} products")\n        \n        # Prepare texts for embedding\n        product_texts = []\n        product_ids = []\n        \n        for product in products:\n            text = self._create_product_text(product)\n            product_texts.append(text)\n            product_ids.append(product[\'id\'])\n        \n        # Generate embeddings\n        embeddings = await self.embedding_generator.generate_embeddings(product_texts)\n        \n        # Create mapping\n        product_embeddings = {}\n        for product_id, embedding in zip(product_ids, embeddings):\n            product_embeddings[product_id] = embedding\n        \n        logger.info(f"Generated embeddings for {len(product_embeddings)} products")\n        return product_embeddings\n    \n    def _create_product_text(self, product: Dict[str, Any]) -> str:\n        """Create text representation of product for embedding"""\n        components = []\n        \n        # Title (highest weight)\n        if product.get(\'title\'):\n            components.append(f"Title: {product[\'title\']}")\n        \n        # Category and subcategory\n        if product.get(\'category\'):\n            components.append(f"Category: {product[\'category\']}")\n        if product.get(\'subcategory\'):\n            components.append(f"Subcategory: {product[\'subcategory\']}")\n        \n        # Brand\n        if product.get(\'brand\'):\n            components.append(f"Brand: {product[\'brand\']}")\n        \n        # Description\n        if product.get(\'description\'):\n            # Truncate very long descriptions\n            description = product[\'description\'][:500]\n            components.append(f"Description: {description}")\n        \n        # Price range\n        if product.get(\'price_range\'):\n            components.append(f"Price range: {product[\'price_range\']}")\n        \n        # Keywords and tags\n        if product.get(\'keywords\'):\n            components.append(f"Keywords: {\', \'.join(product[\'keywords\'])}")\n        \n        # Attributes\n        for key, value in product.get(\'attributes\', {}).items():\n            if isinstance(value, (str, int, float)):\n                components.append(f"{key}: {value}")\n        \n        return \' \'.join(components)\n\nclass UserEmbeddingGenerator:\n    """Specialized embedding generator for users"""\n    \n    def __init__(self, embedding_generator: BaseEmbeddingGenerator):\n        self.embedding_generator = embedding_generator\n        self.dimension = embedding_generator.get_embedding_dimension()\n    \n    async def generate_user_embeddings(self, user_profiles: Dict[str, Dict[str, Any]]) -> Dict[str, np.ndarray]:\n        """Generate embeddings for user profiles"""\n        logger.info(f"Generating embeddings for {len(user_profiles)} users")\n        \n        # Prepare texts for embedding\n        user_texts = []\n        user_ids = []\n        \n        for user_id, profile in user_profiles.items():\n            text = self._create_user_text(profile)\n            user_texts.append(text)\n            user_ids.append(user_id)\n        \n        # Generate embeddings\n        embeddings = await self.embedding_generator.generate_embeddings(user_texts)\n        \n        # Create mapping\n        user_embeddings = {}\n        for user_id, embedding in zip(user_ids, embeddings):\n            user_embeddings[user_id] = embedding\n        \n        logger.info(f"Generated embeddings for {len(user_embeddings)} users")\n        return user_embeddings\n    \n    def _create_user_text(self, profile: Dict[str, Any]) -> str:\n        """Create text representation of user profile for embedding"""\n        components = []\n        \n        # Behavioral features\n        behavioral = profile.get(\'behavioral_features\', {})\n        if behavioral:\n            high_interactions = [k.replace(\'_ratio\', \'\') for k, v in behavioral.items() \n                               if k.endswith(\'_ratio\') and v > 0.1]\n            if high_interactions:\n                components.append(f"Frequently: {\', \'.join(high_interactions)}")\n        \n        # Preferences\n        preferences = profile.get(\'preference_features\', {})\n        if preferences.get(\'preferred_categories\'):\n            components.append(f"Prefers categories: {\', \'.join(preferences[\'preferred_categories\'])}")\n        if preferences.get(\'preferred_brands\'):\n            components.append(f"Prefers brands: {\', \'.join(preferences[\'preferred_brands\'])}")\n        \n        # Engagement level\n        engagement = profile.get(\'engagement_features\', {})\n        if engagement.get(\'session_conversion_rate\', 0) > 0.1:\n            components.append("High converter")\n        elif engagement.get(\'avg_session_duration\', 0) > 300:  # 5 minutes\n            components.append("Engaged browser")\n        \n        # Temporal patterns\n        temporal = profile.get(\'temporal_features\', {})\n        if temporal.get(\'weekend_activity_ratio\', 0) > 0.6:\n            components.append("Weekend shopper")\n        \n        if not components:\n            components.append("New user")\n        \n        return \' \'.join(components)\n\nclass QueryEmbeddingGenerator:\n    """Generate embeddings for search queries"""\n    \n    def __init__(self, embedding_generator: BaseEmbeddingGenerator):\n        self.embedding_generator = embedding_generator\n    \n    async def generate_query_embedding(self, query: str, context: Optional[Dict[str, Any]] = None) -> np.ndarray:\n        """Generate embedding for a search query with optional context"""\n        # Enhance query with context\n        enhanced_query = self._enhance_query_with_context(query, context)\n        \n        # Generate embedding\n        embeddings = await self.embedding_generator.generate_embeddings([enhanced_query])\n        return embeddings[0]\n    \n    def _enhance_query_with_context(self, query: str, context: Optional[Dict[str, Any]] = None) -> str:\n        """Enhance query with contextual information"""\n        if not context:\n            return query\n        \n        enhancements = [query]\n        \n        # Add user preferences\n        if context.get(\'preferred_categories\'):\n            enhancements.append(f"Categories: {\', \'.join(context[\'preferred_categories\'])}")\n        \n        # Add price context\n        if context.get(\'price_range\'):\n            enhancements.append(f"Price range: {context[\'price_range\']}")\n        \n        # Add temporal context\n        if context.get(\'season\'):\n            enhancements.append(f"Season: {context[\'season\']}")\n        \n        return \' \'.join(enhancements)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"embedding-quality-validation",children:"Embedding Quality Validation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# validation/embedding_validator.py\nimport numpy as np\nfrom typing import List, Dict, Tuple, Any\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass EmbeddingQualityValidator:\n    \"\"\"Validate the quality of generated embeddings\"\"\"\n    \n    def __init__(self):\n        self.similarity_threshold = 0.7\n        self.diversity_threshold = 0.3\n    \n    def validate_product_embeddings(\n        self, \n        product_embeddings: Dict[str, np.ndarray],\n        products: List[Dict[str, Any]]\n    ) -> Dict[str, Any]:\n        \"\"\"Comprehensive validation of product embeddings\"\"\"\n        logger.info(\"Validating product embeddings...\")\n        \n        results = {\n            'total_products': len(product_embeddings),\n            'embedding_dimension': len(next(iter(product_embeddings.values()))),\n            'quality_metrics': {},\n            'recommendations': []\n        }\n        \n        # Create product lookup\n        product_lookup = {p['id']: p for p in products}\n        \n        # Test semantic similarity\n        similarity_results = self._test_semantic_similarity(product_embeddings, product_lookup)\n        results['quality_metrics']['semantic_similarity'] = similarity_results\n        \n        # Test category clustering\n        clustering_results = self._test_category_clustering(product_embeddings, product_lookup)\n        results['quality_metrics']['category_clustering'] = clustering_results\n        \n        # Test embedding diversity\n        diversity_results = self._test_embedding_diversity(product_embeddings)\n        results['quality_metrics']['diversity'] = diversity_results\n        \n        # Generate recommendations\n        results['recommendations'] = self._generate_quality_recommendations(results['quality_metrics'])\n        \n        logger.info(f\"Validation complete. Overall quality score: {self._calculate_overall_score(results['quality_metrics']):.3f}\")\n        \n        return results\n    \n    def _test_semantic_similarity(\n        self, \n        embeddings: Dict[str, np.ndarray], \n        products: Dict[str, Dict[str, Any]]\n    ) -> Dict[str, float]:\n        \"\"\"Test if semantically similar products have similar embeddings\"\"\"\n        # Sample products for testing (to avoid O(n\xb2) computation)\n        sample_size = min(100, len(embeddings))\n        sampled_ids = list(embeddings.keys())[:sample_size]\n        \n        similarity_scores = []\n        \n        for i, product_id_1 in enumerate(sampled_ids):\n            for product_id_2 in sampled_ids[i+1:]:\n                # Calculate embedding similarity\n                emb_sim = cosine_similarity(\n                    embeddings[product_id_1].reshape(1, -1),\n                    embeddings[product_id_2].reshape(1, -1)\n                )[0, 0]\n                \n                # Calculate semantic similarity (based on category, brand, etc.)\n                semantic_sim = self._calculate_semantic_similarity(\n                    products[product_id_1], \n                    products[product_id_2]\n                )\n                \n                similarity_scores.append({\n                    'embedding_similarity': emb_sim,\n                    'semantic_similarity': semantic_sim\n                })\n        \n        # Calculate correlation\n        emb_sims = [s['embedding_similarity'] for s in similarity_scores]\n        sem_sims = [s['semantic_similarity'] for s in similarity_scores]\n        \n        correlation = np.corrcoef(emb_sims, sem_sims)[0, 1]\n        \n        return {\n            'correlation': correlation,\n            'avg_embedding_similarity': np.mean(emb_sims),\n            'avg_semantic_similarity': np.mean(sem_sims),\n            'sample_size': len(similarity_scores)\n        }\n    \n    def _test_category_clustering(\n        self, \n        embeddings: Dict[str, np.ndarray], \n        products: Dict[str, Dict[str, Any]]\n    ) -> Dict[str, Any]:\n        \"\"\"Test if products from same category cluster together\"\"\"\n        # Group by category\n        category_embeddings = defaultdict(list)\n        category_products = defaultdict(list)\n        \n        for product_id, embedding in embeddings.items():\n            if product_id in products:\n                category = products[product_id].get('category', 'unknown')\n                category_embeddings[category].append(embedding)\n                category_products[category].append(product_id)\n        \n        # Calculate intra-category similarity\n        intra_similarities = {}\n        for category, embs in category_embeddings.items():\n            if len(embs) > 1:\n                emb_matrix = np.array(embs)\n                sim_matrix = cosine_similarity(emb_matrix)\n                # Average similarity excluding diagonal\n                mask = ~np.eye(sim_matrix.shape[0], dtype=bool)\n                avg_similarity = sim_matrix[mask].mean()\n                intra_similarities[category] = avg_similarity\n        \n        # Calculate inter-category similarity\n        categories = list(category_embeddings.keys())\n        inter_similarities = []\n        \n        for i, cat1 in enumerate(categories):\n            for cat2 in categories[i+1:]:\n                if len(category_embeddings[cat1]) > 0 and len(category_embeddings[cat2]) > 0:\n                    # Calculate average similarity between categories\n                    emb1 = np.mean(category_embeddings[cat1], axis=0)\n                    emb2 = np.mean(category_embeddings[cat2], axis=0)\n                    sim = cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0, 0]\n                    inter_similarities.append(sim)\n        \n        return {\n            'intra_category_similarity': np.mean(list(intra_similarities.values())),\n            'inter_category_similarity': np.mean(inter_similarities),\n            'separation_score': np.mean(list(intra_similarities.values())) - np.mean(inter_similarities),\n            'categories_analyzed': len(category_embeddings)\n        }\n    \n    def _test_embedding_diversity(self, embeddings: Dict[str, np.ndarray]) -> Dict[str, float]:\n        \"\"\"Test diversity of embeddings to avoid mode collapse\"\"\"\n        embedding_matrix = np.array(list(embeddings.values()))\n        \n        # Calculate pairwise similarities\n        similarity_matrix = cosine_similarity(embedding_matrix)\n        \n        # Remove diagonal (self-similarity)\n        mask = ~np.eye(similarity_matrix.shape[0], dtype=bool)\n        similarities = similarity_matrix[mask]\n        \n        # Calculate diversity metrics\n        avg_similarity = similarities.mean()\n        similarity_std = similarities.std()\n        \n        # PCA to check dimensionality usage\n        pca = PCA()\n        pca.fit(embedding_matrix)\n        explained_variance_ratio = pca.explained_variance_ratio_\n        \n        # Calculate effective dimensionality (95% variance)\n        cumsum_variance = np.cumsum(explained_variance_ratio)\n        effective_dims = np.argmax(cumsum_variance >= 0.95) + 1\n        \n        return {\n            'average_similarity': avg_similarity,\n            'similarity_std': similarity_std,\n            'effective_dimensions': effective_dims,\n            'total_dimensions': embedding_matrix.shape[1],\n            'dimension_usage_ratio': effective_dims / embedding_matrix.shape[1]\n        }\n    \n    def _calculate_semantic_similarity(self, product1: Dict[str, Any], product2: Dict[str, Any]) -> float:\n        \"\"\"Calculate semantic similarity between two products\"\"\"\n        similarity = 0.0\n        \n        # Category similarity (highest weight)\n        if product1.get('category') == product2.get('category'):\n            similarity += 0.4\n            \n            # Subcategory similarity\n            if product1.get('subcategory') == product2.get('subcategory'):\n                similarity += 0.2\n        \n        # Brand similarity\n        if product1.get('brand') == product2.get('brand'):\n            similarity += 0.2\n        \n        # Price range similarity\n        if product1.get('price_range') == product2.get('price_range'):\n            similarity += 0.1\n        \n        # Title similarity (simple word overlap)\n        title1_words = set(product1.get('title', '').lower().split())\n        title2_words = set(product2.get('title', '').lower().split())\n        if title1_words and title2_words:\n            word_overlap = len(title1_words & title2_words) / len(title1_words | title2_words)\n            similarity += 0.1 * word_overlap\n        \n        return similarity\n    \n    def _calculate_overall_score(self, metrics: Dict[str, Any]) -> float:\n        \"\"\"Calculate overall quality score\"\"\"\n        scores = []\n        \n        # Semantic similarity correlation (should be positive)\n        if 'semantic_similarity' in metrics:\n            correlation = metrics['semantic_similarity'].get('correlation', 0)\n            scores.append(max(0, correlation))\n        \n        # Category clustering separation\n        if 'category_clustering' in metrics:\n            separation = metrics['category_clustering'].get('separation_score', 0)\n            scores.append(max(0, min(1, separation)))\n        \n        # Diversity (should not be too high or too low)\n        if 'diversity' in metrics:\n            avg_sim = metrics['diversity'].get('average_similarity', 0.5)\n            # Optimal similarity around 0.3-0.5\n            diversity_score = 1 - abs(avg_sim - 0.4) / 0.4\n            scores.append(max(0, diversity_score))\n        \n        return np.mean(scores) if scores else 0.0\n    \n    def _generate_quality_recommendations(self, metrics: Dict[str, Any]) -> List[str]:\n        \"\"\"Generate recommendations based on quality metrics\"\"\"\n        recommendations = []\n        \n        # Check semantic similarity\n        if 'semantic_similarity' in metrics:\n            correlation = metrics['semantic_similarity'].get('correlation', 0)\n            if correlation < 0.3:\n                recommendations.append(\n                    \"Low semantic similarity correlation. Consider improving text preprocessing \"\n                    \"or using a more powerful embedding model.\"\n                )\n        \n        # Check category clustering\n        if 'category_clustering' in metrics:\n            separation = metrics['category_clustering'].get('separation_score', 0)\n            if separation < 0.1:\n                recommendations.append(\n                    \"Poor category separation. Consider adding category information to \"\n                    \"product text or using category-specific embeddings.\"\n                )\n        \n        # Check diversity\n        if 'diversity' in metrics:\n            avg_sim = metrics['diversity'].get('average_similarity', 0.5)\n            if avg_sim > 0.8:\n                recommendations.append(\n                    \"Embeddings are too similar (mode collapse). Consider using a different \"\n                    \"embedding model or improving text diversity.\"\n                )\n            elif avg_sim < 0.1:\n                recommendations.append(\n                    \"Embeddings are too diverse. Consider improving text preprocessing \"\n                    \"or using a more consistent embedding approach.\"\n                )\n        \n        if not recommendations:\n            recommendations.append(\"Embedding quality looks good!\")\n        \n        return recommendations\n"})}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.p,{children:["In ",(0,r.jsx)(n.strong,{children:"Part 3"})," of this series, we'll implement the retrieval system using vector databases and build sophisticated search capabilities. We'll cover:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vector Database Setup"}),": Detailed implementation with Qdrant, Pinecone, and Weaviate"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hybrid Search Strategies"}),": Combining semantic and keyword search"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance Optimization"}),": Indexing, sharding, and caching strategies"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time Updates"}),": Handling dynamic product catalogs"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"what-weve-accomplished",children:(0,r.jsx)(n.strong,{children:"What We've Accomplished"})}),"\n",(0,r.jsx)(n.p,{children:"In this post, we've built the foundation for our RAG recommendation system:"}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Robust Data Processing Pipeline"}),": Handles product catalogs and user behavior data\n\u2705 ",(0,r.jsx)(n.strong,{children:"Quality Vector Embeddings"}),": Generates semantic representations for products and users",(0,r.jsx)(n.br,{}),"\n","\u2705 ",(0,r.jsx)(n.strong,{children:"Comprehensive Validation"}),": Ensures embedding quality and semantic coherence\n\u2705 ",(0,r.jsx)(n.strong,{children:"Scalable Architecture"}),": Designed for production workloads"]}),"\n",(0,r.jsx)(n.h3,{id:"before-part-3",children:(0,r.jsx)(n.strong,{children:"Before Part 3"})}),"\n",(0,r.jsx)(n.p,{children:"Make sure you have:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Generated embeddings for your product catalog"}),"\n",(0,r.jsx)(n.li,{children:"Set up user behavior tracking"}),"\n",(0,r.jsx)(n.li,{children:"Validated embedding quality"}),"\n",(0,r.jsx)(n.li,{children:"Chosen your vector database provider"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"The data pipeline and embeddings are the foundation of intelligent recommendations. Next, we'll build the retrieval engine that makes it all come together!"})})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}}}]);