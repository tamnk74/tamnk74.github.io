"use strict";(self.webpackChunkdocs_site=self.webpackChunkdocs_site||[]).push([[9586],{4064:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"architecture-practices/architecture-execution/resource-scaling","title":"Resource Scaling Guide","description":"This guide provides comprehensive resource scaling strategies and automated scaling mechanisms for NestJS microservices and Next.js microfrontend applications, covering horizontal scaling, vertical scaling, auto-scaling policies, and cloud-native scaling solutions.","source":"@site/docs/architecture-practices/architecture-execution/resource-scaling.md","sourceDirName":"architecture-practices/architecture-execution","slug":"/architecture-practices/architecture-execution/resource-scaling","permalink":"/fullstack-dev/docs/architecture-practices/architecture-execution/resource-scaling","draft":false,"unlisted":false,"editUrl":"https://github.com/tamnk74/fullstack-dev/tree/main/docs-site/docs/architecture-practices/architecture-execution/resource-scaling.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Security Scanning Guide","permalink":"/fullstack-dev/docs/architecture-practices/architecture-execution/security-scanning"},"next":{"title":"Skills Assessment for Architecture Teams","permalink":"/fullstack-dev/docs/architecture-practices/architecture-management/skills-assessment"}}');var r=t(5813),s=t(7814);const a={},c="Resource Scaling Guide",o={},l=[{value:"Overview",id:"overview",level:2},{value:"Scaling Architecture",id:"scaling-architecture",level:2},{value:"Complete Scaling Framework",id:"complete-scaling-framework",level:3},{value:"Horizontal Pod Autoscaler (HPA) Configuration",id:"horizontal-pod-autoscaler-hpa-configuration",level:2},{value:"Kubernetes HPA Setup",id:"kubernetes-hpa-setup",level:3},{value:"Auto-Scaling Service Implementation",id:"auto-scaling-service-implementation",level:2},{value:"NestJS Auto-Scaling Service",id:"nestjs-auto-scaling-service",level:3},{value:"Vertical Pod Autoscaler (VPA) Configuration",id:"vertical-pod-autoscaler-vpa-configuration",level:2},{value:"VPA Setup for Resource Right-Sizing",id:"vpa-setup-for-resource-right-sizing",level:3},{value:"Database Auto-Scaling",id:"database-auto-scaling",level:2},{value:"Database Connection Pool Scaling",id:"database-connection-pool-scaling",level:3},{value:"Frontend Auto-Scaling",id:"frontend-auto-scaling",level:2},{value:"Next.js CDN and Edge Scaling",id:"nextjs-cdn-and-edge-scaling",level:3},{value:"Cost Optimization and Monitoring",id:"cost-optimization-and-monitoring",level:2},{value:"Scaling Cost Analysis Dashboard",id:"scaling-cost-analysis-dashboard",level:3},{value:"Related Documentation",id:"related-documentation",level:2}];function u(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"resource-scaling-guide",children:"Resource Scaling Guide"})}),"\n",(0,r.jsx)(n.p,{children:"This guide provides comprehensive resource scaling strategies and automated scaling mechanisms for NestJS microservices and Next.js microfrontend applications, covering horizontal scaling, vertical scaling, auto-scaling policies, and cloud-native scaling solutions."}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"Resource scaling ensures applications can handle varying load levels efficiently while maintaining performance and cost-effectiveness. This guide covers both reactive scaling (responding to current load) and predictive scaling (anticipating future load) across infrastructure, application, and database layers."}),"\n",(0,r.jsx)(n.h2,{id:"scaling-architecture",children:"Scaling Architecture"}),"\n",(0,r.jsx)(n.h3,{id:"complete-scaling-framework",children:"Complete Scaling Framework"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[Load Monitoring] --\x3e B[Metrics Collection]\n    B --\x3e C[Threshold Analysis]\n    C --\x3e D[Scaling Decision Engine]\n    \n    D --\x3e E[Horizontal Scaling]\n    D --\x3e F[Vertical Scaling]\n    D --\x3e G[Database Scaling]\n    D --\x3e H[CDN Scaling]\n    \n    E --\x3e I[Pod/Container Scaling]\n    E --\x3e J[Load Balancer Updates]\n    \n    F --\x3e K[CPU/Memory Scaling]\n    F --\x3e L[Instance Upgrades]\n    \n    G --\x3e M[Read Replica Scaling]\n    G --\x3e N[Connection Pool Scaling]\n    G --\x3e O[Sharding Decisions]\n    \n    H --\x3e P[Cache Layer Scaling]\n    H --\x3e Q[Edge Location Scaling]\n    \n    R[Scaling Policies] --\x3e S[CPU-based Scaling]\n    R --\x3e T[Memory-based Scaling]\n    R --\x3e U[Custom Metric Scaling]\n    R --\x3e V[Predictive Scaling]\n    \n    W[Cost Optimization] --\x3e X[Right-sizing Analysis]\n    W --\x3e Y[Reserved Capacity]\n    W --\x3e Z[Spot Instance Usage]\n    \n    I --\x3e AA[Health Verification]\n    K --\x3e AA\n    M --\x3e AA\n    P --\x3e AA\n    \n    AA --\x3e BB[Performance Validation]\n    BB --\x3e CC[Cost Analysis]\n    CC --\x3e DD[Scaling Complete]\n"})}),"\n",(0,r.jsx)(n.h2,{id:"horizontal-pod-autoscaler-hpa-configuration",children:"Horizontal Pod Autoscaler (HPA) Configuration"}),"\n",(0,r.jsx)(n.h3,{id:"kubernetes-hpa-setup",children:"Kubernetes HPA Setup"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# kubernetes/hpa/api-gateway-hpa.yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: api-gateway-hpa\n  namespace: microservices\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: api-gateway\n  minReplicas: 2\n  maxReplicas: 20\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n  - type: Pods\n    pods:\n      metric:\n        name: requests_per_second\n      target:\n        type: AverageValue\n        averageValue: "100"\n  - type: Object\n    object:\n      metric:\n        name: queue_depth\n      describedObject:\n        apiVersion: v1\n        kind: Service\n        name: redis-queue\n      target:\n        type: Value\n        value: "30"\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n      - type: Percent\n        value: 50\n        periodSeconds: 60\n      - type: Pods\n        value: 2\n        periodSeconds: 60\n      selectPolicy: Min\n    scaleUp:\n      stabilizationWindowSeconds: 60\n      policies:\n      - type: Percent\n        value: 100\n        periodSeconds: 60\n      - type: Pods\n        value: 4\n        periodSeconds: 60\n      selectPolicy: Max\n\n---\n# kubernetes/hpa/user-service-hpa.yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: user-service-hpa\n  namespace: microservices\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: user-service\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 65\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 75\n  - type: External\n    external:\n      metric:\n        name: pubsub.googleapis.com|subscription|num_undelivered_messages\n        selector:\n          matchLabels:\n            resource.labels.subscription_id: user-events-subscription\n      target:\n        type: AverageValue\n        averageValue: "10"\n\n---\n# kubernetes/hpa/frontend-hpa.yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: frontend-hpa\n  namespace: frontend\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nextjs-frontend\n  minReplicas: 2\n  maxReplicas: 15\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 60\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Pods\n    pods:\n      metric:\n        name: http_requests_per_second\n      target:\n        type: AverageValue\n        averageValue: "50"\n'})}),"\n",(0,r.jsx)(n.h2,{id:"auto-scaling-service-implementation",children:"Auto-Scaling Service Implementation"}),"\n",(0,r.jsx)(n.h3,{id:"nestjs-auto-scaling-service",children:"NestJS Auto-Scaling Service"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// src/scaling/auto-scaling.service.ts\nimport { Injectable, Logger } from '@nestjs/common';\nimport { EventEmitter2 } from '@nestjs/event-emitter';\nimport { Cron, CronExpression } from '@nestjs/schedule';\nimport { HttpService } from '@nestjs/axios';\nimport { firstValueFrom } from 'rxjs';\n\nexport interface ScalingMetrics {\n  timestamp: Date;\n  cpu: {\n    current: number;\n    average: number;\n    target: number;\n  };\n  memory: {\n    current: number;\n    average: number;\n    target: number;\n  };\n  requests: {\n    perSecond: number;\n    average: number;\n    target: number;\n  };\n  responseTime: {\n    current: number;\n    p95: number;\n    target: number;\n  };\n  queueDepth: {\n    current: number;\n    average: number;\n    target: number;\n  };\n  errorRate: {\n    current: number;\n    average: number;\n    threshold: number;\n  };\n  connections: {\n    active: number;\n    average: number;\n    limit: number;\n  };\n}\n\nexport interface ScalingDecision {\n  action: 'scale-up' | 'scale-down' | 'no-action';\n  type: 'horizontal' | 'vertical';\n  currentInstances: number;\n  targetInstances: number;\n  reason: string;\n  confidence: number;\n  estimatedDuration: number;\n  costImpact: number;\n}\n\nexport interface ScalingPolicy {\n  name: string;\n  service: string;\n  enabled: boolean;\n  minInstances: number;\n  maxInstances: number;\n  targetCpuUtilization: number;\n  targetMemoryUtilization: number;\n  targetRequestsPerSecond: number;\n  targetResponseTime: number;\n  scaleUpCooldown: number;\n  scaleDownCooldown: number;\n  scaleUpPolicy: {\n    threshold: number;\n    periodSeconds: number;\n    step: number;\n  };\n  scaleDownPolicy: {\n    threshold: number;\n    periodSeconds: number;\n    step: number;\n  };\n  predictiveScaling: {\n    enabled: boolean;\n    lookAheadMinutes: number;\n    confidenceThreshold: number;\n  };\n}\n\n@Injectable()\nexport class AutoScalingService {\n  private readonly logger = new Logger(AutoScalingService.name);\n  private scalingPolicies: Map<string, ScalingPolicy> = new Map();\n  private lastScalingAction: Map<string, Date> = new Map();\n  private metricsHistory: Map<string, ScalingMetrics[]> = new Map();\n\n  constructor(\n    private readonly eventEmitter: EventEmitter2,\n    private readonly httpService: HttpService\n  ) {\n    this.initializeScalingPolicies();\n  }\n\n  private initializeScalingPolicies(): void {\n    const policies: ScalingPolicy[] = [\n      {\n        name: 'api-gateway-scaling',\n        service: 'api-gateway',\n        enabled: true,\n        minInstances: 2,\n        maxInstances: 20,\n        targetCpuUtilization: 70,\n        targetMemoryUtilization: 80,\n        targetRequestsPerSecond: 100,\n        targetResponseTime: 200,\n        scaleUpCooldown: 300, // 5 minutes\n        scaleDownCooldown: 600, // 10 minutes\n        scaleUpPolicy: {\n          threshold: 80,\n          periodSeconds: 60,\n          step: 2,\n        },\n        scaleDownPolicy: {\n          threshold: 50,\n          periodSeconds: 300,\n          step: 1,\n        },\n        predictiveScaling: {\n          enabled: true,\n          lookAheadMinutes: 30,\n          confidenceThreshold: 0.8,\n        },\n      },\n      {\n        name: 'user-service-scaling',\n        service: 'user-service',\n        enabled: true,\n        minInstances: 1,\n        maxInstances: 10,\n        targetCpuUtilization: 65,\n        targetMemoryUtilization: 75,\n        targetRequestsPerSecond: 50,\n        targetResponseTime: 150,\n        scaleUpCooldown: 180,\n        scaleDownCooldown: 480,\n        scaleUpPolicy: {\n          threshold: 75,\n          periodSeconds: 60,\n          step: 1,\n        },\n        scaleDownPolicy: {\n          threshold: 40,\n          periodSeconds: 300,\n          step: 1,\n        },\n        predictiveScaling: {\n          enabled: false,\n          lookAheadMinutes: 15,\n          confidenceThreshold: 0.7,\n        },\n      },\n      {\n        name: 'product-service-scaling',\n        service: 'product-service',\n        enabled: true,\n        minInstances: 2,\n        maxInstances: 15,\n        targetCpuUtilization: 60,\n        targetMemoryUtilization: 70,\n        targetRequestsPerSecond: 75,\n        targetResponseTime: 180,\n        scaleUpCooldown: 240,\n        scaleDownCooldown: 540,\n        scaleUpPolicy: {\n          threshold: 70,\n          periodSeconds: 90,\n          step: 2,\n        },\n        scaleDownPolicy: {\n          threshold: 35,\n          periodSeconds: 360,\n          step: 1,\n        },\n        predictiveScaling: {\n          enabled: true,\n          lookAheadMinutes: 45,\n          confidenceThreshold: 0.75,\n        },\n      },\n    ];\n\n    policies.forEach(policy => {\n      this.scalingPolicies.set(policy.service, policy);\n    });\n\n    this.logger.log(`Initialized ${policies.length} scaling policies`);\n  }\n\n  @Cron(CronExpression.EVERY_30_SECONDS)\n  async evaluateScaling(): Promise<void> {\n    this.logger.debug('Evaluating auto-scaling decisions');\n\n    for (const [serviceName, policy] of this.scalingPolicies) {\n      if (!policy.enabled) continue;\n\n      try {\n        const metrics = await this.collectServiceMetrics(serviceName);\n        this.updateMetricsHistory(serviceName, metrics);\n\n        const decision = await this.makeScalingDecision(serviceName, policy, metrics);\n        \n        if (decision.action !== 'no-action') {\n          this.logger.log(\n            `Scaling decision for ${serviceName}: ${decision.action} from ${decision.currentInstances} to ${decision.targetInstances} (${decision.reason})`\n          );\n\n          await this.executeScalingDecision(serviceName, decision);\n        }\n      } catch (error) {\n        this.logger.error(`Failed to evaluate scaling for ${serviceName}`, error);\n      }\n    }\n  }\n\n  private async collectServiceMetrics(serviceName: string): Promise<ScalingMetrics> {\n    try {\n      // Collect metrics from Prometheus/monitoring system\n      const [cpuMetrics, memoryMetrics, requestMetrics, responseTimeMetrics, queueMetrics] = await Promise.all([\n        this.getCpuMetrics(serviceName),\n        this.getMemoryMetrics(serviceName),\n        this.getRequestMetrics(serviceName),\n        this.getResponseTimeMetrics(serviceName),\n        this.getQueueMetrics(serviceName),\n      ]);\n\n      const metrics: ScalingMetrics = {\n        timestamp: new Date(),\n        cpu: cpuMetrics,\n        memory: memoryMetrics,\n        requests: requestMetrics,\n        responseTime: responseTimeMetrics,\n        queueDepth: queueMetrics,\n        errorRate: await this.getErrorRateMetrics(serviceName),\n        connections: await this.getConnectionMetrics(serviceName),\n      };\n\n      return metrics;\n    } catch (error) {\n      this.logger.error(`Failed to collect metrics for ${serviceName}`, error);\n      throw error;\n    }\n  }\n\n  private async getCpuMetrics(serviceName: string): Promise<ScalingMetrics['cpu']> {\n    // Query Prometheus for CPU metrics\n    const query = `avg(rate(container_cpu_usage_seconds_total{container=\"${serviceName}\"}[5m])) * 100`;\n    const result = await this.queryPrometheus(query);\n    \n    return {\n      current: parseFloat(result.data.result[0]?.value[1] || '0'),\n      average: await this.getAverageMetric(serviceName, 'cpu', 300), // 5 minutes\n      target: this.scalingPolicies.get(serviceName)?.targetCpuUtilization || 70,\n    };\n  }\n\n  private async getMemoryMetrics(serviceName: string): Promise<ScalingMetrics['memory']> {\n    const query = `avg(container_memory_usage_bytes{container=\"${serviceName}\"} / container_spec_memory_limit_bytes{container=\"${serviceName}\"}) * 100`;\n    const result = await this.queryPrometheus(query);\n    \n    return {\n      current: parseFloat(result.data.result[0]?.value[1] || '0'),\n      average: await this.getAverageMetric(serviceName, 'memory', 300),\n      target: this.scalingPolicies.get(serviceName)?.targetMemoryUtilization || 80,\n    };\n  }\n\n  private async getRequestMetrics(serviceName: string): Promise<ScalingMetrics['requests']> {\n    const query = `sum(rate(http_requests_total{service=\"${serviceName}\"}[1m]))`;\n    const result = await this.queryPrometheus(query);\n    \n    return {\n      perSecond: parseFloat(result.data.result[0]?.value[1] || '0'),\n      average: await this.getAverageMetric(serviceName, 'requests', 300),\n      target: this.scalingPolicies.get(serviceName)?.targetRequestsPerSecond || 100,\n    };\n  }\n\n  private async getResponseTimeMetrics(serviceName: string): Promise<ScalingMetrics['responseTime']> {\n    const currentQuery = `avg(http_request_duration_seconds{service=\"${serviceName}\"}) * 1000`;\n    const p95Query = `histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{service=\"${serviceName}\"}[5m])) * 1000`;\n    \n    const [currentResult, p95Result] = await Promise.all([\n      this.queryPrometheus(currentQuery),\n      this.queryPrometheus(p95Query),\n    ]);\n    \n    return {\n      current: parseFloat(currentResult.data.result[0]?.value[1] || '0'),\n      p95: parseFloat(p95Result.data.result[0]?.value[1] || '0'),\n      target: this.scalingPolicies.get(serviceName)?.targetResponseTime || 200,\n    };\n  }\n\n  private async getQueueMetrics(serviceName: string): Promise<ScalingMetrics['queueDepth']> {\n    const query = `avg(queue_depth{service=\"${serviceName}\"})`;\n    const result = await this.queryPrometheus(query);\n    \n    return {\n      current: parseFloat(result.data.result[0]?.value[1] || '0'),\n      average: await this.getAverageMetric(serviceName, 'queue', 300),\n      target: 30, // Default queue depth target\n    };\n  }\n\n  private async getErrorRateMetrics(serviceName: string): Promise<ScalingMetrics['errorRate']> {\n    const query = `(sum(rate(http_requests_total{service=\"${serviceName}\",status=~\"5..\"}[5m])) / sum(rate(http_requests_total{service=\"${serviceName}\"}[5m]))) * 100`;\n    const result = await this.queryPrometheus(query);\n    \n    return {\n      current: parseFloat(result.data.result[0]?.value[1] || '0'),\n      average: await this.getAverageMetric(serviceName, 'error_rate', 300),\n      threshold: 5, // 5% error rate threshold\n    };\n  }\n\n  private async getConnectionMetrics(serviceName: string): Promise<ScalingMetrics['connections']> {\n    const query = `avg(active_connections{service=\"${serviceName}\"})`;\n    const result = await this.queryPrometheus(query);\n    \n    return {\n      active: parseFloat(result.data.result[0]?.value[1] || '0'),\n      average: await this.getAverageMetric(serviceName, 'connections', 300),\n      limit: 1000, // Default connection limit\n    };\n  }\n\n  private async queryPrometheus(query: string): Promise<any> {\n    try {\n      const response = await firstValueFrom(\n        this.httpService.get('/api/v1/query', {\n          params: { query },\n          baseURL: process.env.PROMETHEUS_URL || 'http://prometheus:9090',\n        })\n      );\n      return response.data;\n    } catch (error) {\n      this.logger.error(`Prometheus query failed: ${query}`, error);\n      throw error;\n    }\n  }\n\n  private async getAverageMetric(serviceName: string, metricType: string, periodSeconds: number): Promise<number> {\n    const history = this.metricsHistory.get(serviceName) || [];\n    const cutoff = new Date(Date.now() - periodSeconds * 1000);\n    const recentMetrics = history.filter(m => m.timestamp >= cutoff);\n    \n    if (recentMetrics.length === 0) return 0;\n    \n    const sum = recentMetrics.reduce((acc, m) => {\n      switch (metricType) {\n        case 'cpu': return acc + m.cpu.current;\n        case 'memory': return acc + m.memory.current;\n        case 'requests': return acc + m.requests.perSecond;\n        case 'queue': return acc + m.queueDepth.current;\n        case 'error_rate': return acc + m.errorRate.current;\n        case 'connections': return acc + m.connections.active;\n        default: return acc;\n      }\n    }, 0);\n    \n    return sum / recentMetrics.length;\n  }\n\n  private updateMetricsHistory(serviceName: string, metrics: ScalingMetrics): void {\n    const history = this.metricsHistory.get(serviceName) || [];\n    history.push(metrics);\n    \n    // Keep only last hour of metrics\n    const cutoff = new Date(Date.now() - 3600 * 1000);\n    const filteredHistory = history.filter(m => m.timestamp >= cutoff);\n    \n    this.metricsHistory.set(serviceName, filteredHistory);\n  }\n\n  private async makeScalingDecision(\n    serviceName: string,\n    policy: ScalingPolicy,\n    metrics: ScalingMetrics\n  ): Promise<ScalingDecision> {\n    const currentInstances = await this.getCurrentInstanceCount(serviceName);\n    \n    // Check cooldown periods\n    const lastAction = this.lastScalingAction.get(serviceName);\n    if (lastAction) {\n      const timeSinceLastAction = Date.now() - lastAction.getTime();\n      const cooldownPeriod = policy.scaleUpCooldown * 1000; // Convert to milliseconds\n      \n      if (timeSinceLastAction < cooldownPeriod) {\n        return {\n          action: 'no-action',\n          type: 'horizontal',\n          currentInstances,\n          targetInstances: currentInstances,\n          reason: 'Cooldown period active',\n          confidence: 1.0,\n          estimatedDuration: 0,\n          costImpact: 0,\n        };\n      }\n    }\n\n    // Evaluate scaling triggers\n    const scaleUpTriggers = this.evaluateScaleUpTriggers(policy, metrics);\n    const scaleDownTriggers = this.evaluateScaleDownTriggers(policy, metrics);\n\n    // Predictive scaling\n    let predictiveRecommendation: Partial<ScalingDecision> = {};\n    if (policy.predictiveScaling.enabled) {\n      predictiveRecommendation = await this.getPredictiveScalingRecommendation(serviceName, policy);\n    }\n\n    // Make decision based on triggers and prediction\n    if (scaleUpTriggers.length > 0 && currentInstances < policy.maxInstances) {\n      const step = Math.min(policy.scaleUpPolicy.step, policy.maxInstances - currentInstances);\n      return {\n        action: 'scale-up',\n        type: 'horizontal',\n        currentInstances,\n        targetInstances: currentInstances + step,\n        reason: `Scale up triggers: ${scaleUpTriggers.join(', ')}`,\n        confidence: this.calculateConfidence(scaleUpTriggers, metrics),\n        estimatedDuration: 120, // 2 minutes\n        costImpact: this.calculateCostImpact(step, 'up'),\n      };\n    }\n\n    if (scaleDownTriggers.length > 0 && currentInstances > policy.minInstances) {\n      const step = Math.min(policy.scaleDownPolicy.step, currentInstances - policy.minInstances);\n      return {\n        action: 'scale-down',\n        type: 'horizontal',\n        currentInstances,\n        targetInstances: currentInstances - step,\n        reason: `Scale down triggers: ${scaleDownTriggers.join(', ')}`,\n        confidence: this.calculateConfidence(scaleDownTriggers, metrics),\n        estimatedDuration: 180, // 3 minutes\n        costImpact: this.calculateCostImpact(step, 'down'),\n      };\n    }\n\n    // Consider predictive scaling if no immediate triggers\n    if (predictiveRecommendation.action && predictiveRecommendation.action !== 'no-action') {\n      return {\n        ...predictiveRecommendation,\n        currentInstances,\n        reason: `Predictive scaling: ${predictiveRecommendation.reason}`,\n      } as ScalingDecision;\n    }\n\n    return {\n      action: 'no-action',\n      type: 'horizontal',\n      currentInstances,\n      targetInstances: currentInstances,\n      reason: 'No scaling triggers met',\n      confidence: 1.0,\n      estimatedDuration: 0,\n      costImpact: 0,\n    };\n  }\n\n  private evaluateScaleUpTriggers(policy: ScalingPolicy, metrics: ScalingMetrics): string[] {\n    const triggers: string[] = [];\n\n    if (metrics.cpu.current > policy.scaleUpPolicy.threshold) {\n      triggers.push(`CPU ${metrics.cpu.current}% > ${policy.scaleUpPolicy.threshold}%`);\n    }\n\n    if (metrics.memory.current > policy.scaleUpPolicy.threshold) {\n      triggers.push(`Memory ${metrics.memory.current}% > ${policy.scaleUpPolicy.threshold}%`);\n    }\n\n    if (metrics.requests.perSecond > policy.targetRequestsPerSecond * 1.2) {\n      triggers.push(`Requests ${metrics.requests.perSecond}/s > ${policy.targetRequestsPerSecond * 1.2}/s`);\n    }\n\n    if (metrics.responseTime.p95 > policy.targetResponseTime * 1.5) {\n      triggers.push(`Response time ${metrics.responseTime.p95}ms > ${policy.targetResponseTime * 1.5}ms`);\n    }\n\n    if (metrics.queueDepth.current > 50) {\n      triggers.push(`Queue depth ${metrics.queueDepth.current} > 50`);\n    }\n\n    if (metrics.errorRate.current > metrics.errorRate.threshold) {\n      triggers.push(`Error rate ${metrics.errorRate.current}% > ${metrics.errorRate.threshold}%`);\n    }\n\n    return triggers;\n  }\n\n  private evaluateScaleDownTriggers(policy: ScalingPolicy, metrics: ScalingMetrics): string[] {\n    const triggers: string[] = [];\n\n    if (metrics.cpu.average < policy.scaleDownPolicy.threshold && \n        metrics.memory.average < policy.scaleDownPolicy.threshold) {\n      triggers.push(`Low resource usage: CPU ${metrics.cpu.average}%, Memory ${metrics.memory.average}%`);\n    }\n\n    if (metrics.requests.perSecond < policy.targetRequestsPerSecond * 0.5) {\n      triggers.push(`Low request rate: ${metrics.requests.perSecond}/s < ${policy.targetRequestsPerSecond * 0.5}/s`);\n    }\n\n    if (metrics.queueDepth.current < 5) {\n      triggers.push(`Low queue depth: ${metrics.queueDepth.current} < 5`);\n    }\n\n    return triggers;\n  }\n\n  private async getPredictiveScalingRecommendation(\n    serviceName: string,\n    policy: ScalingPolicy\n  ): Promise<Partial<ScalingDecision>> {\n    // Implement predictive scaling based on historical patterns\n    // This is a simplified implementation\n    const history = this.metricsHistory.get(serviceName) || [];\n    \n    if (history.length < 10) {\n      return { action: 'no-action', reason: 'Insufficient historical data' };\n    }\n\n    // Analyze trends in the last lookAheadMinutes\n    const lookBackPeriod = policy.predictiveScaling.lookAheadMinutes * 60 * 1000;\n    const cutoff = new Date(Date.now() - lookBackPeriod);\n    const recentHistory = history.filter(m => m.timestamp >= cutoff);\n\n    if (recentHistory.length < 3) {\n      return { action: 'no-action', reason: 'Insufficient recent data' };\n    }\n\n    // Calculate trend\n    const cpuTrend = this.calculateTrend(recentHistory.map(m => m.cpu.current));\n    const requestTrend = this.calculateTrend(recentHistory.map(m => m.requests.perSecond));\n\n    // Predict future load\n    const currentCpu = recentHistory[recentHistory.length - 1].cpu.current;\n    const predictedCpu = currentCpu + (cpuTrend * policy.predictiveScaling.lookAheadMinutes);\n\n    const currentRequests = recentHistory[recentHistory.length - 1].requests.perSecond;\n    const predictedRequests = currentRequests + (requestTrend * policy.predictiveScaling.lookAheadMinutes);\n\n    // Make prediction-based recommendation\n    if (predictedCpu > policy.targetCpuUtilization * 1.2 || \n        predictedRequests > policy.targetRequestsPerSecond * 1.2) {\n      return {\n        action: 'scale-up',\n        type: 'horizontal',\n        targetInstances: await this.getCurrentInstanceCount(serviceName) + 1,\n        reason: `Predicted high load: CPU ${predictedCpu.toFixed(1)}%, Requests ${predictedRequests.toFixed(1)}/s`,\n        confidence: policy.predictiveScaling.confidenceThreshold,\n        estimatedDuration: 120,\n        costImpact: this.calculateCostImpact(1, 'up'),\n      };\n    }\n\n    return { action: 'no-action', reason: 'No scaling predicted' };\n  }\n\n  private calculateTrend(values: number[]): number {\n    if (values.length < 2) return 0;\n\n    // Simple linear regression slope\n    const n = values.length;\n    const sumX = (n * (n - 1)) / 2;\n    const sumY = values.reduce((a, b) => a + b, 0);\n    const sumXY = values.reduce((sum, y, x) => sum + x * y, 0);\n    const sumXX = values.reduce((sum, _, x) => sum + x * x, 0);\n\n    return (n * sumXY - sumX * sumY) / (n * sumXX - sumX * sumX);\n  }\n\n  private calculateConfidence(triggers: string[], metrics: ScalingMetrics): number {\n    // Calculate confidence based on number of triggers and severity\n    const baseConfidence = Math.min(triggers.length / 3, 1.0); // More triggers = higher confidence\n    \n    // Adjust based on metric severity\n    let severityMultiplier = 1.0;\n    \n    if (metrics.cpu.current > 90 || metrics.memory.current > 90) {\n      severityMultiplier = 1.2;\n    } else if (metrics.cpu.current < 30 && metrics.memory.current < 30) {\n      severityMultiplier = 0.8;\n    }\n\n    return Math.min(baseConfidence * severityMultiplier, 1.0);\n  }\n\n  private calculateCostImpact(instanceDelta: number, direction: 'up' | 'down'): number {\n    // Simplified cost calculation (would integrate with cloud pricing APIs)\n    const costPerInstancePerHour = 0.10; // $0.10 per hour\n    const multiplier = direction === 'up' ? 1 : -1;\n    \n    return instanceDelta * costPerInstancePerHour * multiplier;\n  }\n\n  private async getCurrentInstanceCount(serviceName: string): Promise<number> {\n    try {\n      // Query Kubernetes API for current replica count\n      const query = `count(up{job=\"${serviceName}\"})`;\n      const result = await this.queryPrometheus(query);\n      return parseInt(result.data.result[0]?.value[1] || '1');\n    } catch (error) {\n      this.logger.warn(`Failed to get instance count for ${serviceName}`, error);\n      return 1; // Default to 1 instance\n    }\n  }\n\n  private async executeScalingDecision(serviceName: string, decision: ScalingDecision): Promise<void> {\n    try {\n      this.logger.log(`Executing scaling decision for ${serviceName}: ${decision.action} to ${decision.targetInstances} instances`);\n\n      // Update Kubernetes deployment\n      await this.updateKubernetesDeployment(serviceName, decision.targetInstances);\n\n      // Record scaling action\n      this.lastScalingAction.set(serviceName, new Date());\n\n      // Emit scaling event\n      this.eventEmitter.emit('scaling.executed', {\n        service: serviceName,\n        decision,\n        timestamp: new Date(),\n      });\n\n      // Wait for scaling to complete\n      await this.waitForScalingCompletion(serviceName, decision.targetInstances, decision.estimatedDuration);\n\n      this.logger.log(`Scaling completed for ${serviceName}`);\n    } catch (error) {\n      this.logger.error(`Failed to execute scaling decision for ${serviceName}`, error);\n      throw error;\n    }\n  }\n\n  private async updateKubernetesDeployment(serviceName: string, targetReplicas: number): Promise<void> {\n    // Implementation would use Kubernetes API to update deployment replicas\n    // This is a placeholder implementation\n    this.logger.debug(`Updating deployment ${serviceName} to ${targetReplicas} replicas`);\n    \n    // kubectl patch deployment ${serviceName} -p '{\"spec\":{\"replicas\":${targetReplicas}}}'\n    // Or use Kubernetes JavaScript client\n  }\n\n  private async waitForScalingCompletion(\n    serviceName: string,\n    targetInstances: number,\n    maxWaitTime: number\n  ): Promise<void> {\n    const startTime = Date.now();\n    \n    while (Date.now() - startTime < maxWaitTime * 1000) {\n      const currentInstances = await this.getCurrentInstanceCount(serviceName);\n      \n      if (currentInstances === targetInstances) {\n        return;\n      }\n      \n      await new Promise(resolve => setTimeout(resolve, 5000)); // Wait 5 seconds\n    }\n    \n    this.logger.warn(`Scaling did not complete within ${maxWaitTime} seconds for ${serviceName}`);\n  }\n\n  // Public API methods\n  async getScalingPolicies(): Promise<ScalingPolicy[]> {\n    return Array.from(this.scalingPolicies.values());\n  }\n\n  async updateScalingPolicy(serviceName: string, updates: Partial<ScalingPolicy>): Promise<void> {\n    const policy = this.scalingPolicies.get(serviceName);\n    if (!policy) {\n      throw new Error(`Scaling policy not found for service: ${serviceName}`);\n    }\n\n    const updatedPolicy = { ...policy, ...updates };\n    this.scalingPolicies.set(serviceName, updatedPolicy);\n\n    this.logger.log(`Updated scaling policy for ${serviceName}`);\n    this.eventEmitter.emit('scaling.policy.updated', {\n      service: serviceName,\n      policy: updatedPolicy,\n    });\n  }\n\n  async getServiceMetrics(serviceName: string): Promise<ScalingMetrics | null> {\n    try {\n      return await this.collectServiceMetrics(serviceName);\n    } catch (error) {\n      this.logger.error(`Failed to get metrics for ${serviceName}`, error);\n      return null;\n    }\n  }\n\n  async getMetricsHistory(serviceName: string, hours: number = 1): Promise<ScalingMetrics[]> {\n    const history = this.metricsHistory.get(serviceName) || [];\n    const cutoff = new Date(Date.now() - hours * 3600 * 1000);\n    return history.filter(m => m.timestamp >= cutoff);\n  }\n\n  async manualScale(serviceName: string, targetInstances: number, reason: string): Promise<void> {\n    const policy = this.scalingPolicies.get(serviceName);\n    if (!policy) {\n      throw new Error(`Scaling policy not found for service: ${serviceName}`);\n    }\n\n    if (targetInstances < policy.minInstances || targetInstances > policy.maxInstances) {\n      throw new Error(`Target instances ${targetInstances} outside allowed range [${policy.minInstances}, ${policy.maxInstances}]`);\n    }\n\n    const currentInstances = await this.getCurrentInstanceCount(serviceName);\n    const decision: ScalingDecision = {\n      action: targetInstances > currentInstances ? 'scale-up' : targetInstances < currentInstances ? 'scale-down' : 'no-action',\n      type: 'horizontal',\n      currentInstances,\n      targetInstances,\n      reason: `Manual scaling: ${reason}`,\n      confidence: 1.0,\n      estimatedDuration: Math.abs(targetInstances - currentInstances) * 30, // 30 seconds per instance\n      costImpact: this.calculateCostImpact(Math.abs(targetInstances - currentInstances), targetInstances > currentInstances ? 'up' : 'down'),\n    };\n\n    if (decision.action !== 'no-action') {\n      await this.executeScalingDecision(serviceName, decision);\n    }\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"vertical-pod-autoscaler-vpa-configuration",children:"Vertical Pod Autoscaler (VPA) Configuration"}),"\n",(0,r.jsx)(n.h3,{id:"vpa-setup-for-resource-right-sizing",children:"VPA Setup for Resource Right-Sizing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# kubernetes/vpa/api-gateway-vpa.yaml\napiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: api-gateway-vpa\n  namespace: microservices\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: api-gateway\n  updatePolicy:\n    updateMode: "Auto"\n  resourcePolicy:\n    containerPolicies:\n    - containerName: api-gateway\n      minAllowed:\n        cpu: 100m\n        memory: 128Mi\n      maxAllowed:\n        cpu: 2\n        memory: 2Gi\n      controlledResources: ["cpu", "memory"]\n      controlledValues: RequestsAndLimits\n\n---\n# kubernetes/vpa/user-service-vpa.yaml\napiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: user-service-vpa\n  namespace: microservices\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: user-service\n  updatePolicy:\n    updateMode: "Auto"\n  resourcePolicy:\n    containerPolicies:\n    - containerName: user-service\n      minAllowed:\n        cpu: 50m\n        memory: 64Mi\n      maxAllowed:\n        cpu: 1\n        memory: 1Gi\n      controlledResources: ["cpu", "memory"]\n      controlledValues: RequestsAndLimits\n'})}),"\n",(0,r.jsx)(n.h2,{id:"database-auto-scaling",children:"Database Auto-Scaling"}),"\n",(0,r.jsx)(n.h3,{id:"database-connection-pool-scaling",children:"Database Connection Pool Scaling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// src/database/connection-pool-scaling.service.ts\nimport { Injectable, Logger } from '@nestjs/common';\nimport { DataSource } from 'typeorm';\n\nexport interface ConnectionPoolMetrics {\n  active: number;\n  idle: number;\n  waiting: number;\n  maxConnections: number;\n  utilizationRate: number;\n  averageWaitTime: number;\n  queryThroughput: number;\n}\n\nexport interface PoolScalingConfig {\n  minConnections: number;\n  maxConnections: number;\n  targetUtilization: number;\n  scaleUpThreshold: number;\n  scaleDownThreshold: number;\n  scaleStep: number;\n  cooldownPeriod: number;\n}\n\n@Injectable()\nexport class ConnectionPoolScalingService {\n  private readonly logger = new Logger(ConnectionPoolScalingService.name);\n  private lastScalingAction: Date | null = null;\n  \n  constructor(private readonly dataSource: DataSource) {}\n\n  async evaluatePoolScaling(config: PoolScalingConfig): Promise<void> {\n    const metrics = await this.getConnectionPoolMetrics();\n    \n    // Check cooldown period\n    if (this.lastScalingAction) {\n      const timeSinceLastAction = Date.now() - this.lastScalingAction.getTime();\n      if (timeSinceLastAction < config.cooldownPeriod * 1000) {\n        this.logger.debug('Connection pool scaling in cooldown period');\n        return;\n      }\n    }\n\n    // Evaluate scaling decision\n    if (metrics.utilizationRate > config.scaleUpThreshold && \n        metrics.maxConnections < config.maxConnections) {\n      \n      const newMaxConnections = Math.min(\n        metrics.maxConnections + config.scaleStep,\n        config.maxConnections\n      );\n      \n      await this.scaleConnectionPool(newMaxConnections);\n      this.logger.log(`Scaled up connection pool to ${newMaxConnections} connections`);\n      \n    } else if (metrics.utilizationRate < config.scaleDownThreshold && \n               metrics.maxConnections > config.minConnections) {\n      \n      const newMaxConnections = Math.max(\n        metrics.maxConnections - config.scaleStep,\n        config.minConnections\n      );\n      \n      await this.scaleConnectionPool(newMaxConnections);\n      this.logger.log(`Scaled down connection pool to ${newMaxConnections} connections`);\n    }\n  }\n\n  private async getConnectionPoolMetrics(): Promise<ConnectionPoolMetrics> {\n    // Implementation would query database connection pool statistics\n    // This is a placeholder implementation\n    return {\n      active: 15,\n      idle: 5,\n      waiting: 2,\n      maxConnections: 20,\n      utilizationRate: 0.75,\n      averageWaitTime: 50,\n      queryThroughput: 150,\n    };\n  }\n\n  private async scaleConnectionPool(newMaxConnections: number): Promise<void> {\n    // Implementation would update connection pool configuration\n    this.logger.debug(`Updating connection pool max connections to ${newMaxConnections}`);\n    this.lastScalingAction = new Date();\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"frontend-auto-scaling",children:"Frontend Auto-Scaling"}),"\n",(0,r.jsx)(n.h3,{id:"nextjs-cdn-and-edge-scaling",children:"Next.js CDN and Edge Scaling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// lib/scaling/frontend-scaling.service.ts\ninterface FrontendScalingMetrics {\n  requestsPerSecond: number;\n  responseTime: number;\n  cacheHitRatio: number;\n  errorRate: number;\n  bandwidthUsage: number;\n  geographicDistribution: Record<string, number>;\n}\n\ninterface EdgeScalingDecision {\n  action: 'scale-up' | 'scale-down' | 'no-action';\n  regions: string[];\n  cacheConfiguration: {\n    ttl: number;\n    strategies: string[];\n  };\n  reason: string;\n}\n\nclass FrontendScalingService {\n  private logger = console;\n\n  async evaluateFrontendScaling(): Promise<EdgeScalingDecision> {\n    const metrics = await this.getFrontendMetrics();\n    \n    // Evaluate if additional edge locations are needed\n    if (metrics.responseTime > 500 || metrics.cacheHitRatio < 0.8) {\n      const underservedRegions = await this.identifyUnderservedRegions(metrics);\n      \n      if (underservedRegions.length > 0) {\n        return {\n          action: 'scale-up',\n          regions: underservedRegions,\n          cacheConfiguration: {\n            ttl: 3600,\n            strategies: ['stale-while-revalidate', 'cache-first'],\n          },\n          reason: `High response time (${metrics.responseTime}ms) or low cache hit ratio (${metrics.cacheHitRatio})`,\n        };\n      }\n    }\n\n    // Evaluate if edge locations can be reduced\n    if (metrics.requestsPerSecond < 10 && metrics.cacheHitRatio > 0.95) {\n      const overProvisionedRegions = await this.identifyOverProvisionedRegions(metrics);\n      \n      if (overProvisionedRegions.length > 0) {\n        return {\n          action: 'scale-down',\n          regions: overProvisionedRegions,\n          cacheConfiguration: {\n            ttl: 1800,\n            strategies: ['cache-first'],\n          },\n          reason: `Low request rate (${metrics.requestsPerSecond}/s) with high cache efficiency`,\n        };\n      }\n    }\n\n    return {\n      action: 'no-action',\n      regions: [],\n      cacheConfiguration: {\n        ttl: 3600,\n        strategies: ['stale-while-revalidate'],\n      },\n      reason: 'No scaling needed',\n    };\n  }\n\n  private async getFrontendMetrics(): Promise<FrontendScalingMetrics> {\n    // Implementation would collect metrics from CDN provider\n    return {\n      requestsPerSecond: 50,\n      responseTime: 150,\n      cacheHitRatio: 0.85,\n      errorRate: 0.01,\n      bandwidthUsage: 100,\n      geographicDistribution: {\n        'us-east-1': 40,\n        'us-west-2': 30,\n        'eu-west-1': 20,\n        'ap-southeast-1': 10,\n      },\n    };\n  }\n\n  private async identifyUnderservedRegions(metrics: FrontendScalingMetrics): Promise<string[]> {\n    const regions: string[] = [];\n    \n    // Analyze geographic distribution for underserved areas\n    for (const [region, percentage] of Object.entries(metrics.geographicDistribution)) {\n      if (percentage > 15 && !this.hasEdgeLocation(region)) {\n        regions.push(region);\n      }\n    }\n    \n    return regions;\n  }\n\n  private async identifyOverProvisionedRegions(metrics: FrontendScalingMetrics): Promise<string[]> {\n    const regions: string[] = [];\n    \n    // Identify regions with low traffic that can be consolidated\n    for (const [region, percentage] of Object.entries(metrics.geographicDistribution)) {\n      if (percentage < 5 && this.hasEdgeLocation(region)) {\n        regions.push(region);\n      }\n    }\n    \n    return regions;\n  }\n\n  private hasEdgeLocation(region: string): boolean {\n    // Check if edge location exists in region\n    return true; // Placeholder\n  }\n}\n\nexport const frontendScalingService = new FrontendScalingService();\n"})}),"\n",(0,r.jsx)(n.h2,{id:"cost-optimization-and-monitoring",children:"Cost Optimization and Monitoring"}),"\n",(0,r.jsx)(n.h3,{id:"scaling-cost-analysis-dashboard",children:"Scaling Cost Analysis Dashboard"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'// components/admin/scaling-cost-dashboard.tsx\nimport React, { useState, useEffect } from \'react\';\nimport { Card, CardHeader, CardContent } from \'@/components/ui/card\';\nimport { Line, Bar } from \'react-chartjs-2\';\n\ninterface ScalingCostData {\n  service: string;\n  period: string;\n  actualCost: number;\n  projectedCost: number;\n  savings: number;\n  efficiency: number;\n  scalingEvents: Array<{\n    timestamp: string;\n    action: string;\n    instances: number;\n    cost: number;\n  }>;\n}\n\nexport const ScalingCostDashboard: React.FC = () => {\n  const [costData, setCostData] = useState<ScalingCostData[]>([]);\n  const [timeRange, setTimeRange] = useState(\'24h\');\n\n  useEffect(() => {\n    fetchCostData();\n    const interval = setInterval(fetchCostData, 300000); // Refresh every 5 minutes\n    return () => clearInterval(interval);\n  }, [timeRange]);\n\n  const fetchCostData = async () => {\n    try {\n      const response = await fetch(`/api/admin/scaling-costs?range=${timeRange}`);\n      const data = await response.json();\n      setCostData(data);\n    } catch (error) {\n      console.error(\'Failed to fetch scaling cost data:\', error);\n    }\n  };\n\n  const totalSavings = costData.reduce((sum, item) => sum + item.savings, 0);\n  const averageEfficiency = costData.reduce((sum, item) => sum + item.efficiency, 0) / (costData.length || 1);\n\n  return (\n    <div className="space-y-6">\n      <h1 className="text-3xl font-bold">Scaling Cost Analysis</h1>\n      \n      {/* Summary Cards */}\n      <div className="grid grid-cols-1 md:grid-cols-4 gap-4">\n        <Card>\n          <CardHeader>\n            <h3 className="text-lg font-semibold">Total Savings</h3>\n          </CardHeader>\n          <CardContent>\n            <p className="text-2xl font-bold text-green-600">\n              ${totalSavings.toFixed(2)}\n            </p>\n          </CardContent>\n        </Card>\n        \n        <Card>\n          <CardHeader>\n            <h3 className="text-lg font-semibold">Avg Efficiency</h3>\n          </CardHeader>\n          <CardContent>\n            <p className="text-2xl font-bold text-blue-600">\n              {averageEfficiency.toFixed(1)}%\n            </p>\n          </CardContent>\n        </Card>\n        \n        <Card>\n          <CardHeader>\n            <h3 className="text-lg font-semibold">Active Services</h3>\n          </CardHeader>\n          <CardContent>\n            <p className="text-2xl font-bold">\n              {costData.length}\n            </p>\n          </CardContent>\n        </Card>\n        \n        <Card>\n          <CardHeader>\n            <h3 className="text-lg font-semibold">Time Range</h3>\n          </CardHeader>\n          <CardContent>\n            <select \n              value={timeRange} \n              onChange={(e) => setTimeRange(e.target.value)}\n              className="w-full p-2 border rounded"\n            >\n              <option value="1h">Last Hour</option>\n              <option value="24h">Last 24 Hours</option>\n              <option value="7d">Last 7 Days</option>\n              <option value="30d">Last 30 Days</option>\n            </select>\n          </CardContent>\n        </Card>\n      </div>\n\n      {/* Service Cost Breakdown */}\n      <div className="grid grid-cols-1 lg:grid-cols-2 gap-6">\n        {costData.map((service) => (\n          <Card key={service.service}>\n            <CardHeader>\n              <h3 className="text-xl font-semibold capitalize">{service.service}</h3>\n            </CardHeader>\n            <CardContent className="space-y-4">\n              <div className="grid grid-cols-2 gap-4">\n                <div>\n                  <p className="text-sm text-gray-500">Actual Cost</p>\n                  <p className="text-lg font-semibold">${service.actualCost.toFixed(2)}</p>\n                </div>\n                <div>\n                  <p className="text-sm text-gray-500">Projected Cost</p>\n                  <p className="text-lg font-semibold">${service.projectedCost.toFixed(2)}</p>\n                </div>\n                <div>\n                  <p className="text-sm text-gray-500">Savings</p>\n                  <p className="text-lg font-semibold text-green-600">\n                    ${service.savings.toFixed(2)}\n                  </p>\n                </div>\n                <div>\n                  <p className="text-sm text-gray-500">Efficiency</p>\n                  <p className="text-lg font-semibold text-blue-600">\n                    {service.efficiency.toFixed(1)}%\n                  </p>\n                </div>\n              </div>\n              \n              {/* Recent Scaling Events */}\n              <div>\n                <h4 className="font-medium mb-2">Recent Scaling Events</h4>\n                <div className="space-y-1 max-h-32 overflow-y-auto">\n                  {service.scalingEvents.slice(0, 5).map((event, index) => (\n                    <div key={index} className="flex justify-between text-sm">\n                      <span>{new Date(event.timestamp).toLocaleTimeString()}</span>\n                      <span className={event.action === \'scale-up\' ? \'text-red-600\' : \'text-green-600\'}>\n                        {event.action} ({event.instances} instances)\n                      </span>\n                      <span>${event.cost.toFixed(2)}</span>\n                    </div>\n                  ))}\n                </div>\n              </div>\n            </CardContent>\n          </Card>\n        ))}\n      </div>\n    </div>\n  );\n};\n'})}),"\n",(0,r.jsx)(n.h2,{id:"related-documentation",children:"Related Documentation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"/fullstack-dev/docs/architecture-practices/architecture-execution/infrastructure-monitoring",children:"Infrastructure Monitoring"})})," - Monitoring metrics for scaling decisions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"/fullstack-dev/docs/architecture-practices/architecture-execution/performance-testing",children:"Performance Testing"})})," - Load testing to determine scaling requirements"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"/fullstack-dev/docs/architecture-practices/architecture-execution/configuration-management",children:"Configuration Management"})})," - Managing scaling configurations"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.p,{children:"This resource scaling guide should be regularly updated to incorporate new scaling strategies and cloud-native scaling solutions."})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},7814:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>c});var i=t(9729);const r={},s=i.createContext(r);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);