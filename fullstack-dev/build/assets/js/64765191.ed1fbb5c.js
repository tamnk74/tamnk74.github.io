"use strict";(self.webpackChunkdocs_site=self.webpackChunkdocs_site||[]).push([[14],{2585:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(3863),r=t(5813),s=t(5741);const o={slug:"nestjs-rate-limiting-microservices",title:"Handling Rate Limiting in NestJS for Production Microservices",authors:["tam"],tags:["nestjs","microservices","rate-limiting","security","performance","redis","production"],date:new Date("2025-10-05T00:00:00.000Z")},a="Handling Rate Limiting in NestJS for Production Microservices",l={authorsImageUrls:[void 0]},c=[{value:"Why Rate Limiting is Critical in Microservices",id:"why-rate-limiting-is-critical-in-microservices",level:2},{value:"\ud83d\udee1\ufe0f <strong>Security Benefits</strong>",id:"\ufe0f-security-benefits",level:3},{value:"\u26a1 <strong>Performance Benefits</strong>",id:"-performance-benefits",level:3},{value:"\ud83c\udfd7\ufe0f <strong>Microservice-Specific Challenges</strong>",id:"\ufe0f-microservice-specific-challenges",level:3},{value:"Popular Open Source Rate Limiting Libraries",id:"popular-open-source-rate-limiting-libraries",level:2},{value:"\ud83d\ude80 <strong>@nestjs/throttler</strong>",id:"-nestjsthrottler",level:3},{value:"\u26a1 <strong>express-rate-limit</strong>",id:"-express-rate-limit",level:3},{value:"\ud83d\udd04 <strong>bottleneck</strong>",id:"-bottleneck",level:3},{value:"\ud83c\udfea <strong>ioredis-rate-limiter</strong>",id:"-ioredis-rate-limiter",level:3},{value:"\ud83d\udcca <strong>Library Comparison</strong>",id:"-library-comparison",level:3},{value:"\ud83d\udd04 <strong>Quick Migration Guide</strong>",id:"-quick-migration-guide",level:3},{value:"Rate Limiting Strategies for Microservices",id:"rate-limiting-strategies-for-microservices",level:2},{value:"1. Token Bucket Algorithm",id:"1-token-bucket-algorithm",level:3},{value:"2. Sliding Window Algorithm",id:"2-sliding-window-algorithm",level:3},{value:"Advanced Rate Limiting Guard",id:"advanced-rate-limiting-guard",level:2},{value:"Rate Limiting Decorators",id:"rate-limiting-decorators",level:2},{value:"Microservice-Aware Rate Limiting",id:"microservice-aware-rate-limiting",level:2},{value:"Production-Ready Controller Implementation",id:"production-ready-controller-implementation",level:2},{value:"Middleware for Global Rate Limiting",id:"middleware-for-global-rate-limiting",level:2},{value:"Configuration and Module Setup",id:"configuration-and-module-setup",level:2},{value:"Monitoring and Observability",id:"monitoring-and-observability",level:2},{value:"Testing Rate Limiting",id:"testing-rate-limiting",level:2},{value:"Best Practices and Production Considerations",id:"best-practices-and-production-considerations",level:2},{value:"\ud83d\udd27 <strong>Configuration Management</strong>",id:"-configuration-management",level:3},{value:"\ud83d\udcca <strong>Monitoring and Alerting</strong>",id:"-monitoring-and-alerting",level:3},{value:"\u26a1 <strong>Performance Optimization</strong>",id:"-performance-optimization",level:3},{value:"\ud83d\udee1\ufe0f <strong>Security Considerations</strong>",id:"\ufe0f-security-considerations",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Rate limiting is a critical security and performance mechanism in production microservice architectures. It protects your services from abuse, ensures fair resource allocation, and maintains system stability under high load. In this comprehensive guide, we'll explore how to implement robust, scalable rate limiting in NestJS applications designed for microservice environments."}),"\n",(0,r.jsx)(n.h2,{id:"why-rate-limiting-is-critical-in-microservices",children:"Why Rate Limiting is Critical in Microservices"}),"\n",(0,r.jsxs)(n.h3,{id:"\ufe0f-security-benefits",children:["\ud83d\udee1\ufe0f ",(0,r.jsx)(n.strong,{children:"Security Benefits"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"DDoS Protection"}),": Prevents overwhelming individual services with excessive requests"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"API Abuse Prevention"}),": Stops malicious actors from exhausting system resources"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Brute Force Attack Mitigation"}),": Limits login attempts and sensitive operations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Resource Protection"}),": Ensures no single client can monopolize service capacity"]}),"\n"]}),"\n",(0,r.jsxs)(n.h3,{id:"-performance-benefits",children:["\u26a1 ",(0,r.jsx)(n.strong,{children:"Performance Benefits"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fair Resource Allocation"}),": Ensures all users get equitable access to services"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cascade Failure Prevention"}),": Prevents one overloaded service from affecting others"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost Control"}),": Manages computational and bandwidth costs in cloud environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"SLA Compliance"}),": Helps maintain service level agreements under varying loads"]}),"\n"]}),"\n",(0,r.jsxs)(n.h3,{id:"\ufe0f-microservice-specific-challenges",children:["\ud83c\udfd7\ufe0f ",(0,r.jsx)(n.strong,{children:"Microservice-Specific Challenges"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Distributed State Management"}),": Rate limits must work across multiple service instances"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Service-to-Service Communication"}),": Internal APIs need different limits than public ones"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dynamic Scaling"}),": Rate limits must adapt to auto-scaling scenarios"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cross-Service Coordination"}),": Shared limits across multiple related services"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"popular-open-source-rate-limiting-libraries",children:"Popular Open Source Rate Limiting Libraries"}),"\n",(0,r.jsx)(n.p,{children:"Before implementing custom rate limiting solutions, consider these battle-tested libraries that can significantly speed up development:"}),"\n",(0,r.jsxs)(n.h3,{id:"-nestjsthrottler",children:["\ud83d\ude80 ",(0,r.jsx)(n.strong,{children:"@nestjs/throttler"})]}),"\n",(0,r.jsx)(n.p,{children:"The official NestJS throttling library provides simple, decorator-based rate limiting."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// Installation\nnpm install @nestjs/throttler\n\n// Basic setup\nimport { ThrottlerModule, ThrottlerGuard } from '@nestjs/throttler';\nimport { APP_GUARD } from '@nestjs/core';\n\n@Module({\n  imports: [\n    ThrottlerModule.forRoot([\n      {\n        name: 'short',\n        ttl: 1000,  // 1 second\n        limit: 3,   // 3 requests per second\n      },\n      {\n        name: 'medium',\n        ttl: 10000, // 10 seconds\n        limit: 20,  // 20 requests per 10 seconds\n      },\n      {\n        name: 'long',\n        ttl: 60000, // 1 minute\n        limit: 100, // 100 requests per minute\n      },\n    ]),\n  ],\n  providers: [\n    {\n      provide: APP_GUARD,\n      useClass: ThrottlerGuard,\n    },\n  ],\n})\nexport class AppModule {}\n\n// Usage in controllers\nimport { Throttle } from '@nestjs/throttler';\n\n@Controller('api')\nexport class ApiController {\n  @Get('public')\n  @Throttle({ default: { limit: 10, ttl: 60000 } })\n  getPublicData() {\n    return { data: 'Limited to 10 requests per minute' };\n  }\n\n  @Post('auth/login')\n  @Throttle({ default: { limit: 5, ttl: 300000 } }) // 5 attempts per 5 minutes\n  login(@Body() credentials: any) {\n    return { token: 'jwt-token' };\n  }\n}\n"})}),"\n",(0,r.jsxs)(n.h3,{id:"-express-rate-limit",children:["\u26a1 ",(0,r.jsx)(n.strong,{children:"express-rate-limit"})]}),"\n",(0,r.jsx)(n.p,{children:"A mature Express.js rate limiting middleware that works seamlessly with NestJS."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// Installation\nnpm install express-rate-limit\n\n// Global setup\nimport rateLimit from 'express-rate-limit';\n\nconst app = await NestFactory.create(AppModule);\n\n// Global rate limiting\napp.use(\n  rateLimit({\n    windowMs: 15 * 60 * 1000, // 15 minutes\n    max: 100, // Limit each IP to 100 requests per windowMs\n    message: {\n      error: 'Too many requests, please try again later.',\n      retryAfter: '15 minutes',\n    },\n    standardHeaders: true, // Return rate limit info in headers\n    legacyHeaders: false,\n    // Redis store for distributed rate limiting\n    store: new RedisStore({\n      sendCommand: (...args: string[]) => redisClient.call(...args),\n    }),\n  })\n);\n\n// Route-specific limiting\nimport { Request, Response, NextFunction } from 'express';\n\nconst loginLimiter = rateLimit({\n  windowMs: 15 * 60 * 1000, // 15 minutes\n  max: 5, // Limit each IP to 5 login requests per windowMs\n  message: 'Too many login attempts, please try again later.',\n  skipSuccessfulRequests: true, // Don't count successful requests\n});\n\n@Controller('auth')\nexport class AuthController {\n  @Post('login')\n  @UseInterceptors(\n    // Apply rate limiting middleware as interceptor\n    (req: Request, res: Response, next: NextFunction) => \n      loginLimiter(req, res, next)\n  )\n  login(@Body() credentials: any) {\n    return { token: 'jwt-token' };\n  }\n}\n"})}),"\n",(0,r.jsxs)(n.h3,{id:"-bottleneck",children:["\ud83d\udd04 ",(0,r.jsx)(n.strong,{children:"bottleneck"})]}),"\n",(0,r.jsx)(n.p,{children:"Advanced rate limiting with queuing, clustering, and Redis support."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// Installation\nnpm install bottleneck\n\n// Service implementation\nimport Bottleneck from 'bottleneck';\nimport { Injectable } from '@nestjs/common';\n\n@Injectable()\nexport class AdvancedRateLimitService {\n  private limiters = new Map<string, Bottleneck>();\n\n  constructor() {\n    // Create different limiters for different use cases\n    this.setupLimiters();\n  }\n\n  private setupLimiters() {\n    // API rate limiter with Redis clustering\n    this.limiters.set('api', new Bottleneck({\n      reservoir: 100,           // Initial capacity\n      reservoirRefreshAmount: 100, // Tokens to add\n      reservoirRefreshInterval: 60 * 1000, // Every minute\n      maxConcurrent: 10,        // Max concurrent requests\n      minTime: 100,             // Min time between requests (100ms)\n      \n      // Redis clustering for distributed rate limiting\n      datastore: 'redis',\n      clearDatastore: false,\n      clientOptions: {\n        host: 'localhost',\n        port: 6379,\n      },\n      clusterNodes: [\n        { host: 'redis-1', port: 6379 },\n        { host: 'redis-2', port: 6379 },\n      ],\n    }));\n\n    // Premium API with higher limits\n    this.limiters.set('premium', new Bottleneck({\n      reservoir: 1000,\n      reservoirRefreshAmount: 1000,\n      reservoirRefreshInterval: 60 * 1000,\n      maxConcurrent: 50,\n      minTime: 20,\n    }));\n\n    // Heavy processing limiter\n    this.limiters.set('processing', new Bottleneck({\n      maxConcurrent: 3,         // Only 3 heavy operations at once\n      minTime: 2000,            // 2 seconds between operations\n    }));\n  }\n\n  async executeWithRateLimit<T>(\n    limiterType: string,\n    operation: () => Promise<T>,\n    priority?: number\n  ): Promise<T> {\n    const limiter = this.limiters.get(limiterType);\n    if (!limiter) {\n      throw new Error(`Unknown limiter type: ${limiterType}`);\n    }\n\n    return limiter.schedule({ priority }, operation);\n  }\n\n  async checkRateLimit(\n    limiterType: string,\n    weight: number = 1\n  ): Promise<{ allowed: boolean; msBeforeNext: number }> {\n    const limiter = this.limiters.get(limiterType);\n    if (!limiter) {\n      return { allowed: false, msBeforeNext: 0 };\n    }\n\n    try {\n      const result = await limiter.check(weight);\n      return {\n        allowed: result >= 0,\n        msBeforeNext: result < 0 ? Math.abs(result) : 0,\n      };\n    } catch (error) {\n      return { allowed: false, msBeforeNext: 1000 };\n    }\n  }\n\n  // Get limiter statistics\n  getStats(limiterType: string) {\n    const limiter = this.limiters.get(limiterType);\n    if (!limiter) return null;\n\n    return {\n      running: limiter.running(),\n      queued: limiter.queued(),\n      reservoir: limiter.reservoir(),\n      capacity: limiter.capacity,\n    };\n  }\n}\n\n// Usage in controllers\n@Controller('api')\nexport class ApiController {\n  constructor(\n    private readonly rateLimitService: AdvancedRateLimitService\n  ) {}\n\n  @Get('data')\n  async getData() {\n    return this.rateLimitService.executeWithRateLimit(\n      'api',\n      async () => {\n        // Your API logic here\n        return { data: 'API response' };\n      }\n    );\n  }\n\n  @Post('heavy-process')\n  async heavyProcess(@Body() data: any) {\n    const check = await this.rateLimitService.checkRateLimit('processing');\n    \n    if (!check.allowed) {\n      throw new HttpException(\n        {\n          message: 'Processing queue full',\n          retryAfter: Math.ceil(check.msBeforeNext / 1000),\n        },\n        HttpStatus.TOO_MANY_REQUESTS\n      );\n    }\n\n    return this.rateLimitService.executeWithRateLimit(\n      'processing',\n      async () => {\n        // Heavy processing logic\n        await this.performHeavyOperation(data);\n        return { status: 'completed' };\n      },\n      5 // High priority\n    );\n  }\n\n  @Get('stats')\n  getSystemStats() {\n    return {\n      api: this.rateLimitService.getStats('api'),\n      premium: this.rateLimitService.getStats('premium'),\n      processing: this.rateLimitService.getStats('processing'),\n    };\n  }\n}\n"})}),"\n",(0,r.jsxs)(n.h3,{id:"-ioredis-rate-limiter",children:["\ud83c\udfea ",(0,r.jsx)(n.strong,{children:"ioredis-rate-limiter"})]}),"\n",(0,r.jsx)(n.p,{children:"Redis-based rate limiting with sliding window implementation."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// Installation\nnpm install ioredis-rate-limiter ioredis\n\n// Service implementation\nimport RateLimiter from 'ioredis-rate-limiter';\nimport Redis from 'ioredis';\nimport { Injectable } from '@nestjs/common';\n\n@Injectable()\nexport class RedisRateLimitService {\n  private redis: Redis;\n  private limiters = new Map<string, RateLimiter>();\n\n  constructor() {\n    this.redis = new Redis({\n      host: 'localhost',\n      port: 6379,\n      retryDelayOnFailover: 100,\n    });\n\n    this.setupLimiters();\n  }\n\n  private setupLimiters() {\n    // API rate limiter - 100 requests per minute\n    this.limiters.set('api', new RateLimiter({\n      redis: this.redis,\n      key: (id: string) => `rate_limit:api:${id}`,\n      window: 60000,        // 1 minute window\n      limit: 100,           // 100 requests per window\n    }));\n\n    // Login rate limiter - 5 attempts per 15 minutes\n    this.limiters.set('login', new RateLimiter({\n      redis: this.redis,\n      key: (id: string) => `rate_limit:login:${id}`,\n      window: 15 * 60000,   // 15 minutes\n      limit: 5,             // 5 attempts\n    }));\n\n    // Premium API - 1000 requests per minute\n    this.limiters.set('premium', new RateLimiter({\n      redis: this.redis,\n      key: (id: string) => `rate_limit:premium:${id}`,\n      window: 60000,\n      limit: 1000,\n    }));\n  }\n\n  async checkLimit(\n    type: string,\n    identifier: string,\n    cost: number = 1\n  ): Promise<{\n    allowed: boolean;\n    remaining: number;\n    resetTime: Date;\n    totalHits: number;\n  }> {\n    const limiter = this.limiters.get(type);\n    if (!limiter) {\n      throw new Error(`Unknown rate limiter type: ${type}`);\n    }\n\n    try {\n      const result = await limiter.check(identifier, cost);\n      \n      return {\n        allowed: result.allowed,\n        remaining: result.remaining,\n        resetTime: new Date(result.resetTime),\n        totalHits: result.totalHits,\n      };\n    } catch (error) {\n      // Fallback in case of Redis issues\n      console.error('Rate limit check failed:', error);\n      return {\n        allowed: true, // Fail open for availability\n        remaining: 0,\n        resetTime: new Date(Date.now() + 60000),\n        totalHits: 0,\n      };\n    }\n  }\n\n  async getRemainingQuota(type: string, identifier: string): Promise<number> {\n    const result = await this.checkLimit(type, identifier, 0); // Cost 0 for check only\n    return result.remaining;\n  }\n}\n\n// Guard implementation\n@Injectable()\nexport class RedisRateLimitGuard implements CanActivate {\n  constructor(\n    private readonly rateLimitService: RedisRateLimitService,\n    private readonly reflector: Reflector\n  ) {}\n\n  async canActivate(context: ExecutionContext): Promise<boolean> {\n    const request = context.switchToHttp().getRequest();\n    const response = context.switchToHttp().getResponse();\n    \n    // Get rate limit configuration from decorator\n    const config = this.reflector.get<{\n      type: string;\n      identifier?: (req: any) => string;\n      cost?: number;\n    }>('REDIS_RATE_LIMIT', context.getHandler());\n\n    if (!config) {\n      return true; // No rate limiting configured\n    }\n\n    // Generate identifier\n    const identifier = config.identifier \n      ? config.identifier(request)\n      : request.ip;\n\n    // Check rate limit\n    const result = await this.rateLimitService.checkLimit(\n      config.type,\n      identifier,\n      config.cost || 1\n    );\n\n    // Set headers\n    response.setHeader('X-RateLimit-Remaining', result.remaining);\n    response.setHeader('X-RateLimit-Reset', result.resetTime.toISOString());\n    response.setHeader('X-RateLimit-Total', result.totalHits);\n\n    if (!result.allowed) {\n      response.setHeader('Retry-After', Math.ceil((result.resetTime.getTime() - Date.now()) / 1000));\n      \n      throw new HttpException(\n        {\n          statusCode: HttpStatus.TOO_MANY_REQUESTS,\n          message: 'Rate limit exceeded',\n          retryAfter: result.resetTime.toISOString(),\n        },\n        HttpStatus.TOO_MANY_REQUESTS\n      );\n    }\n\n    return true;\n  }\n}\n\n// Decorator for easy usage\nexport const RedisRateLimit = (config: {\n  type: string;\n  identifier?: (req: any) => string;\n  cost?: number;\n}) => SetMetadata('REDIS_RATE_LIMIT', config);\n\n// Usage example\n@Controller('api')\n@UseGuards(RedisRateLimitGuard)\nexport class ApiController {\n  @Get('data')\n  @RedisRateLimit({\n    type: 'api',\n    identifier: (req) => req.ip,\n  })\n  getData() {\n    return { data: 'API response' };\n  }\n\n  @Post('login')\n  @RedisRateLimit({\n    type: 'login',\n    identifier: (req) => `${req.ip}:${req.body.email}`,\n  })\n  login(@Body() credentials: any) {\n    return { token: 'jwt-token' };\n  }\n\n  @Get('premium')\n  @RedisRateLimit({\n    type: 'premium',\n    identifier: (req) => req.user?.id || req.ip,\n    cost: 5, // This endpoint costs 5 \"points\"\n  })\n  getPremiumData() {\n    return { data: 'Premium API response' };\n  }\n}\n"})}),"\n",(0,r.jsxs)(n.h3,{id:"-library-comparison",children:["\ud83d\udcca ",(0,r.jsx)(n.strong,{children:"Library Comparison"})]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Library"}),(0,r.jsx)(n.th,{children:"Pros"}),(0,r.jsx)(n.th,{children:"Cons"}),(0,r.jsx)(n.th,{children:"Best For"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"@nestjs/throttler"})}),(0,r.jsxs)(n.td,{children:["\u2705 Official NestJS support",(0,r.jsx)("br",{}),"\u2705 Decorator-based",(0,r.jsx)("br",{}),"\u2705 Multiple rate limits",(0,r.jsx)("br",{}),"\u2705 Redis support"]}),(0,r.jsxs)(n.td,{children:["\u274c Limited algorithms",(0,r.jsx)("br",{}),"\u274c Basic queuing"]}),(0,r.jsx)(n.td,{children:"Simple rate limiting, Getting started"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"express-rate-limit"})}),(0,r.jsxs)(n.td,{children:["\u2705 Mature & stable",(0,r.jsx)("br",{}),"\u2705 Extensive middleware",(0,r.jsx)("br",{}),"\u2705 Good documentation",(0,r.jsx)("br",{}),"\u2705 Flexible configuration"]}),(0,r.jsxs)(n.td,{children:["\u274c Express-specific",(0,r.jsx)("br",{}),"\u274c Limited queue management"]}),(0,r.jsx)(n.td,{children:"General web applications, Migration from Express"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"bottleneck"})}),(0,r.jsxs)(n.td,{children:["\u2705 Advanced queuing",(0,r.jsx)("br",{}),"\u2705 Clustering support",(0,r.jsx)("br",{}),"\u2705 Priority queues",(0,r.jsx)("br",{}),"\u2705 Multiple algorithms"]}),(0,r.jsxs)(n.td,{children:["\u274c Complex configuration",(0,r.jsx)("br",{}),"\u274c Learning curve"]}),(0,r.jsx)(n.td,{children:"Complex rate limiting, Job queues, Distributed systems"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"ioredis-rate-limiter"})}),(0,r.jsxs)(n.td,{children:["\u2705 Sliding window",(0,r.jsx)("br",{}),"\u2705 High performance",(0,r.jsx)("br",{}),"\u2705 Simple API",(0,r.jsx)("br",{}),"\u2705 Redis-native"]}),(0,r.jsxs)(n.td,{children:["\u274c Redis dependency",(0,r.jsx)("br",{}),"\u274c Limited to sliding window"]}),(0,r.jsx)(n.td,{children:"High-performance APIs, Redis-based infrastructure"})]})]})]}),"\n",(0,r.jsxs)(n.h3,{id:"-quick-migration-guide",children:["\ud83d\udd04 ",(0,r.jsx)(n.strong,{children:"Quick Migration Guide"})]}),"\n",(0,r.jsx)(n.p,{children:"If you're using custom rate limiting and want to migrate to a library:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// From custom implementation\n@RateLimit({\n  algorithm: 'sliding-window',\n  limit: 100,\n  windowMs: 60000,\n})\n\n// To @nestjs/throttler\n@Throttle({ default: { limit: 100, ttl: 60000 } })\n\n// To bottleneck\nasync method() {\n  return this.rateLimitService.executeWithRateLimit('api', async () => {\n    // Your logic here\n  });\n}\n\n// To ioredis-rate-limiter\n@RedisRateLimit({\n  type: 'api',\n  identifier: (req) => req.ip,\n})\n"})}),"\n",(0,r.jsx)(n.h2,{id:"rate-limiting-strategies-for-microservices",children:"Rate Limiting Strategies for Microservices"}),"\n",(0,r.jsx)(n.h3,{id:"1-token-bucket-algorithm",children:"1. Token Bucket Algorithm"}),"\n",(0,r.jsx)(n.p,{children:"The token bucket algorithm is ideal for allowing bursts while maintaining average rate limits."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// rate-limiting/token-bucket.service.ts\nimport { Injectable } from '@nestjs/common';\nimport { RedisService } from '@nestjs-modules/ioredis';\n\ninterface TokenBucketConfig {\n  capacity: number;      // Maximum tokens in bucket\n  refillRate: number;    // Tokens added per second\n  windowSize: number;    // Time window in seconds\n}\n\n@Injectable()\nexport class TokenBucketService {\n  constructor(private readonly redis: RedisService) {}\n\n  async checkRateLimit(\n    key: string,\n    config: TokenBucketConfig,\n    tokensRequested: number = 1\n  ): Promise<{\n    allowed: boolean;\n    remainingTokens: number;\n    resetTime: number;\n    retryAfter?: number;\n  }> {\n    const script = `\n      local key = KEYS[1]\n      local capacity = tonumber(ARGV[1])\n      local refillRate = tonumber(ARGV[2])\n      local windowSize = tonumber(ARGV[3])\n      local tokensRequested = tonumber(ARGV[4])\n      local now = tonumber(ARGV[5])\n      \n      -- Get current bucket state\n      local bucket = redis.call('HMGET', key, 'tokens', 'lastRefill')\n      local tokens = tonumber(bucket[1]) or capacity\n      local lastRefill = tonumber(bucket[2]) or now\n      \n      -- Calculate tokens to add based on time elapsed\n      local elapsed = math.max(0, now - lastRefill)\n      local tokensToAdd = math.floor(elapsed * refillRate)\n      tokens = math.min(capacity, tokens + tokensToAdd)\n      \n      -- Check if request can be fulfilled\n      local allowed = tokens >= tokensRequested\n      local remainingTokens = tokens\n      local retryAfter = 0\n      \n      if allowed then\n        remainingTokens = tokens - tokensRequested\n        -- Update bucket state\n        redis.call('HMSET', key, 'tokens', remainingTokens, 'lastRefill', now)\n        redis.call('EXPIRE', key, windowSize * 2)\n      else\n        -- Calculate retry after time\n        local tokensNeeded = tokensRequested - tokens\n        retryAfter = math.ceil(tokensNeeded / refillRate)\n      end\n      \n      return {\n        allowed and 1 or 0,\n        remainingTokens,\n        now + (capacity - remainingTokens) / refillRate,\n        retryAfter\n      }\n    `;\n\n    const now = Math.floor(Date.now() / 1000);\n    const result = await this.redis.eval(\n      script,\n      1,\n      key,\n      config.capacity,\n      config.refillRate,\n      config.windowSize,\n      tokensRequested,\n      now\n    ) as [number, number, number, number];\n\n    return {\n      allowed: result[0] === 1,\n      remainingTokens: result[1],\n      resetTime: result[2],\n      retryAfter: result[3] > 0 ? result[3] : undefined,\n    };\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-sliding-window-algorithm",children:"2. Sliding Window Algorithm"}),"\n",(0,r.jsx)(n.p,{children:"Sliding window provides more accurate rate limiting by tracking requests in a rolling time window."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// rate-limiting/sliding-window.service.ts\nimport { Injectable } from '@nestjs/common';\nimport { RedisService } from '@nestjs-modules/ioredis';\n\ninterface SlidingWindowConfig {\n  limit: number;         // Maximum requests per window\n  windowMs: number;      // Window size in milliseconds\n  precision: number;     // Sub-window precision (e.g., 10 for 10 sub-windows)\n}\n\n@Injectable()\nexport class SlidingWindowService {\n  constructor(private readonly redis: RedisService) {}\n\n  async checkRateLimit(\n    key: string,\n    config: SlidingWindowConfig\n  ): Promise<{\n    allowed: boolean;\n    remaining: number;\n    resetTime: number;\n    currentUsage: number;\n  }> {\n    const script = `\n      local key = KEYS[1]\n      local limit = tonumber(ARGV[1])\n      local windowMs = tonumber(ARGV[2])\n      local precision = tonumber(ARGV[3])\n      local now = tonumber(ARGV[4])\n      \n      -- Calculate sub-window size\n      local subWindowMs = windowMs / precision\n      local currentWindow = math.floor(now / subWindowMs)\n      local windowStart = currentWindow - precision + 1\n      \n      -- Clean old sub-windows\n      redis.call('ZREMRANGEBYSCORE', key, 0, windowStart - 1)\n      \n      -- Count current requests in the sliding window\n      local currentCount = 0\n      local subWindows = redis.call('ZRANGE', key, 0, -1, 'WITHSCORES')\n      \n      for i = 1, #subWindows, 2 do\n        local subWindow = tonumber(subWindows[i + 1])\n        local count = tonumber(subWindows[i])\n        \n        if subWindow >= windowStart then\n          -- Calculate overlap percentage for partial windows\n          local overlap = 1\n          if subWindow == windowStart then\n            local subWindowTime = subWindow * subWindowMs\n            local windowTime = now - windowMs\n            overlap = math.max(0, (subWindowTime + subWindowMs - windowTime) / subWindowMs)\n          end\n          currentCount = currentCount + (count * overlap)\n        end\n      end\n      \n      local allowed = currentCount < limit\n      local remaining = math.max(0, limit - currentCount - 1)\n      \n      if allowed then\n        -- Increment counter for current sub-window\n        redis.call('ZINCRBY', key, 1, currentWindow)\n        redis.call('EXPIRE', key, math.ceil(windowMs / 1000) * 2)\n      end\n      \n      local resetTime = (currentWindow + 1) * subWindowMs\n      \n      return {\n        allowed and 1 or 0,\n        remaining,\n        resetTime,\n        currentCount + (allowed and 1 or 0)\n      }\n    `;\n\n    const now = Date.now();\n    const result = await this.redis.eval(\n      script,\n      1,\n      key,\n      config.limit,\n      config.windowMs,\n      config.precision,\n      now\n    ) as [number, number, number, number];\n\n    return {\n      allowed: result[0] === 1,\n      remaining: result[1],\n      resetTime: result[2],\n      currentUsage: result[3],\n    };\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"advanced-rate-limiting-guard",children:"Advanced Rate Limiting Guard"}),"\n",(0,r.jsx)(n.p,{children:"Create a flexible, configurable rate limiting guard that supports multiple algorithms and strategies."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// guards/rate-limit.guard.ts\nimport {\n  Injectable,\n  CanActivate,\n  ExecutionContext,\n  HttpException,\n  HttpStatus,\n  Inject,\n} from '@nestjs/common';\nimport { Reflector } from '@nestjs/core';\nimport { Request, Response } from 'express';\nimport { TokenBucketService } from '../rate-limiting/token-bucket.service';\nimport { SlidingWindowService } from '../rate-limiting/sliding-window.service';\nimport { RATE_LIMIT_OPTIONS } from '../decorators/rate-limit.decorator';\n\nexport interface RateLimitOptions {\n  algorithm: 'token-bucket' | 'sliding-window' | 'fixed-window';\n  keyGenerator?: (req: Request) => string;\n  skipIf?: (req: Request) => boolean;\n  onLimitReached?: (req: Request, res: Response) => void;\n  \n  // Token bucket specific\n  capacity?: number;\n  refillRate?: number;\n  \n  // Sliding/Fixed window specific\n  limit?: number;\n  windowMs?: number;\n  precision?: number; // For sliding window\n  \n  // Common options\n  message?: string;\n  standardHeaders?: boolean;\n  legacyHeaders?: boolean;\n  skipSuccessfulRequests?: boolean;\n  skipFailedRequests?: boolean;\n}\n\n@Injectable()\nexport class RateLimitGuard implements CanActivate {\n  constructor(\n    private readonly reflector: Reflector,\n    private readonly tokenBucket: TokenBucketService,\n    private readonly slidingWindow: SlidingWindowService,\n    @Inject('CONFIG_SERVICE') private readonly config: any\n  ) {}\n\n  async canActivate(context: ExecutionContext): Promise<boolean> {\n    const request = context.switchToHttp().getRequest<Request>();\n    const response = context.switchToHttp().getResponse<Response>();\n    \n    // Get rate limit options from decorator or global config\n    const options = this.getRateLimitOptions(context);\n    \n    if (!options) {\n      return true; // No rate limiting configured\n    }\n\n    // Check if request should be skipped\n    if (options.skipIf && options.skipIf(request)) {\n      return true;\n    }\n\n    // Generate rate limit key\n    const key = this.generateKey(request, options);\n    \n    // Apply rate limiting based on algorithm\n    const result = await this.applyRateLimit(key, options);\n    \n    // Set response headers\n    this.setHeaders(response, result, options);\n    \n    if (!result.allowed) {\n      // Execute custom handler if provided\n      if (options.onLimitReached) {\n        options.onLimitReached(request, response);\n      }\n      \n      // Throw rate limit exceeded exception\n      throw new HttpException(\n        {\n          statusCode: HttpStatus.TOO_MANY_REQUESTS,\n          message: options.message || 'Rate limit exceeded',\n          error: 'Too Many Requests',\n          retryAfter: result.retryAfter,\n        },\n        HttpStatus.TOO_MANY_REQUESTS\n      );\n    }\n\n    return true;\n  }\n\n  private getRateLimitOptions(context: ExecutionContext): RateLimitOptions | null {\n    // Check method-level decorator first\n    const methodOptions = this.reflector.get<RateLimitOptions>(\n      RATE_LIMIT_OPTIONS,\n      context.getHandler()\n    );\n    \n    if (methodOptions) {\n      return methodOptions;\n    }\n\n    // Check class-level decorator\n    const classOptions = this.reflector.get<RateLimitOptions>(\n      RATE_LIMIT_OPTIONS,\n      context.getClass()\n    );\n    \n    if (classOptions) {\n      return classOptions;\n    }\n\n    // Return global default options if configured\n    return this.config.get('rateLimit.default');\n  }\n\n  private generateKey(request: Request, options: RateLimitOptions): string {\n    if (options.keyGenerator) {\n      return options.keyGenerator(request);\n    }\n\n    // Default key generation strategy\n    const ip = request.ip || request.connection.remoteAddress;\n    const userId = (request as any).user?.id || 'anonymous';\n    const route = `${request.method}:${request.route?.path || request.path}`;\n    \n    return `rate_limit:${ip}:${userId}:${route}`;\n  }\n\n  private async applyRateLimit(\n    key: string,\n    options: RateLimitOptions\n  ): Promise<{\n    allowed: boolean;\n    remaining?: number;\n    resetTime?: number;\n    retryAfter?: number;\n    currentUsage?: number;\n  }> {\n    switch (options.algorithm) {\n      case 'token-bucket':\n        return this.tokenBucket.checkRateLimit(key, {\n          capacity: options.capacity || 10,\n          refillRate: options.refillRate || 1,\n          windowSize: Math.floor((options.windowMs || 60000) / 1000),\n        });\n\n      case 'sliding-window':\n        return this.slidingWindow.checkRateLimit(key, {\n          limit: options.limit || 100,\n          windowMs: options.windowMs || 60000,\n          precision: options.precision || 10,\n        });\n\n      case 'fixed-window':\n        return this.applyFixedWindow(key, {\n          limit: options.limit || 100,\n          windowMs: options.windowMs || 60000,\n        });\n\n      default:\n        throw new Error(`Unknown rate limiting algorithm: ${options.algorithm}`);\n    }\n  }\n\n  private async applyFixedWindow(\n    key: string,\n    config: { limit: number; windowMs: number }\n  ): Promise<any> {\n    // Simple fixed window implementation using Redis\n    const window = Math.floor(Date.now() / config.windowMs);\n    const windowKey = `${key}:${window}`;\n    \n    const current = await this.tokenBucket['redis'].incr(windowKey);\n    await this.tokenBucket['redis'].expire(windowKey, Math.ceil(config.windowMs / 1000));\n    \n    const allowed = current <= config.limit;\n    const remaining = Math.max(0, config.limit - current);\n    const resetTime = (window + 1) * config.windowMs;\n    \n    return {\n      allowed,\n      remaining,\n      resetTime,\n      currentUsage: current,\n    };\n  }\n\n  private setHeaders(response: Response, result: any, options: RateLimitOptions): void {\n    if (options.standardHeaders !== false) {\n      // Standard rate limit headers (draft RFC)\n      response.setHeader('RateLimit-Limit', options.limit || options.capacity);\n      response.setHeader('RateLimit-Remaining', result.remaining || 0);\n      response.setHeader('RateLimit-Reset', result.resetTime);\n      \n      if (result.retryAfter) {\n        response.setHeader('Retry-After', result.retryAfter);\n      }\n    }\n\n    if (options.legacyHeaders) {\n      // Legacy X-RateLimit headers for compatibility\n      response.setHeader('X-RateLimit-Limit', options.limit || options.capacity);\n      response.setHeader('X-RateLimit-Remaining', result.remaining || 0);\n      response.setHeader('X-RateLimit-Reset', result.resetTime);\n    }\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"rate-limiting-decorators",children:"Rate Limiting Decorators"}),"\n",(0,r.jsx)(n.p,{children:"Create flexible decorators for easy application of rate limits."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// decorators/rate-limit.decorator.ts\nimport { SetMetadata } from '@nestjs/common';\nimport { RateLimitOptions } from '../guards/rate-limit.guard';\n\nexport const RATE_LIMIT_OPTIONS = 'RATE_LIMIT_OPTIONS';\n\nexport const RateLimit = (options: RateLimitOptions) => \n  SetMetadata(RATE_LIMIT_OPTIONS, options);\n\n// Convenience decorators for common patterns\nexport const PublicApiRateLimit = () => \n  RateLimit({\n    algorithm: 'sliding-window',\n    limit: 1000,\n    windowMs: 60000, // 1 minute\n    message: 'Too many requests from this IP, please try again later',\n    standardHeaders: true,\n  });\n\nexport const AuthRateLimit = () => \n  RateLimit({\n    algorithm: 'token-bucket',\n    capacity: 5,\n    refillRate: 1/60, // 1 token per minute\n    windowMs: 300000, // 5 minutes\n    message: 'Too many authentication attempts, please try again later',\n    keyGenerator: (req) => `auth:${req.ip}:${req.body?.email || 'unknown'}`,\n  });\n\nexport const PremiumApiRateLimit = () => \n  RateLimit({\n    algorithm: 'sliding-window',\n    limit: 10000,\n    windowMs: 60000,\n    skipIf: (req) => (req as any).user?.plan === 'enterprise',\n    keyGenerator: (req) => `premium:${(req as any).user?.id || req.ip}`,\n  });\n\nexport const InternalServiceRateLimit = () => \n  RateLimit({\n    algorithm: 'token-bucket',\n    capacity: 1000,\n    refillRate: 10,\n    windowMs: 60000,\n    skipIf: (req) => req.headers['x-service-token'] === process.env.INTERNAL_SERVICE_TOKEN,\n    keyGenerator: (req) => `internal:${req.headers['x-service-name'] || 'unknown'}`,\n  });\n\n// Dynamic rate limiting based on user tier\nexport const TieredRateLimit = () => \n  RateLimit({\n    algorithm: 'sliding-window',\n    windowMs: 60000,\n    keyGenerator: (req) => {\n      const user = (req as any).user;\n      const tier = user?.plan || 'free';\n      return `tiered:${tier}:${user?.id || req.ip}`;\n    },\n    limit: 0, // Will be dynamically set\n  });\n"})}),"\n",(0,r.jsx)(n.h2,{id:"microservice-aware-rate-limiting",children:"Microservice-Aware Rate Limiting"}),"\n",(0,r.jsx)(n.p,{children:"Implement service-specific rate limiting strategies that work across distributed systems."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// services/distributed-rate-limit.service.ts\nimport { Injectable, Inject } from '@nestjs/common';\nimport { ClientProxy } from '@nestjs/microservices';\nimport { RedisService } from '@nestjs-modules/ioredis';\n\ninterface ServiceRateLimitConfig {\n  serviceName: string;\n  globalLimit: number;\n  perInstanceLimit: number;\n  windowMs: number;\n  coordinationStrategy: 'redis' | 'gossip' | 'leader-election';\n}\n\n@Injectable()\nexport class DistributedRateLimitService {\n  private instanceId: string;\n  \n  constructor(\n    private readonly redis: RedisService,\n    @Inject('MESSAGE_BROKER') private readonly messageBroker: ClientProxy\n  ) {\n    this.instanceId = `${process.env.POD_NAME || 'local'}-${Date.now()}`;\n  }\n\n  async checkServiceRateLimit(\n    config: ServiceRateLimitConfig,\n    clientKey: string\n  ): Promise<{\n    allowed: boolean;\n    globalUsage: number;\n    instanceUsage: number;\n    recommendation: 'allow' | 'deny' | 'throttle';\n  }> {\n    const script = `\n      local globalKey = KEYS[1]\n      local instanceKey = KEYS[2]\n      local globalLimit = tonumber(ARGV[1])\n      local perInstanceLimit = tonumber(ARGV[2])\n      local windowMs = tonumber(ARGV[3])\n      local now = tonumber(ARGV[4])\n      local instanceId = ARGV[5]\n      \n      -- Clean expired entries\n      local cutoff = now - windowMs\n      redis.call('ZREMRANGEBYSCORE', globalKey, 0, cutoff)\n      redis.call('ZREMRANGEBYSCORE', instanceKey, 0, cutoff)\n      \n      -- Get current usage\n      local globalUsage = redis.call('ZCARD', globalKey)\n      local instanceUsage = redis.call('ZCARD', instanceKey)\n      \n      -- Calculate available capacity\n      local globalAvailable = globalLimit - globalUsage\n      local instanceAvailable = perInstanceLimit - instanceUsage\n      \n      -- Determine if request should be allowed\n      local allowed = globalAvailable > 0 and instanceAvailable > 0\n      local recommendation = 'allow'\n      \n      if not allowed then\n        recommendation = 'deny'\n      elseif globalAvailable < globalLimit * 0.1 then\n        recommendation = 'throttle'\n      end\n      \n      if allowed then\n        -- Record the request\n        local requestId = instanceId .. ':' .. now .. ':' .. math.random(1000000)\n        redis.call('ZADD', globalKey, now, requestId)\n        redis.call('ZADD', instanceKey, now, requestId)\n        redis.call('EXPIRE', globalKey, math.ceil(windowMs / 1000) * 2)\n        redis.call('EXPIRE', instanceKey, math.ceil(windowMs / 1000) * 2)\n        \n        globalUsage = globalUsage + 1\n        instanceUsage = instanceUsage + 1\n      end\n      \n      return {\n        allowed and 1 or 0,\n        globalUsage,\n        instanceUsage,\n        recommendation\n      }\n    `;\n\n    const now = Date.now();\n    const globalKey = `service_rate_limit:${config.serviceName}:global:${clientKey}`;\n    const instanceKey = `service_rate_limit:${config.serviceName}:${this.instanceId}:${clientKey}`;\n\n    const result = await this.redis.eval(\n      script,\n      2,\n      globalKey,\n      instanceKey,\n      config.globalLimit,\n      config.perInstanceLimit,\n      config.windowMs,\n      now,\n      this.instanceId\n    ) as [number, number, number, string];\n\n    // Publish metrics for monitoring\n    await this.publishMetrics(config.serviceName, {\n      globalUsage: result[1],\n      instanceUsage: result[2],\n      recommendation: result[3],\n      timestamp: now,\n    });\n\n    return {\n      allowed: result[0] === 1,\n      globalUsage: result[1],\n      instanceUsage: result[2],\n      recommendation: result[3] as any,\n    };\n  }\n\n  private async publishMetrics(serviceName: string, metrics: any): Promise<void> {\n    try {\n      await this.messageBroker.emit('rate_limit.metrics', {\n        serviceName,\n        instanceId: this.instanceId,\n        ...metrics,\n      });\n    } catch (error) {\n      // Metrics publishing failure shouldn't affect rate limiting\n      console.warn('Failed to publish rate limit metrics:', error);\n    }\n  }\n\n  async getServiceHealth(serviceName: string): Promise<{\n    totalInstances: number;\n    healthyInstances: number;\n    globalUsage: number;\n    averageInstanceUsage: number;\n  }> {\n    const healthKey = `service_health:${serviceName}`;\n    const instances = await this.redis.hgetall(healthKey);\n    \n    const now = Date.now();\n    const healthyThreshold = 30000; // 30 seconds\n    \n    let healthyCount = 0;\n    let totalUsage = 0;\n    \n    for (const [instanceId, lastSeen] of Object.entries(instances)) {\n      if (now - parseInt(lastSeen) < healthyThreshold) {\n        healthyCount++;\n        \n        // Get instance usage\n        const usageKey = `service_rate_limit:${serviceName}:${instanceId}:*`;\n        const usageKeys = await this.redis.keys(usageKey);\n        for (const key of usageKeys) {\n          const usage = await this.redis.zcard(key);\n          totalUsage += usage;\n        }\n      }\n    }\n    \n    return {\n      totalInstances: Object.keys(instances).length,\n      healthyInstances: healthyCount,\n      globalUsage: totalUsage,\n      averageInstanceUsage: healthyCount > 0 ? totalUsage / healthyCount : 0,\n    };\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"production-ready-controller-implementation",children:"Production-Ready Controller Implementation"}),"\n",(0,r.jsx)(n.p,{children:"Here's how to apply rate limiting in real microservice controllers:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// controllers/api.controller.ts\nimport {\n  Controller,\n  Get,\n  Post,\n  Body,\n  UseGuards,\n  Req,\n  HttpStatus,\n  HttpException,\n} from '@nestjs/common';\nimport { Request } from 'express';\nimport { RateLimitGuard } from '../guards/rate-limit.guard';\nimport {\n  PublicApiRateLimit,\n  AuthRateLimit,\n  TieredRateLimit,\n  InternalServiceRateLimit,\n} from '../decorators/rate-limit.decorator';\n\n@Controller('api/v1')\n@UseGuards(RateLimitGuard)\nexport class ApiController {\n  \n  @Get('public/data')\n  @PublicApiRateLimit()\n  async getPublicData() {\n    return { data: 'This is public data with standard rate limiting' };\n  }\n\n  @Post('auth/login')\n  @AuthRateLimit()\n  async login(@Body() credentials: any, @Req() request: Request) {\n    // Implement authentication logic\n    return { token: 'jwt-token' };\n  }\n\n  @Get('user/profile')\n  @TieredRateLimit()\n  async getUserProfile(@Req() request: Request) {\n    // Dynamic rate limiting based on user tier\n    const user = (request as any).user;\n    \n    // This would be handled by a custom guard that sets limits based on user tier\n    return { profile: user.profile };\n  }\n\n  @Post('internal/sync')\n  @InternalServiceRateLimit()\n  async internalSync(@Body() data: any, @Req() request: Request) {\n    // Internal service communication with special rate limiting\n    return { status: 'synced' };\n  }\n\n  @Get('premium/analytics')\n  @RateLimit({\n    algorithm: 'sliding-window',\n    limit: 1000,\n    windowMs: 60000,\n    keyGenerator: (req) => {\n      const user = (req as any).user;\n      const tier = user?.plan || 'free';\n      \n      // Different limits for different tiers\n      const limits = {\n        free: 10,\n        pro: 100,\n        enterprise: 1000,\n      };\n      \n      return `analytics:${tier}:${user?.id || req.ip}`;\n    },\n    onLimitReached: (req, res) => {\n      // Custom handling for limit exceeded\n      const user = (req as any).user;\n      if (user?.plan === 'free') {\n        // Suggest upgrade for free users\n        res.setHeader('X-Upgrade-Suggestion', 'Consider upgrading to Pro for higher limits');\n      }\n    },\n  })\n  async getPremiumAnalytics(@Req() request: Request) {\n    const user = (request as any).user;\n    const tierLimits = {\n      free: 10,\n      pro: 100,\n      enterprise: 1000,\n    };\n    \n    const userLimit = tierLimits[user?.plan] || tierLimits.free;\n    \n    return {\n      analytics: 'Advanced analytics data',\n      currentPlan: user?.plan || 'free',\n      apiLimit: userLimit,\n    };\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"middleware-for-global-rate-limiting",children:"Middleware for Global Rate Limiting"}),"\n",(0,r.jsx)(n.p,{children:"Implement middleware for application-wide rate limiting with exemptions."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// middleware/global-rate-limit.middleware.ts\nimport { Injectable, NestMiddleware, HttpException, HttpStatus } from '@nestjs/common';\nimport { Request, Response, NextFunction } from 'express';\nimport { TokenBucketService } from '../rate-limiting/token-bucket.service';\n\n@Injectable()\nexport class GlobalRateLimitMiddleware implements NestMiddleware {\n  constructor(private readonly tokenBucket: TokenBucketService) {}\n\n  async use(req: Request, res: Response, next: NextFunction) {\n    // Skip rate limiting for certain paths\n    const exemptPaths = [\n      '/health',\n      '/metrics',\n      '/api/v1/internal',\n    ];\n\n    if (exemptPaths.some(path => req.path.startsWith(path))) {\n      return next();\n    }\n\n    // Skip for internal service calls\n    if (req.headers['x-service-token'] === process.env.INTERNAL_SERVICE_TOKEN) {\n      return next();\n    }\n\n    // Apply global rate limiting\n    const clientKey = this.generateClientKey(req);\n    const result = await this.tokenBucket.checkRateLimit(clientKey, {\n      capacity: 10000,     // 10k requests\n      refillRate: 100,     // 100 requests per second\n      windowSize: 3600,    // 1 hour window\n    });\n\n    // Set rate limit headers\n    res.setHeader('X-Global-RateLimit-Limit', '10000');\n    res.setHeader('X-Global-RateLimit-Remaining', result.remainingTokens);\n    res.setHeader('X-Global-RateLimit-Reset', result.resetTime);\n\n    if (!result.allowed) {\n      throw new HttpException(\n        {\n          statusCode: HttpStatus.TOO_MANY_REQUESTS,\n          message: 'Global rate limit exceeded',\n          error: 'Too Many Requests',\n          retryAfter: result.retryAfter,\n        },\n        HttpStatus.TOO_MANY_REQUESTS\n      );\n    }\n\n    next();\n  }\n\n  private generateClientKey(req: Request): string {\n    // Prioritize API key, then user ID, then IP\n    const apiKey = req.headers['x-api-key'] as string;\n    const userId = (req as any).user?.id;\n    const ip = req.ip || req.connection.remoteAddress;\n\n    if (apiKey) {\n      return `global:api:${apiKey}`;\n    }\n    if (userId) {\n      return `global:user:${userId}`;\n    }\n    return `global:ip:${ip}`;\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"configuration-and-module-setup",children:"Configuration and Module Setup"}),"\n",(0,r.jsx)(n.p,{children:"Set up the rate limiting module with proper configuration management."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// modules/rate-limit.module.ts\nimport { Module, Global } from '@nestjs/common';\nimport { ConfigModule, ConfigService } from '@nestjs/config';\nimport { RedisModule } from '@nestjs-modules/ioredis';\nimport { TokenBucketService } from '../rate-limiting/token-bucket.service';\nimport { SlidingWindowService } from '../rate-limiting/sliding-window.service';\nimport { DistributedRateLimitService } from '../services/distributed-rate-limit.service';\nimport { RateLimitGuard } from '../guards/rate-limit.guard';\n\n@Global()\n@Module({\n  imports: [\n    ConfigModule,\n    RedisModule.forRootAsync({\n      imports: [ConfigModule],\n      useFactory: (configService: ConfigService) => ({\n        config: {\n          host: configService.get('REDIS_HOST', 'localhost'),\n          port: configService.get('REDIS_PORT', 6379),\n          password: configService.get('REDIS_PASSWORD'),\n          db: configService.get('REDIS_DB', 0),\n          keyPrefix: configService.get('REDIS_KEY_PREFIX', 'rate_limit:'),\n          retryDelayOnFailover: 100,\n          enableReadyCheck: true,\n          maxRetriesPerRequest: 3,\n        },\n      }),\n      inject: [ConfigService],\n    }),\n  ],\n  providers: [\n    TokenBucketService,\n    SlidingWindowService,\n    DistributedRateLimitService,\n    RateLimitGuard,\n    {\n      provide: 'CONFIG_SERVICE',\n      useFactory: (configService: ConfigService) => configService,\n      inject: [ConfigService],\n    },\n  ],\n  exports: [\n    TokenBucketService,\n    SlidingWindowService,\n    DistributedRateLimitService,\n    RateLimitGuard,\n  ],\n})\nexport class RateLimitModule {}\n\n// config/rate-limit.config.ts\nexport const rateLimitConfig = () => ({\n  rateLimit: {\n    default: {\n      algorithm: 'sliding-window',\n      limit: 100,\n      windowMs: 60000,\n      standardHeaders: true,\n      legacyHeaders: false,\n    },\n    redis: {\n      host: process.env.REDIS_HOST || 'localhost',\n      port: parseInt(process.env.REDIS_PORT) || 6379,\n      password: process.env.REDIS_PASSWORD,\n      db: parseInt(process.env.REDIS_DB) || 0,\n      keyPrefix: process.env.REDIS_KEY_PREFIX || 'rate_limit:',\n    },\n    microservice: {\n      instanceId: process.env.POD_NAME || `local-${Date.now()}`,\n      serviceName: process.env.SERVICE_NAME || 'unknown-service',\n      coordinationStrategy: process.env.RATE_LIMIT_COORDINATION || 'redis',\n    },\n  },\n});\n"})}),"\n",(0,r.jsx)(n.h2,{id:"monitoring-and-observability",children:"Monitoring and Observability"}),"\n",(0,r.jsx)(n.p,{children:"Implement comprehensive monitoring for rate limiting performance."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// monitoring/rate-limit-metrics.service.ts\nimport { Injectable } from '@nestjs/common';\nimport { Gauge, Counter, Histogram, register } from 'prom-client';\n\n@Injectable()\nexport class RateLimitMetricsService {\n  private readonly requestsTotal: Counter<string>;\n  private readonly requestsBlocked: Counter<string>;\n  private readonly rateLimitUsage: Gauge<string>;\n  private readonly rateLimitLatency: Histogram<string>;\n\n  constructor() {\n    this.requestsTotal = new Counter({\n      name: 'rate_limit_requests_total',\n      help: 'Total number of requests processed by rate limiter',\n      labelNames: ['service', 'algorithm', 'key_type'],\n      registers: [register],\n    });\n\n    this.requestsBlocked = new Counter({\n      name: 'rate_limit_requests_blocked_total',\n      help: 'Total number of requests blocked by rate limiter',\n      labelNames: ['service', 'algorithm', 'key_type', 'reason'],\n      registers: [register],\n    });\n\n    this.rateLimitUsage = new Gauge({\n      name: 'rate_limit_usage_ratio',\n      help: 'Current usage ratio of rate limits (0-1)',\n      labelNames: ['service', 'algorithm', 'key_type'],\n      registers: [register],\n    });\n\n    this.rateLimitLatency = new Histogram({\n      name: 'rate_limit_check_duration_seconds',\n      help: 'Time spent checking rate limits',\n      labelNames: ['service', 'algorithm'],\n      buckets: [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0],\n      registers: [register],\n    });\n  }\n\n  recordRequest(service: string, algorithm: string, keyType: string): void {\n    this.requestsTotal.inc({ service, algorithm, key_type: keyType });\n  }\n\n  recordBlocked(\n    service: string, \n    algorithm: string, \n    keyType: string, \n    reason: string\n  ): void {\n    this.requestsBlocked.inc({ service, algorithm, key_type: keyType, reason });\n  }\n\n  recordUsage(\n    service: string, \n    algorithm: string, \n    keyType: string, \n    ratio: number\n  ): void {\n    this.rateLimitUsage.set({ service, algorithm, key_type: keyType }, ratio);\n  }\n\n  recordLatency(service: string, algorithm: string, duration: number): void {\n    this.rateLimitLatency.observe({ service, algorithm }, duration);\n  }\n\n  async getMetrics(): Promise<string> {\n    return register.metrics();\n  }\n}\n\n// Health check for rate limiting system\nimport { Injectable } from '@nestjs/common';\nimport { HealthIndicator, HealthIndicatorResult, HealthCheckError } from '@nestjs/terminus';\nimport { RedisService } from '@nestjs-modules/ioredis';\n\n@Injectable()\nexport class RateLimitHealthIndicator extends HealthIndicator {\n  constructor(private readonly redis: RedisService) {\n    super();\n  }\n\n  async isHealthy(key: string): Promise<HealthIndicatorResult> {\n    try {\n      // Test Redis connectivity\n      const start = Date.now();\n      await this.redis.ping();\n      const latency = Date.now() - start;\n\n      // Test rate limiting functionality\n      const testKey = `health_check:${Date.now()}`;\n      await this.redis.incr(testKey);\n      await this.redis.expire(testKey, 60);\n\n      const isHealthy = latency < 100; // Consider healthy if latency < 100ms\n\n      const result = this.getStatus(key, isHealthy, {\n        redis_latency: latency,\n        redis_status: 'connected',\n      });\n\n      if (isHealthy) {\n        return result;\n      }\n\n      throw new HealthCheckError('Rate limiting is unhealthy', result);\n    } catch (error) {\n      throw new HealthCheckError('Rate limiting check failed', {\n        [key]: {\n          status: 'down',\n          error: error.message,\n        },\n      });\n    }\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"testing-rate-limiting",children:"Testing Rate Limiting"}),"\n",(0,r.jsx)(n.p,{children:"Comprehensive testing strategies for rate limiting functionality."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// tests/rate-limit.integration.spec.ts\nimport { Test, TestingModule } from '@nestjs/testing';\nimport { INestApplication } from '@nestjs/common';\nimport { RedisService } from '@nestjs-modules/ioredis';\nimport * as request from 'supertest';\nimport { AppModule } from '../src/app.module';\n\ndescribe('Rate Limiting Integration', () => {\n  let app: INestApplication;\n  let redis: RedisService;\n\n  beforeEach(async () => {\n    const moduleFixture: TestingModule = await Test.createTestingModule({\n      imports: [AppModule],\n    }).compile();\n\n    app = moduleFixture.createNestApplication();\n    redis = moduleFixture.get<RedisService>(RedisService);\n    \n    await app.init();\n    \n    // Clear Redis before each test\n    await redis.flushdb();\n  });\n\n  afterEach(async () => {\n    await app.close();\n  });\n\n  describe('Token Bucket Rate Limiting', () => {\n    it('should allow requests within capacity', async () => {\n      const endpoint = '/api/v1/auth/login';\n      \n      // Should allow first 5 requests (capacity)\n      for (let i = 0; i < 5; i++) {\n        const response = await request(app.getHttpServer())\n          .post(endpoint)\n          .send({ email: 'test@example.com', password: 'password' })\n          .expect(200);\n          \n        expect(response.headers['ratelimit-remaining']).toBeDefined();\n      }\n    });\n\n    it('should block requests exceeding capacity', async () => {\n      const endpoint = '/api/v1/auth/login';\n      const email = 'test@example.com';\n      \n      // Exhaust the token bucket\n      for (let i = 0; i < 5; i++) {\n        await request(app.getHttpServer())\n          .post(endpoint)\n          .send({ email, password: 'password' });\n      }\n      \n      // Next request should be blocked\n      const response = await request(app.getHttpServer())\n        .post(endpoint)\n        .send({ email, password: 'password' })\n        .expect(429);\n        \n      expect(response.body.message).toContain('Too many authentication attempts');\n      expect(response.headers['retry-after']).toBeDefined();\n    });\n\n    it('should refill tokens over time', async () => {\n      const endpoint = '/api/v1/auth/login';\n      \n      // Exhaust capacity\n      for (let i = 0; i < 5; i++) {\n        await request(app.getHttpServer())\n          .post(endpoint)\n          .send({ email: 'test@example.com', password: 'password' });\n      }\n      \n      // Wait for token refill (mock time or use actual delay)\n      await new Promise(resolve => setTimeout(resolve, 60000)); // 1 minute\n      \n      // Should allow request after refill\n      await request(app.getHttpServer())\n        .post(endpoint)\n        .send({ email: 'test@example.com', password: 'password' })\n        .expect(200);\n    });\n  });\n\n  describe('Sliding Window Rate Limiting', () => {\n    it('should track requests in sliding window', async () => {\n      const endpoint = '/api/v1/public/data';\n      \n      // Make requests at different times within window\n      const responses = [];\n      for (let i = 0; i < 10; i++) {\n        const response = await request(app.getHttpServer())\n          .get(endpoint)\n          .expect(200);\n        responses.push(response);\n        \n        // Small delay between requests\n        await new Promise(resolve => setTimeout(resolve, 100));\n      }\n      \n      // Check that remaining count decreases properly\n      responses.forEach((response, index) => {\n        const remaining = parseInt(response.headers['ratelimit-remaining']);\n        expect(remaining).toBe(1000 - index - 1); // Assuming limit of 1000\n      });\n    });\n  });\n\n  describe('Distributed Rate Limiting', () => {\n    it('should coordinate limits across service instances', async () => {\n      // This would require multiple app instances for proper testing\n      // Mock multiple instances using different instance IDs\n      \n      const endpoint = '/api/v1/premium/analytics';\n      const userId = 'test-user-123';\n      \n      // Simulate requests from different instances\n      for (let instance = 0; instance < 3; instance++) {\n        // Mock different instance IDs\n        process.env.POD_NAME = `test-pod-${instance}`;\n        \n        const response = await request(app.getHttpServer())\n          .get(endpoint)\n          .set('Authorization', `Bearer ${generateJWT({ id: userId })}`)\n          .expect(200);\n          \n        expect(response.headers['x-global-usage']).toBeDefined();\n      }\n    });\n  });\n\n  describe('Rate Limit Headers', () => {\n    it('should return correct rate limit headers', async () => {\n      const response = await request(app.getHttpServer())\n        .get('/api/v1/public/data')\n        .expect(200);\n        \n      expect(response.headers['ratelimit-limit']).toBeDefined();\n      expect(response.headers['ratelimit-remaining']).toBeDefined();\n      expect(response.headers['ratelimit-reset']).toBeDefined();\n    });\n\n    it('should return retry-after header when rate limited', async () => {\n      const endpoint = '/api/v1/auth/login';\n      \n      // Exhaust rate limit\n      for (let i = 0; i < 5; i++) {\n        await request(app.getHttpServer())\n          .post(endpoint)\n          .send({ email: 'test@example.com', password: 'password' });\n      }\n      \n      const response = await request(app.getHttpServer())\n        .post(endpoint)\n        .send({ email: 'test@example.com', password: 'password' })\n        .expect(429);\n        \n      expect(response.headers['retry-after']).toBeDefined();\n      expect(parseInt(response.headers['retry-after'])).toBeGreaterThan(0);\n    });\n  });\n});\n\n// Load testing for rate limiting\ndescribe('Rate Limiting Load Tests', () => {\n  it('should handle high concurrent load', async () => {\n    const concurrentRequests = 100;\n    const endpoint = '/api/v1/public/data';\n    \n    const promises = Array(concurrentRequests)\n      .fill(null)\n      .map(() => \n        request(app.getHttpServer())\n          .get(endpoint)\n      );\n    \n    const results = await Promise.allSettled(promises);\n    \n    const successful = results.filter(r => \n      r.status === 'fulfilled' && r.value.status === 200\n    ).length;\n    \n    const rateLimited = results.filter(r => \n      r.status === 'fulfilled' && r.value.status === 429\n    ).length;\n    \n    expect(successful + rateLimited).toBe(concurrentRequests);\n    expect(successful).toBeLessThanOrEqual(1000); // Assuming limit of 1000\n  });\n});\n\nfunction generateJWT(payload: any): string {\n  // Mock JWT generation for testing\n  return 'mock.jwt.token';\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"best-practices-and-production-considerations",children:"Best Practices and Production Considerations"}),"\n",(0,r.jsxs)(n.h3,{id:"-configuration-management",children:["\ud83d\udd27 ",(0,r.jsx)(n.strong,{children:"Configuration Management"})]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Environment-Specific Limits"}),": Use different rate limits for development, staging, and production"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dynamic Configuration"}),": Allow runtime adjustment of rate limits without deployment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Graceful Degradation"}),": Fall back to in-memory limiting if Redis is unavailable"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Circuit Breakers"}),": Implement circuit breakers for rate limiting services"]}),"\n"]}),"\n",(0,r.jsxs)(n.h3,{id:"-monitoring-and-alerting",children:["\ud83d\udcca ",(0,r.jsx)(n.strong,{children:"Monitoring and Alerting"})]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Metrics Collection"}),": Track rate limit usage, blocked requests, and system health"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Alerting"}),": Set up alerts for high block rates or system failures"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dashboard"}),": Create monitoring dashboards for real-time rate limit visibility"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Log Analysis"}),": Analyze logs to identify patterns and potential abuse"]}),"\n"]}),"\n",(0,r.jsxs)(n.h3,{id:"-performance-optimization",children:["\u26a1 ",(0,r.jsx)(n.strong,{children:"Performance Optimization"})]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Redis Optimization"}),": Use Redis clusters for high availability and performance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lua Scripts"}),": Leverage atomic Lua scripts for consistent rate limiting"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Connection Pooling"}),": Optimize Redis connection management"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Caching"}),": Cache rate limit decisions for frequently accessed keys"]}),"\n"]}),"\n",(0,r.jsxs)(n.h3,{id:"\ufe0f-security-considerations",children:["\ud83d\udee1\ufe0f ",(0,r.jsx)(n.strong,{children:"Security Considerations"})]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Key Generation"}),": Use secure, unpredictable key generation strategies"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"API Key Management"}),": Implement proper API key validation and rotation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Bypass Protection"}),": Secure internal service communication channels"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audit Logging"}),": Log all rate limit decisions for security auditing"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.p,{children:"Implementing robust rate limiting in NestJS microservices requires careful consideration of algorithms, distribution strategies, and production requirements. The solutions presented here provide:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multiple Rate Limiting Algorithms"}),": Token bucket, sliding window, and fixed window implementations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Distributed Coordination"}),": Service-aware rate limiting across multiple instances"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Flexible Configuration"}),": Decorator-based and middleware approaches for different use cases"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Production Readiness"}),": Monitoring, health checks, and comprehensive testing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance Optimization"}),": Redis-based storage with efficient Lua scripts"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Start with simple rate limiting for critical endpoints, gradually expanding to more sophisticated distributed strategies as your microservice architecture grows. Always monitor rate limiting effectiveness and adjust limits based on actual usage patterns and business requirements."}),"\n",(0,r.jsx)(n.p,{children:"Remember that rate limiting is just one part of a comprehensive security and performance strategy. Combine it with other techniques like authentication, authorization, caching, and circuit breakers for maximum effectiveness in production environments."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"Ready to implement rate limiting? This comprehensive guide provides everything you need to build robust rate limiting for your NestJS microservices architecture."})})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},3863:e=>{e.exports=JSON.parse('{"permalink":"/fullstack-dev/blog/nestjs-rate-limiting-microservices","editUrl":"https://github.com/tamnk74/fullstack-dev/tree/main/docs-site/blog/2025-10-05-nestjs-rate-limiting-microservices.md","source":"@site/blog/2025-10-05-nestjs-rate-limiting-microservices.md","title":"Handling Rate Limiting in NestJS for Production Microservices","description":"Rate limiting is a critical security and performance mechanism in production microservice architectures. It protects your services from abuse, ensures fair resource allocation, and maintains system stability under high load. In this comprehensive guide, we\'ll explore how to implement robust, scalable rate limiting in NestJS applications designed for microservice environments.","date":"2025-10-05T00:00:00.000Z","tags":[{"inline":false,"label":"NestJS","permalink":"/fullstack-dev/blog/tags/nestjs","description":"NestJS framework for Node.js applications"},{"inline":false,"label":"Microservices","permalink":"/fullstack-dev/blog/tags/microservices","description":"Microservices architecture and patterns"},{"inline":false,"label":"Rate Limiting","permalink":"/fullstack-dev/blog/tags/rate-limiting","description":"Rate limiting and throttling techniques"},{"inline":false,"label":"Security","permalink":"/fullstack-dev/blog/tags/security","description":"Application security practices"},{"inline":false,"label":"Performance","permalink":"/fullstack-dev/blog/tags/performance","description":"Performance optimization techniques"},{"inline":false,"label":"Redis","permalink":"/fullstack-dev/blog/tags/redis","description":"Redis caching and data storage"},{"inline":false,"label":"Production","permalink":"/fullstack-dev/blog/tags/production","description":"Production-ready development and deployment"}],"readingTime":25.63,"hasTruncateMarker":true,"authors":[{"name":"Tam Nguyen","title":"Full Stack Developer, Next.js for Production Creator","url":"https://github.com/tamnk74","page":{"permalink":"/fullstack-dev/blog/authors/tam"},"socials":{"github":"https://github.com/tamnk74"},"imageURL":"https://github.com/tamnk74.png","key":"tam"}],"frontMatter":{"slug":"nestjs-rate-limiting-microservices","title":"Handling Rate Limiting in NestJS for Production Microservices","authors":["tam"],"tags":["nestjs","microservices","rate-limiting","security","performance","redis","production"],"date":"2025-10-05T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Building an Interactive Tour Guide for New Features - Full-Stack Developer Guide","permalink":"/fullstack-dev/blog/building-interactive-tour-guide"},"nextItem":{"title":"Elasticsearch Full-Text Search Setup","permalink":"/fullstack-dev/blog/ELASTICSEARCH_SETUP"}}')},5741:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var i=t(9729);const r={},s=i.createContext(r);function o(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);