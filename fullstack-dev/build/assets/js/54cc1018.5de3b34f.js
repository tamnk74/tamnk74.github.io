"use strict";(self.webpackChunkdocs_site=self.webpackChunkdocs_site||[]).push([[6430],{4404:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>l,frontMatter:()=>o,metadata:()=>i,toc:()=>u});const i=JSON.parse('{"id":"architecture-practices/architecture-execution/database-migrations","title":"Database Migration Management Guide","description":"This guide provides comprehensive database migration strategies for NestJS microservices applications, covering schema versioning, zero-downtime migrations, rollback procedures, and data migration best practices for PostgreSQL and other databases.","source":"@site/docs/architecture-practices/architecture-execution/database-migrations.md","sourceDirName":"architecture-practices/architecture-execution","slug":"/architecture-practices/architecture-execution/database-migrations","permalink":"/fullstack-dev/docs/architecture-practices/architecture-execution/database-migrations","draft":false,"unlisted":false,"editUrl":"https://github.com/tamnk74/fullstack-dev/tree/main/docs-site/docs/architecture-practices/architecture-execution/database-migrations.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Automated Deployment Guide","permalink":"/fullstack-dev/docs/architecture-practices/architecture-execution/automated-deployment"},"next":{"title":"Performance Testing Guide","permalink":"/fullstack-dev/docs/architecture-practices/architecture-execution/performance-testing"}}');var r=t(5813),a=t(7814);const o={},s="Database Migration Management Guide",c={},u=[{value:"Overview",id:"overview",level:2},{value:"Migration Architecture",id:"migration-architecture",level:2},{value:"Migration Pipeline Overview",id:"migration-pipeline-overview",level:3},{value:"TypeORM Migration Setup",id:"typeorm-migration-setup",level:2},{value:"Migration Configuration",id:"migration-configuration",level:3},{value:"Migration Service",id:"migration-service",level:3},{value:"Advanced Migration Patterns",id:"advanced-migration-patterns",level:3},{value:"Migration Testing",id:"migration-testing",level:2},{value:"Migration Test Suite",id:"migration-test-suite",level:3},{value:"Automated Migration Pipeline",id:"automated-migration-pipeline",level:2},{value:"Migration CI/CD Script",id:"migration-cicd-script",level:3},{value:"Migration GitHub Action",id:"migration-github-action",level:3},{value:"Migration Monitoring",id:"migration-monitoring",level:2},{value:"Migration Metrics Collection",id:"migration-metrics-collection",level:3},{value:"Related Documentation",id:"related-documentation",level:2}];function g(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"database-migration-management-guide",children:"Database Migration Management Guide"})}),"\n",(0,r.jsx)(e.p,{children:"This guide provides comprehensive database migration strategies for NestJS microservices applications, covering schema versioning, zero-downtime migrations, rollback procedures, and data migration best practices for PostgreSQL and other databases."}),"\n",(0,r.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(e.p,{children:"Database migration management ensures consistent, reliable, and safe evolution of database schemas across environments. This guide covers migration strategies, version control, automated migration pipelines, and data integrity practices for microservices architectures."}),"\n",(0,r.jsx)(e.h2,{id:"migration-architecture",children:"Migration Architecture"}),"\n",(0,r.jsx)(e.h3,{id:"migration-pipeline-overview",children:"Migration Pipeline Overview"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-mermaid",children:"graph TD\n    A[Developer Changes] --\x3e B[Migration Scripts]\n    B --\x3e C[Local Testing]\n    C --\x3e D[Migration Review]\n    D --\x3e E[Version Control]\n    \n    E --\x3e F[CI/CD Pipeline]\n    F --\x3e G[Migration Validation]\n    G --\x3e H[Staging Migration]\n    H --\x3e I[Production Migration]\n    \n    J[Schema Versioning] --\x3e K[Migration History]\n    K --\x3e L[Rollback Plans]\n    L --\x3e M[Data Backup]\n    \n    N[Zero-Downtime Strategy] --\x3e O[Blue-Green Migration]\n    N --\x3e P[Rolling Migration]\n    N --\x3e Q[Shadow Tables]\n    \n    B --\x3e J\n    H --\x3e N\n    I --\x3e N\n    \n    R[Monitoring] --\x3e S[Migration Metrics]\n    S --\x3e T[Performance Impact]\n    T --\x3e U[Rollback Triggers]\n    \n    I --\x3e R\n"})}),"\n",(0,r.jsx)(e.h2,{id:"typeorm-migration-setup",children:"TypeORM Migration Setup"}),"\n",(0,r.jsx)(e.h3,{id:"migration-configuration",children:"Migration Configuration"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-typescript",children:"// src/config/migration.config.ts\nimport { DataSource } from 'typeorm';\nimport { ConfigService } from '@nestjs/config';\nimport { config } from 'dotenv';\n\nconfig();\n\nconst configService = new ConfigService();\n\nexport const migrationDataSource = new DataSource({\n  type: 'postgres',\n  host: configService.get('DATABASE_HOST'),\n  port: configService.get('DATABASE_PORT'),\n  username: configService.get('DATABASE_USERNAME'),\n  password: configService.get('DATABASE_PASSWORD'),\n  database: configService.get('DATABASE_NAME'),\n  ssl: configService.get('DATABASE_SSL') === 'true' ? {\n    rejectUnauthorized: false,\n  } : false,\n  \n  // Migration-specific settings\n  entities: ['src/**/*.entity.ts'],\n  migrations: ['src/migrations/*.ts'],\n  subscribers: ['src/**/*.subscriber.ts'],\n  \n  // Migration table configuration\n  migrationsTableName: 'migrations_history',\n  migrationsRun: false, // Don't auto-run migrations\n  \n  // Enable logging for migrations\n  logging: ['query', 'error', 'schema', 'warn', 'info', 'log', 'migration'],\n  \n  // Connection pool settings for migrations\n  extra: {\n    connectionLimit: 1, // Single connection for migrations\n    acquireTimeout: 60000,\n    timeout: 60000,\n  },\n});\n"})}),"\n",(0,r.jsx)(e.h3,{id:"migration-service",children:"Migration Service"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-typescript",children:"// src/migration/migration.service.ts\nimport { Injectable, Logger, OnModuleInit } from '@nestjs/common';\nimport { DataSource } from 'typeorm';\nimport { InjectDataSource } from '@nestjs/typeorm';\nimport { ConfigService } from '@nestjs/config';\nimport * as fs from 'fs/promises';\nimport * as path from 'path';\n\nexport interface MigrationInfo {\n  id: number;\n  timestamp: number;\n  name: string;\n  executedAt: Date;\n  executionTime: number;\n}\n\nexport interface MigrationPlan {\n  pending: MigrationInfo[];\n  executed: MigrationInfo[];\n  canRollback: boolean;\n  rollbackTarget?: MigrationInfo;\n}\n\n@Injectable()\nexport class MigrationService implements OnModuleInit {\n  private readonly logger = new Logger(MigrationService.name);\n\n  constructor(\n    @InjectDataSource()\n    private readonly dataSource: DataSource,\n    private readonly configService: ConfigService,\n  ) {}\n\n  async onModuleInit(): Promise<void> {\n    if (this.configService.get('AUTO_RUN_MIGRATIONS') === 'true') {\n      await this.runPendingMigrations();\n    }\n  }\n\n  async getMigrationPlan(): Promise<MigrationPlan> {\n    const queryRunner = this.dataSource.createQueryRunner();\n    \n    try {\n      // Get executed migrations\n      const executedMigrations = await queryRunner.query(`\n        SELECT id, timestamp, name, \"executedAt\", \"executionTime\"\n        FROM migrations_history \n        ORDER BY timestamp ASC\n      `);\n\n      // Get all available migrations\n      const allMigrations = this.dataSource.migrations.map(migration => ({\n        id: 0, // Will be set when executed\n        timestamp: parseInt(migration.name.split('-')[0]),\n        name: migration.name,\n        executedAt: null,\n        executionTime: 0,\n      }));\n\n      // Determine pending migrations\n      const executedTimestamps = new Set(executedMigrations.map(m => m.timestamp));\n      const pendingMigrations = allMigrations.filter(\n        m => !executedTimestamps.has(m.timestamp)\n      );\n\n      return {\n        pending: pendingMigrations,\n        executed: executedMigrations,\n        canRollback: executedMigrations.length > 0,\n        rollbackTarget: executedMigrations[executedMigrations.length - 1],\n      };\n    } finally {\n      await queryRunner.release();\n    }\n  }\n\n  async runPendingMigrations(dryRun = false): Promise<{\n    success: boolean;\n    migrationsRun: string[];\n    errors: string[];\n    executionTime: number;\n  }> {\n    const startTime = Date.now();\n    const migrationsRun: string[] = [];\n    const errors: string[] = [];\n\n    try {\n      const plan = await this.getMigrationPlan();\n      \n      if (plan.pending.length === 0) {\n        this.logger.log('No pending migrations to run');\n        return {\n          success: true,\n          migrationsRun: [],\n          errors: [],\n          executionTime: Date.now() - startTime,\n        };\n      }\n\n      this.logger.log(`Found ${plan.pending.length} pending migrations`);\n\n      if (dryRun) {\n        this.logger.log('DRY RUN: Would execute the following migrations:');\n        plan.pending.forEach(migration => {\n          this.logger.log(`- ${migration.name}`);\n        });\n        return {\n          success: true,\n          migrationsRun: plan.pending.map(m => m.name),\n          errors: [],\n          executionTime: Date.now() - startTime,\n        };\n      }\n\n      // Create backup before migration\n      await this.createBackup(`pre-migration-${Date.now()}`);\n\n      // Run migrations\n      await this.dataSource.runMigrations({\n        transaction: 'each', // Run each migration in its own transaction\n      });\n\n      // Validate migrations\n      const postPlan = await this.getMigrationPlan();\n      if (postPlan.pending.length > 0) {\n        throw new Error(`${postPlan.pending.length} migrations still pending after execution`);\n      }\n\n      plan.pending.forEach(migration => {\n        migrationsRun.push(migration.name);\n        this.logger.log(`Successfully executed migration: ${migration.name}`);\n      });\n\n      return {\n        success: true,\n        migrationsRun,\n        errors,\n        executionTime: Date.now() - startTime,\n      };\n    } catch (error) {\n      this.logger.error('Migration failed', error);\n      errors.push(error.message);\n\n      // Attempt to rollback on failure\n      try {\n        await this.rollbackLastMigration();\n        this.logger.log('Rollback completed successfully');\n      } catch (rollbackError) {\n        this.logger.error('Rollback failed', rollbackError);\n        errors.push(`Rollback failed: ${rollbackError.message}`);\n      }\n\n      return {\n        success: false,\n        migrationsRun,\n        errors,\n        executionTime: Date.now() - startTime,\n      };\n    }\n  }\n\n  async rollbackLastMigration(): Promise<void> {\n    try {\n      await this.dataSource.undoLastMigration({\n        transaction: 'each',\n      });\n      this.logger.log('Last migration rolled back successfully');\n    } catch (error) {\n      this.logger.error('Failed to rollback last migration', error);\n      throw error;\n    }\n  }\n\n  async rollbackToMigration(targetMigrationName: string): Promise<void> {\n    const plan = await this.getMigrationPlan();\n    const targetMigration = plan.executed.find(m => m.name === targetMigrationName);\n    \n    if (!targetMigration) {\n      throw new Error(`Migration ${targetMigrationName} not found in executed migrations`);\n    }\n\n    // Create backup before rollback\n    await this.createBackup(`pre-rollback-${Date.now()}`);\n\n    // Rollback migrations one by one until target\n    const migrationsToRollback = plan.executed\n      .filter(m => m.timestamp > targetMigration.timestamp)\n      .reverse(); // Rollback in reverse order\n\n    for (const migration of migrationsToRollback) {\n      try {\n        await this.dataSource.undoLastMigration({\n          transaction: 'each',\n        });\n        this.logger.log(`Rolled back migration: ${migration.name}`);\n      } catch (error) {\n        this.logger.error(`Failed to rollback migration: ${migration.name}`, error);\n        throw error;\n      }\n    }\n  }\n\n  async createBackup(backupName: string): Promise<void> {\n    const dbConfig = {\n      host: this.configService.get('DATABASE_HOST'),\n      port: this.configService.get('DATABASE_PORT'),\n      database: this.configService.get('DATABASE_NAME'),\n      username: this.configService.get('DATABASE_USERNAME'),\n      password: this.configService.get('DATABASE_PASSWORD'),\n    };\n\n    const backupPath = path.join(process.cwd(), 'backups', `${backupName}.sql`);\n    \n    // Ensure backup directory exists\n    await fs.mkdir(path.dirname(backupPath), { recursive: true });\n\n    // Create pg_dump command\n    const pgDumpCommand = [\n      'pg_dump',\n      `--host=${dbConfig.host}`,\n      `--port=${dbConfig.port}`,\n      `--username=${dbConfig.username}`,\n      '--no-password',\n      '--verbose',\n      '--clean',\n      '--if-exists',\n      '--create',\n      '--format=custom',\n      `--file=${backupPath}`,\n      dbConfig.database,\n    ].join(' ');\n\n    // Set PGPASSWORD environment variable\n    process.env.PGPASSWORD = dbConfig.password;\n\n    try {\n      const { exec } = await import('child_process');\n      const { promisify } = await import('util');\n      const execAsync = promisify(exec);\n\n      await execAsync(pgDumpCommand);\n      this.logger.log(`Database backup created: ${backupPath}`);\n    } catch (error) {\n      this.logger.error('Failed to create database backup', error);\n      throw error;\n    } finally {\n      delete process.env.PGPASSWORD;\n    }\n  }\n\n  async validateMigration(migrationName: string): Promise<{\n    isValid: boolean;\n    issues: string[];\n  }> {\n    const issues: string[] = [];\n\n    try {\n      // Read migration file\n      const migrationPath = path.join(process.cwd(), 'src/migrations', `${migrationName}.ts`);\n      const migrationContent = await fs.readFile(migrationPath, 'utf-8');\n\n      // Check for common migration issues\n      const commonIssues = [\n        {\n          pattern: /DROP TABLE/gi,\n          message: 'DROP TABLE operations can cause data loss',\n        },\n        {\n          pattern: /ALTER TABLE.*DROP COLUMN/gi,\n          message: 'DROP COLUMN operations can cause data loss',\n        },\n        {\n          pattern: /ALTER TABLE.*RENAME COLUMN/gi,\n          message: 'RENAME COLUMN may require application changes',\n        },\n        {\n          pattern: /CREATE UNIQUE INDEX/gi,\n          message: 'Creating unique indexes on existing data may fail',\n        },\n        {\n          pattern: /ALTER TABLE.*ADD CONSTRAINT.*NOT NULL/gi,\n          message: 'Adding NOT NULL constraints to existing columns may fail',\n        },\n      ];\n\n      commonIssues.forEach(({ pattern, message }) => {\n        if (pattern.test(migrationContent)) {\n          issues.push(message);\n        }\n      });\n\n      // Check for transaction handling\n      if (!migrationContent.includes('transaction')) {\n        issues.push('Migration should be wrapped in a transaction');\n      }\n\n      // Check for rollback method\n      if (!migrationContent.includes('down(')) {\n        issues.push('Migration should include a down() method for rollback');\n      }\n\n      return {\n        isValid: issues.length === 0,\n        issues,\n      };\n    } catch (error) {\n      return {\n        isValid: false,\n        issues: [`Failed to validate migration: ${error.message}`],\n      };\n    }\n  }\n\n  async generateMigrationReport(): Promise<{\n    totalMigrations: number;\n    executedMigrations: number;\n    pendingMigrations: number;\n    lastMigration: MigrationInfo | null;\n    migrationHistory: MigrationInfo[];\n    averageExecutionTime: number;\n  }> {\n    const plan = await this.getMigrationPlan();\n    \n    const averageExecutionTime = plan.executed.length > 0\n      ? plan.executed.reduce((sum, m) => sum + m.executionTime, 0) / plan.executed.length\n      : 0;\n\n    return {\n      totalMigrations: plan.executed.length + plan.pending.length,\n      executedMigrations: plan.executed.length,\n      pendingMigrations: plan.pending.length,\n      lastMigration: plan.executed[plan.executed.length - 1] || null,\n      migrationHistory: plan.executed,\n      averageExecutionTime,\n    };\n  }\n}\n"})}),"\n",(0,r.jsx)(e.h3,{id:"advanced-migration-patterns",children:"Advanced Migration Patterns"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-typescript",children:"// src/migrations/example-migrations.ts\nimport { MigrationInterface, QueryRunner, Table, Index, Column } from 'typeorm';\n\n// Example: Safe column addition with default value\nexport class AddUserStatusColumn1640995200000 implements MigrationInterface {\n  name = 'AddUserStatusColumn1640995200000';\n\n  public async up(queryRunner: QueryRunner): Promise<void> {\n    // Step 1: Add column with default value\n    await queryRunner.addColumn('users', new Column({\n      name: 'status',\n      type: 'varchar',\n      length: '20',\n      default: \"'active'\",\n      isNullable: false,\n    }));\n\n    // Step 2: Update existing records (if needed)\n    await queryRunner.query(`\n      UPDATE users \n      SET status = 'active' \n      WHERE status IS NULL OR status = ''\n    `);\n\n    // Step 3: Add index for performance\n    await queryRunner.createIndex('users', new Index('IDX_users_status', ['status']));\n  }\n\n  public async down(queryRunner: QueryRunner): Promise<void> {\n    await queryRunner.dropIndex('users', 'IDX_users_status');\n    await queryRunner.dropColumn('users', 'status');\n  }\n}\n\n// Example: Zero-downtime table restructuring\nexport class RestructureProductsTable1641081600000 implements MigrationInterface {\n  name = 'RestructureProductsTable1641081600000';\n\n  public async up(queryRunner: QueryRunner): Promise<void> {\n    // Create new table structure\n    await queryRunner.createTable(new Table({\n      name: 'products_new',\n      columns: [\n        {\n          name: 'id',\n          type: 'uuid',\n          isPrimary: true,\n          generationStrategy: 'uuid',\n          default: 'gen_random_uuid()',\n        },\n        {\n          name: 'name',\n          type: 'varchar',\n          length: '255',\n          isNullable: false,\n        },\n        {\n          name: 'description',\n          type: 'text',\n          isNullable: true,\n        },\n        {\n          name: 'price',\n          type: 'decimal',\n          precision: 10,\n          scale: 2,\n          isNullable: false,\n        },\n        {\n          name: 'category_id',\n          type: 'uuid',\n          isNullable: false,\n        },\n        {\n          name: 'created_at',\n          type: 'timestamp',\n          default: 'CURRENT_TIMESTAMP',\n        },\n        {\n          name: 'updated_at',\n          type: 'timestamp',\n          default: 'CURRENT_TIMESTAMP',\n        },\n      ],\n    }));\n\n    // Copy data from old table to new table\n    await queryRunner.query(`\n      INSERT INTO products_new (id, name, description, price, category_id, created_at, updated_at)\n      SELECT \n        id, \n        name, \n        description, \n        price::decimal(10,2),\n        category_id,\n        created_at,\n        updated_at\n      FROM products\n    `);\n\n    // Create indexes on new table\n    await queryRunner.createIndex('products_new', new Index('IDX_products_new_category', ['category_id']));\n    await queryRunner.createIndex('products_new', new Index('IDX_products_new_name', ['name']));\n\n    // Atomic table swap (requires coordination with application deployment)\n    await queryRunner.query('BEGIN');\n    try {\n      await queryRunner.renameTable('products', 'products_old');\n      await queryRunner.renameTable('products_new', 'products');\n      await queryRunner.query('COMMIT');\n    } catch (error) {\n      await queryRunner.query('ROLLBACK');\n      throw error;\n    }\n  }\n\n  public async down(queryRunner: QueryRunner): Promise<void> {\n    // Rollback table swap\n    await queryRunner.query('BEGIN');\n    try {\n      await queryRunner.renameTable('products', 'products_new');\n      await queryRunner.renameTable('products_old', 'products');\n      await queryRunner.query('COMMIT');\n    } catch (error) {\n      await queryRunner.query('ROLLBACK');\n      throw error;\n    }\n\n    // Clean up\n    await queryRunner.dropTable('products_new');\n  }\n}\n\n// Example: Data migration with batch processing\nexport class MigrateUserPreferences1641168000000 implements MigrationInterface {\n  name = 'MigrateUserPreferences1641168000000';\n\n  public async up(queryRunner: QueryRunner): Promise<void> {\n    // Create new preferences table\n    await queryRunner.createTable(new Table({\n      name: 'user_preferences',\n      columns: [\n        {\n          name: 'id',\n          type: 'uuid',\n          isPrimary: true,\n          generationStrategy: 'uuid',\n          default: 'gen_random_uuid()',\n        },\n        {\n          name: 'user_id',\n          type: 'uuid',\n          isNullable: false,\n        },\n        {\n          name: 'preference_key',\n          type: 'varchar',\n          length: '100',\n          isNullable: false,\n        },\n        {\n          name: 'preference_value',\n          type: 'jsonb',\n          isNullable: true,\n        },\n        {\n          name: 'created_at',\n          type: 'timestamp',\n          default: 'CURRENT_TIMESTAMP',\n        },\n      ],\n    }));\n\n    // Migrate data in batches to avoid memory issues\n    const batchSize = 1000;\n    let offset = 0;\n    let hasMoreData = true;\n\n    while (hasMoreData) {\n      const users = await queryRunner.query(`\n        SELECT id, settings \n        FROM users \n        WHERE settings IS NOT NULL \n        ORDER BY id\n        LIMIT $1 OFFSET $2\n      `, [batchSize, offset]);\n\n      if (users.length === 0) {\n        hasMoreData = false;\n        break;\n      }\n\n      // Transform and insert preferences\n      for (const user of users) {\n        const settings = JSON.parse(user.settings || '{}');\n        \n        for (const [key, value] of Object.entries(settings)) {\n          await queryRunner.query(`\n            INSERT INTO user_preferences (user_id, preference_key, preference_value)\n            VALUES ($1, $2, $3)\n          `, [user.id, key, JSON.stringify(value)]);\n        }\n      }\n\n      offset += batchSize;\n      \n      // Log progress\n      console.log(`Migrated preferences for ${offset} users`);\n    }\n\n    // Create indexes\n    await queryRunner.createIndex('user_preferences', \n      new Index('IDX_user_preferences_user_key', ['user_id', 'preference_key'], { isUnique: true })\n    );\n  }\n\n  public async down(queryRunner: QueryRunner): Promise<void> {\n    await queryRunner.dropTable('user_preferences');\n  }\n}\n"})}),"\n",(0,r.jsx)(e.h2,{id:"migration-testing",children:"Migration Testing"}),"\n",(0,r.jsx)(e.h3,{id:"migration-test-suite",children:"Migration Test Suite"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-typescript",children:"// src/migration/migration.spec.ts\nimport { Test } from '@nestjs/testing';\nimport { DataSource } from 'typeorm';\nimport { MigrationService } from './migration.service';\nimport { ConfigModule } from '@nestjs/config';\n\ndescribe('Migration Service', () => {\n  let service: MigrationService;\n  let dataSource: DataSource;\n\n  beforeEach(async () => {\n    const module = await Test.createTestingModule({\n      imports: [\n        ConfigModule.forRoot({\n          envFilePath: '.env.test',\n        }),\n      ],\n      providers: [MigrationService],\n    }).compile();\n\n    service = module.get<MigrationService>(MigrationService);\n    dataSource = module.get<DataSource>(DataSource);\n  });\n\n  describe('Migration Planning', () => {\n    it('should identify pending migrations', async () => {\n      const plan = await service.getMigrationPlan();\n      expect(plan).toBeDefined();\n      expect(plan.pending).toBeInstanceOf(Array);\n      expect(plan.executed).toBeInstanceOf(Array);\n    });\n\n    it('should correctly calculate rollback capability', async () => {\n      const plan = await service.getMigrationPlan();\n      expect(typeof plan.canRollback).toBe('boolean');\n      \n      if (plan.executed.length > 0) {\n        expect(plan.canRollback).toBe(true);\n        expect(plan.rollbackTarget).toBeDefined();\n      }\n    });\n  });\n\n  describe('Migration Validation', () => {\n    it('should validate migration files', async () => {\n      // Create a test migration file\n      const testMigrationName = 'TestMigration1640995200000';\n      const validation = await service.validateMigration(testMigrationName);\n      \n      expect(validation).toBeDefined();\n      expect(typeof validation.isValid).toBe('boolean');\n      expect(validation.issues).toBeInstanceOf(Array);\n    });\n\n    it('should detect potentially dangerous operations', async () => {\n      // Test with a migration containing DROP TABLE\n      const dangerousMigration = `\n        export class DangerousMigration {\n          async up(queryRunner) {\n            await queryRunner.query('DROP TABLE users');\n          }\n        }\n      `;\n      \n      // This would require mocking the file system\n      // In a real test, you'd create temporary migration files\n    });\n  });\n\n  describe('Backup and Restore', () => {\n    it('should create database backups', async () => {\n      const backupName = `test-backup-${Date.now()}`;\n      \n      // Mock the backup creation\n      jest.spyOn(service, 'createBackup').mockResolvedValue(undefined);\n      \n      await expect(service.createBackup(backupName)).resolves.not.toThrow();\n    });\n  });\n\n  describe('Migration Execution', () => {\n    it('should handle dry run migrations', async () => {\n      const result = await service.runPendingMigrations(true);\n      \n      expect(result.success).toBe(true);\n      expect(result.migrationsRun).toBeInstanceOf(Array);\n      expect(result.errors).toBeInstanceOf(Array);\n      expect(typeof result.executionTime).toBe('number');\n    });\n\n    it('should rollback on migration failure', async () => {\n      // Mock a failing migration\n      jest.spyOn(dataSource, 'runMigrations').mockRejectedValue(new Error('Migration failed'));\n      jest.spyOn(service, 'rollbackLastMigration').mockResolvedValue(undefined);\n      \n      const result = await service.runPendingMigrations(false);\n      \n      expect(result.success).toBe(false);\n      expect(result.errors.length).toBeGreaterThan(0);\n    });\n  });\n\n  describe('Migration Reporting', () => {\n    it('should generate comprehensive migration reports', async () => {\n      const report = await service.generateMigrationReport();\n      \n      expect(report).toBeDefined();\n      expect(typeof report.totalMigrations).toBe('number');\n      expect(typeof report.executedMigrations).toBe('number');\n      expect(typeof report.pendingMigrations).toBe('number');\n      expect(typeof report.averageExecutionTime).toBe('number');\n      expect(report.migrationHistory).toBeInstanceOf(Array);\n    });\n  });\n});\n"})}),"\n",(0,r.jsx)(e.h2,{id:"automated-migration-pipeline",children:"Automated Migration Pipeline"}),"\n",(0,r.jsx)(e.h3,{id:"migration-cicd-script",children:"Migration CI/CD Script"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:'#!/bin/bash\n# scripts/migrate-database.sh\n\nset -euo pipefail\n\nENVIRONMENT=${1:-development}\nDRY_RUN=${2:-false}\nBACKUP_ENABLED=${3:-true}\n\necho "Starting database migration for environment: $ENVIRONMENT"\n\n# Load environment-specific configuration\ncase $ENVIRONMENT in\n  "development")\n    DB_HOST=$DEV_DB_HOST\n    DB_NAME=$DEV_DB_NAME\n    DB_USER=$DEV_DB_USER\n    DB_PASS=$DEV_DB_PASS\n    ;;\n  "staging")\n    DB_HOST=$STAGING_DB_HOST\n    DB_NAME=$STAGING_DB_NAME\n    DB_USER=$STAGING_DB_USER\n    DB_PASS=$STAGING_DB_PASS\n    ;;\n  "production")\n    DB_HOST=$PROD_DB_HOST\n    DB_NAME=$PROD_DB_NAME\n    DB_USER=$PROD_DB_USER\n    DB_PASS=$PROD_DB_PASS\n    ;;\n  *)\n    echo "Unknown environment: $ENVIRONMENT"\n    exit 1\n    ;;\nesac\n\n# Set database connection\nexport DATABASE_HOST=$DB_HOST\nexport DATABASE_NAME=$DB_NAME\nexport DATABASE_USERNAME=$DB_USER\nexport DATABASE_PASSWORD=$DB_PASS\n\n# Function to run migration with monitoring\nrun_migration() {\n  local start_time=$(date +%s)\n  \n  echo "Checking migration plan..."\n  npm run migration:plan\n  \n  if [ "$DRY_RUN" = "true" ]; then\n    echo "Running dry-run migration..."\n    npm run migration:run -- --dry-run\n    return 0\n  fi\n  \n  if [ "$BACKUP_ENABLED" = "true" ]; then\n    echo "Creating pre-migration backup..."\n    npm run migration:backup\n  fi\n  \n  echo "Running database migrations..."\n  npm run migration:run\n  \n  local end_time=$(date +%s)\n  local duration=$((end_time - start_time))\n  \n  echo "Migration completed in ${duration} seconds"\n  \n  # Validate migration\n  echo "Validating migration..."\n  npm run migration:validate\n  \n  # Generate migration report\n  npm run migration:report > "migration-report-${ENVIRONMENT}-$(date +%Y%m%d-%H%M%S).json"\n}\n\n# Function to handle migration failure\nhandle_failure() {\n  echo "Migration failed. Checking if rollback is needed..."\n  \n  if [ "$ENVIRONMENT" = "production" ]; then\n    echo "Production migration failed. Manual intervention required."\n    echo "Please check logs and consider manual rollback."\n    exit 1\n  else\n    echo "Attempting automatic rollback..."\n    npm run migration:rollback\n    exit 1\n  fi\n}\n\n# Trap errors and handle failure\ntrap handle_failure ERR\n\n# Pre-migration checks\necho "Running pre-migration checks..."\n\n# Check database connectivity\nnpm run migration:check-connection || {\n  echo "Database connection failed"\n  exit 1\n}\n\n# Check migration files\nnpm run migration:validate-files || {\n  echo "Migration file validation failed"\n  exit 1\n}\n\n# Check for pending migrations\nPENDING_MIGRATIONS=$(npm run migration:pending --silent | wc -l)\nif [ "$PENDING_MIGRATIONS" -eq 0 ]; then\n  echo "No pending migrations found"\n  exit 0\nfi\n\necho "Found $PENDING_MIGRATIONS pending migrations"\n\n# Production-specific checks\nif [ "$ENVIRONMENT" = "production" ]; then\n  echo "Production environment detected. Running additional checks..."\n  \n  # Check for destructive operations\n  npm run migration:check-destructive || {\n    echo "Destructive operations detected in migrations"\n    echo "Please review migrations manually"\n    exit 1\n  }\n  \n  # Require explicit confirmation for production\n  if [ -z "${MIGRATION_CONFIRMED:-}" ]; then\n    echo "Production migration requires explicit confirmation"\n    echo "Set MIGRATION_CONFIRMED=true to proceed"\n    exit 1\n  fi\nfi\n\n# Run migration\nrun_migration\n\necho "Migration pipeline completed successfully"\n'})}),"\n",(0,r.jsx)(e.h3,{id:"migration-github-action",children:"Migration GitHub Action"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-yaml",children:'# .github/workflows/migrate.yml\nname: Database Migration\n\non:\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: \'Target environment\'\n        required: true\n        default: \'staging\'\n        type: choice\n        options:\n        - development\n        - staging\n        - production\n      dry_run:\n        description: \'Dry run only\'\n        required: false\n        default: true\n        type: boolean\n\njobs:\n  migrate:\n    runs-on: ubuntu-latest\n    environment: ${{ github.event.inputs.environment }}\n    \n    steps:\n    - name: Checkout\n      uses: actions/checkout@v4\n\n    - name: Setup Node.js\n      uses: actions/setup-node@v4\n      with:\n        node-version: \'18\'\n        cache: \'npm\'\n\n    - name: Install dependencies\n      run: npm ci\n\n    - name: Setup environment variables\n      run: |\n        case "${{ github.event.inputs.environment }}" in\n          "development")\n            echo "DATABASE_HOST=${{ secrets.DEV_DB_HOST }}" >> $GITHUB_ENV\n            echo "DATABASE_NAME=${{ secrets.DEV_DB_NAME }}" >> $GITHUB_ENV\n            echo "DATABASE_USERNAME=${{ secrets.DEV_DB_USER }}" >> $GITHUB_ENV\n            echo "DATABASE_PASSWORD=${{ secrets.DEV_DB_PASS }}" >> $GITHUB_ENV\n            ;;\n          "staging")\n            echo "DATABASE_HOST=${{ secrets.STAGING_DB_HOST }}" >> $GITHUB_ENV\n            echo "DATABASE_NAME=${{ secrets.STAGING_DB_NAME }}" >> $GITHUB_ENV\n            echo "DATABASE_USERNAME=${{ secrets.STAGING_DB_USER }}" >> $GITHUB_ENV\n            echo "DATABASE_PASSWORD=${{ secrets.STAGING_DB_PASS }}" >> $GITHUB_ENV\n            ;;\n          "production")\n            echo "DATABASE_HOST=${{ secrets.PROD_DB_HOST }}" >> $GITHUB_ENV\n            echo "DATABASE_NAME=${{ secrets.PROD_DB_NAME }}" >> $GITHUB_ENV\n            echo "DATABASE_USERNAME=${{ secrets.PROD_DB_USER }}" >> $GITHUB_ENV\n            echo "DATABASE_PASSWORD=${{ secrets.PROD_DB_PASS }}" >> $GITHUB_ENV\n            ;;\n        esac\n\n    - name: Run migration checks\n      run: |\n        npm run migration:check-connection\n        npm run migration:validate-files\n\n    - name: Run migration plan\n      run: npm run migration:plan\n\n    - name: Run migration (dry run)\n      if: ${{ github.event.inputs.dry_run == \'true\' }}\n      run: npm run migration:run -- --dry-run\n\n    - name: Create backup\n      if: ${{ github.event.inputs.dry_run == \'false\' && github.event.inputs.environment == \'production\' }}\n      run: npm run migration:backup\n\n    - name: Run migration\n      if: ${{ github.event.inputs.dry_run == \'false\' }}\n      run: npm run migration:run\n\n    - name: Validate migration\n      if: ${{ github.event.inputs.dry_run == \'false\' }}\n      run: npm run migration:validate\n\n    - name: Generate migration report\n      run: npm run migration:report > migration-report.json\n\n    - name: Upload migration report\n      uses: actions/upload-artifact@v3\n      with:\n        name: migration-report-${{ github.event.inputs.environment }}\n        path: migration-report.json\n\n    - name: Notify on success\n      if: success()\n      uses: 8398a7/action-slack@v3\n      with:\n        status: success\n        text: |\n          \u2705 Database migration completed successfully\n          Environment: ${{ github.event.inputs.environment }}\n          Dry Run: ${{ github.event.inputs.dry_run }}\n      env:\n        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}\n\n    - name: Notify on failure\n      if: failure()\n      uses: 8398a7/action-slack@v3\n      with:\n        status: failure\n        text: |\n          \u274c Database migration failed\n          Environment: ${{ github.event.inputs.environment }}\n          Please check logs and consider rollback\n      env:\n        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}\n'})}),"\n",(0,r.jsx)(e.h2,{id:"migration-monitoring",children:"Migration Monitoring"}),"\n",(0,r.jsx)(e.h3,{id:"migration-metrics-collection",children:"Migration Metrics Collection"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-typescript",children:"// src/migration/migration-metrics.service.ts\nimport { Injectable } from '@nestjs/common';\nimport { PrometheusService } from '../monitoring/prometheus.service';\nimport { Counter, Histogram, Gauge } from 'prom-client';\n\n@Injectable()\nexport class MigrationMetricsService {\n  private readonly migrationCounter: Counter<string>;\n  private readonly migrationDuration: Histogram<string>;\n  private readonly pendingMigrations: Gauge<string>;\n  private readonly migrationErrors: Counter<string>;\n\n  constructor(private readonly prometheusService: PrometheusService) {\n    this.migrationCounter = new Counter({\n      name: 'database_migrations_total',\n      help: 'Total number of database migrations executed',\n      labelNames: ['status', 'migration_name'],\n      registers: [this.prometheusService.getRegistry()],\n    });\n\n    this.migrationDuration = new Histogram({\n      name: 'database_migration_duration_seconds',\n      help: 'Duration of database migration execution',\n      labelNames: ['migration_name'],\n      buckets: [1, 5, 10, 30, 60, 300, 600],\n      registers: [this.prometheusService.getRegistry()],\n    });\n\n    this.pendingMigrations = new Gauge({\n      name: 'database_pending_migrations',\n      help: 'Number of pending database migrations',\n      registers: [this.prometheusService.getRegistry()],\n    });\n\n    this.migrationErrors = new Counter({\n      name: 'database_migration_errors_total',\n      help: 'Total number of database migration errors',\n      labelNames: ['migration_name', 'error_type'],\n      registers: [this.prometheusService.getRegistry()],\n    });\n  }\n\n  recordMigrationSuccess(migrationName: string, duration: number): void {\n    this.migrationCounter.inc({ status: 'success', migration_name: migrationName });\n    this.migrationDuration.observe({ migration_name: migrationName }, duration);\n  }\n\n  recordMigrationFailure(migrationName: string, errorType: string): void {\n    this.migrationCounter.inc({ status: 'failure', migration_name: migrationName });\n    this.migrationErrors.inc({ migration_name: migrationName, error_type: errorType });\n  }\n\n  updatePendingMigrations(count: number): void {\n    this.pendingMigrations.set(count);\n  }\n}\n"})}),"\n",(0,r.jsx)(e.h2,{id:"related-documentation",children:"Related Documentation"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:(0,r.jsx)(e.a,{href:"/fullstack-dev/docs/architecture-practices/architecture-execution/configuration-management",children:"Configuration Management"})})," - Environment-specific database configurations"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:(0,r.jsx)(e.a,{href:"/fullstack-dev/docs/architecture-practices/architecture-execution/infrastructure-monitoring",children:"Infrastructure Monitoring"})})," - Database monitoring and alerting"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:(0,r.jsx)(e.a,{href:"/fullstack-dev/docs/architecture-practices/architecture-definition/backup-recovery",children:"Backup and Recovery"})})," - Database backup strategies"]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.p,{children:"This database migration management guide should be regularly updated to incorporate new migration patterns and database technologies."})]})}function l(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(g,{...n})}):g(n)}},7814:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>s});var i=t(9729);const r={},a=i.createContext(r);function o(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);