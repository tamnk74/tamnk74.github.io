"use strict";(self.webpackChunkdocs_site=self.webpackChunkdocs_site||[]).push([[4698],{6132:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"ai-machine-learning/vertex-ai-gemini-guide","title":"Getting Started with Vertex AI and Gemini Models","description":"This comprehensive guide covers everything you need to know about using Google\'s Vertex AI platform with Gemini models for enterprise AI applications. Learn how to integrate powerful generative AI capabilities into your applications using the Google Gen AI SDK.","source":"@site/docs/ai-machine-learning/vertex-ai-gemini-guide.md","sourceDirName":"ai-machine-learning","slug":"/ai-machine-learning/vertex-ai-gemini-guide","permalink":"/fullstack-dev/docs/ai-machine-learning/vertex-ai-gemini-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/tamnk74/fullstack-dev/tree/main/docs-site/docs/ai-machine-learning/vertex-ai-gemini-guide.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Generation and Personalization Engine","permalink":"/fullstack-dev/docs/rag-recommendations/generation-engine"},"next":{"title":"Features","permalink":"/fullstack-dev/docs/features"}}');var r=t(5813),o=t(7814);const s={},a="Getting Started with Vertex AI and Gemini Models",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Why Choose Vertex AI with Gemini?",id:"why-choose-vertex-ai-with-gemini",level:2},{value:"<strong>Enterprise-Ready Platform</strong>",id:"enterprise-ready-platform",level:3},{value:"<strong>Gemini Model Capabilities</strong>",id:"gemini-model-capabilities",level:3},{value:"Getting Started",id:"getting-started",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Installation",id:"installation",level:3},{value:"Basic Setup and Configuration",id:"basic-setup-and-configuration",level:2},{value:"1. Import Required Libraries",id:"1-import-required-libraries",level:3},{value:"2. Configure Your Project",id:"2-configure-your-project",level:3},{value:"Core Features and Implementation",id:"core-features-and-implementation",level:2},{value:"1. Text Generation",id:"1-text-generation",level:3},{value:"2. Multimodal Content Processing",id:"2-multimodal-content-processing",level:3},{value:"3. Processing Files from Cloud Storage",id:"3-processing-files-from-cloud-storage",level:3},{value:"Advanced Features",id:"advanced-features",level:2},{value:"1. System Instructions",id:"1-system-instructions",level:3},{value:"2. Parameter Configuration",id:"2-parameter-configuration",level:3},{value:"3. Safety Filters",id:"3-safety-filters",level:3},{value:"4. Multi-turn Conversations",id:"4-multi-turn-conversations",level:3},{value:"5. Controlled Generation (Structured Output)",id:"5-controlled-generation-structured-output",level:3},{value:"6. Function Calling",id:"6-function-calling",level:3},{value:"Production Features",id:"production-features",level:2},{value:"1. Context Caching",id:"1-context-caching",level:3},{value:"2. Batch Processing",id:"2-batch-processing",level:3},{value:"3. Text Embeddings",id:"3-text-embeddings",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Error Handling and Resilience",id:"1-error-handling-and-resilience",level:3},{value:"2. Cost Optimization",id:"2-cost-optimization",level:3},{value:"3. Monitoring and Logging",id:"3-monitoring-and-logging",level:3},{value:"Enterprise Integration Patterns",id:"enterprise-integration-patterns",level:2},{value:"1. API Gateway Integration",id:"1-api-gateway-integration",level:3},{value:"2. Microservices Architecture",id:"2-microservices-architecture",level:3},{value:"Security and Compliance",id:"security-and-compliance",level:2},{value:"1. Data Privacy",id:"1-data-privacy",level:3},{value:"2. Access Control",id:"2-access-control",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Connection Pooling",id:"1-connection-pooling",level:3},{value:"2. Caching Layer",id:"2-caching-layer",level:3},{value:"Deployment and DevOps",id:"deployment-and-devops",level:2},{value:"1. Docker Configuration",id:"1-docker-configuration",level:3},{value:"2. Kubernetes Deployment",id:"2-kubernetes-deployment",level:3},{value:"3. Terraform Infrastructure",id:"3-terraform-infrastructure",level:3},{value:"Monitoring and Observability",id:"monitoring-and-observability",level:2},{value:"1. Metrics and Logging",id:"1-metrics-and-logging",level:3},{value:"2. Health Checks",id:"2-health-checks",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"getting-started-with-vertex-ai-and-gemini-models",children:"Getting Started with Vertex AI and Gemini Models"})}),"\n",(0,r.jsx)(n.p,{children:"This comprehensive guide covers everything you need to know about using Google's Vertex AI platform with Gemini models for enterprise AI applications. Learn how to integrate powerful generative AI capabilities into your applications using the Google Gen AI SDK."}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.a,{href:"https://googleapis.github.io/python-genai/",children:"Google Gen AI SDK"})," provides a unified interface to Google's generative AI API services, including the powerful Gemini models. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks."]}),"\n",(0,r.jsx)(n.h2,{id:"why-choose-vertex-ai-with-gemini",children:"Why Choose Vertex AI with Gemini?"}),"\n",(0,r.jsx)(n.h3,{id:"enterprise-ready-platform",children:(0,r.jsx)(n.strong,{children:"Enterprise-Ready Platform"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Scalability"}),": Built for production workloads with enterprise-grade infrastructure"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Security"}),": Advanced security controls and compliance certifications"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integration"}),": Seamless integration with Google Cloud services"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost Efficiency"}),": Flexible pricing models including batch processing options"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"gemini-model-capabilities",children:(0,r.jsx)(n.strong,{children:"Gemini Model Capabilities"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multimodal"}),": Process text, images, audio, video, and PDFs in a single request"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Advanced Reasoning"}),": State-of-the-art language understanding and generation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Function Calling"}),": Integrate with external APIs and tools"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context Caching"}),": Optimize costs for frequently used content"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Controlled Generation"}),": Enforce structured output formats"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,r.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"Before you begin, ensure you have:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Google Cloud Project"}),": ",(0,r.jsx)(n.a,{href:"https://console.cloud.google.com/",children:"Create a project"})," if you don't have one"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vertex AI API"}),": ",(0,r.jsx)(n.a,{href:"https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com",children:"Enable the Vertex AI API"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Authentication"}),": Set up ",(0,r.jsx)(n.a,{href:"https://cloud.google.com/docs/authentication/application-default-credentials",children:"Application Default Credentials"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Python Environment"}),": Python 3.8 or higher"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,r.jsx)(n.p,{children:"Install the required packages:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install --upgrade google-genai pandas pillow requests\n"})}),"\n",(0,r.jsx)(n.h2,{id:"basic-setup-and-configuration",children:"Basic Setup and Configuration"}),"\n",(0,r.jsx)(n.h3,{id:"1-import-required-libraries",children:"1. Import Required Libraries"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import datetime\nimport os\nfrom google import genai\nfrom google.genai.types import (\n    CreateBatchJobConfig,\n    CreateCachedContentConfig,\n    EmbedContentConfig,\n    FunctionDeclaration,\n    GenerateContentConfig,\n    HarmBlockThreshold,\n    HarmCategory,\n    Part,\n    SafetySetting,\n    Tool,\n)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-configure-your-project",children:"2. Configure Your Project"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Set your Google Cloud project information\nPROJECT_ID = "your-project-id"\nLOCATION = "us-central1"  # or your preferred region\n\n# Initialize the client for Vertex AI\nclient = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n\n# Choose your model\nMODEL_ID = "gemini-2.5-flash"  # Latest Gemini model\n'})}),"\n",(0,r.jsx)(n.h2,{id:"core-features-and-implementation",children:"Core Features and Implementation"}),"\n",(0,r.jsx)(n.h3,{id:"1-text-generation",children:"1. Text Generation"}),"\n",(0,r.jsx)(n.p,{children:"Generate responses to text prompts with simple API calls:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def generate_text_response(prompt):\n    """Generate a text response from Gemini."""\n    response = client.models.generate_content(\n        model=MODEL_ID,\n        contents=prompt\n    )\n    return response.text\n\n# Example usage\nprompt = "Explain the benefits of using Vertex AI for enterprise applications."\nresponse = generate_text_response(prompt)\nprint(response)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-multimodal-content-processing",children:"2. Multimodal Content Processing"}),"\n",(0,r.jsx)(n.p,{children:"Process images, documents, and other media alongside text:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from PIL import Image\nimport requests\n\ndef analyze_image_with_text(image_url, prompt):\n    """Analyze an image with accompanying text prompt."""\n    # Load image from URL\n    image = Image.open(requests.get(image_url, stream=True).raw)\n    \n    response = client.models.generate_content(\n        model=MODEL_ID,\n        contents=[\n            image,\n            prompt\n        ]\n    )\n    return response.text\n\n# Example: Analyze a business document\nimage_url = "https://example.com/business-chart.png"\nprompt = "Analyze this business chart and provide key insights."\nanalysis = analyze_image_with_text(image_url, prompt)\nprint(analysis)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3-processing-files-from-cloud-storage",children:"3. Processing Files from Cloud Storage"}),"\n",(0,r.jsx)(n.p,{children:"For production applications, process files directly from Google Cloud Storage:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def process_cloud_storage_file(file_uri, mime_type, prompt):\n    """Process files stored in Google Cloud Storage."""\n    response = client.models.generate_content(\n        model=MODEL_ID,\n        contents=[\n            Part.from_uri(\n                file_uri=file_uri,\n                mime_type=mime_type\n            ),\n            prompt\n        ]\n    )\n    return response.text\n\n# Example: Process a PDF document\nfile_uri = "gs://your-bucket/document.pdf"\nprompt = "Summarize the key points from this document."\nsummary = process_cloud_storage_file(file_uri, "application/pdf", prompt)\nprint(summary)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,r.jsx)(n.h3,{id:"1-system-instructions",children:"1. System Instructions"}),"\n",(0,r.jsx)(n.p,{children:"Guide model behavior with system instructions for consistent responses:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def create_specialized_assistant(system_instruction, user_prompt):\n    """Create a specialized AI assistant with specific instructions."""\n    response = client.models.generate_content(\n        model=MODEL_ID,\n        contents=user_prompt,\n        config=GenerateContentConfig(\n            system_instruction=system_instruction,\n        )\n    )\n    return response.text\n\n# Example: Create a code review assistant\nsystem_instruction = """\nYou are an expert software architect and code reviewer.\nAnalyze code for:\n- Security vulnerabilities\n- Performance issues\n- Best practices compliance\n- Maintainability concerns\nProvide specific, actionable recommendations.\n"""\n\nuser_prompt = "Review this Python function for potential issues: [code here]"\nreview = create_specialized_assistant(system_instruction, user_prompt)\nprint(review)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-parameter-configuration",children:"2. Parameter Configuration"}),"\n",(0,r.jsx)(n.p,{children:"Fine-tune model behavior with generation parameters:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def generate_with_parameters(prompt, temperature=0.7, top_p=0.9, top_k=40):\n    """Generate content with specific parameters for control."""\n    response = client.models.generate_content(\n        model=MODEL_ID,\n        contents=prompt,\n        config=GenerateContentConfig(\n            temperature=temperature,      # Creativity (0.0-1.0)\n            top_p=top_p,                 # Nucleus sampling\n            top_k=top_k,                 # Top-k sampling\n            candidate_count=1,           # Number of responses\n            max_output_tokens=1024,      # Response length limit\n        )\n    )\n    return response.text\n\n# Example: Generate creative content\ncreative_prompt = "Write a story about AI in the future."\nstory = generate_with_parameters(creative_prompt, temperature=0.9)\n\n# Example: Generate factual content\nfactual_prompt = "List the steps to implement OAuth 2.0."\nsteps = generate_with_parameters(factual_prompt, temperature=0.1)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3-safety-filters",children:"3. Safety Filters"}),"\n",(0,r.jsx)(n.p,{children:"Configure safety settings for enterprise applications:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def generate_with_safety_controls(prompt):\n    """Generate content with enterprise-grade safety controls."""\n    safety_settings = [\n        SafetySetting(\n            category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n            threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        ),\n        SafetySetting(\n            category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n            threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        ),\n        SafetySetting(\n            category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n            threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        ),\n        SafetySetting(\n            category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n            threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        ),\n    ]\n    \n    response = client.models.generate_content(\n        model=MODEL_ID,\n        contents=prompt,\n        config=GenerateContentConfig(\n            safety_settings=safety_settings,\n        )\n    )\n    \n    # Check safety ratings\n    safety_ratings = response.candidates[0].safety_ratings\n    print(f"Safety ratings: {safety_ratings}")\n    \n    return response.text\n'})}),"\n",(0,r.jsx)(n.h3,{id:"4-multi-turn-conversations",children:"4. Multi-turn Conversations"}),"\n",(0,r.jsx)(n.p,{children:"Create interactive chat experiences:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class GeminiChatBot:\n    def __init__(self, system_instruction="You are a helpful AI assistant."):\n        self.chat = client.chats.create(\n            model=MODEL_ID,\n            config=GenerateContentConfig(\n                system_instruction=system_instruction,\n                temperature=0.7,\n            )\n        )\n    \n    def send_message(self, message):\n        """Send a message and get response."""\n        response = self.chat.send_message(message)\n        return response.text\n    \n    def get_history(self):\n        """Get conversation history."""\n        return self.chat.history\n\n# Example: Create a coding assistant\ncoding_assistant = GeminiChatBot(\n    "You are an expert software developer specializing in cloud applications."\n)\n\nresponse1 = coding_assistant.send_message("How do I deploy a Python app to Google Cloud Run?")\nresponse2 = coding_assistant.send_message("What about environment variables?")\nresponse3 = coding_assistant.send_message("Show me a Dockerfile example.")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"5-controlled-generation-structured-output",children:"5. Controlled Generation (Structured Output)"}),"\n",(0,r.jsx)(n.p,{children:"Force the model to return structured data:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from pydantic import BaseModel\nimport json\n\nclass ProductAnalysis(BaseModel):\n    product_name: str\n    category: str\n    pros: list[str]\n    cons: list[str]\n    rating: float\n    recommendation: str\n\ndef analyze_product_structured(product_description):\n    """Analyze a product and return structured data."""\n    response = client.models.generate_content(\n        model=MODEL_ID,\n        contents=f"Analyze this product: {product_description}",\n        config=GenerateContentConfig(\n            response_mime_type="application/json",\n            response_schema=ProductAnalysis,\n        )\n    )\n    \n    # Parse JSON response\n    return json.loads(response.text)\n\n# Example usage\nproduct = "iPhone 15 Pro - Latest smartphone with advanced camera system"\nanalysis = analyze_product_structured(product)\nprint(json.dumps(analysis, indent=2))\n'})}),"\n",(0,r.jsx)(n.h3,{id:"6-function-calling",children:"6. Function Calling"}),"\n",(0,r.jsx)(n.p,{children:"Integrate external APIs and tools:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Define a function for the model to call\ndef create_weather_function():\n    """Create a function declaration for weather data."""\n    return FunctionDeclaration(\n        name="get_weather",\n        description="Get current weather information for a location",\n        parameters={\n            "type": "OBJECT",\n            "properties": {\n                "location": {\n                    "type": "STRING",\n                    "description": "City and country/state",\n                },\n                "unit": {\n                    "type": "STRING",\n                    "enum": ["celsius", "fahrenheit"],\n                    "description": "Temperature unit",\n                }\n            },\n            "required": ["location"]\n        },\n    )\n\ndef handle_function_calling(user_request):\n    """Handle requests that might need function calling."""\n    weather_tool = Tool(function_declarations=[create_weather_function()])\n    \n    response = client.models.generate_content(\n        model=MODEL_ID,\n        contents=user_request,\n        config=GenerateContentConfig(\n            tools=[weather_tool],\n            temperature=0,\n        )\n    )\n    \n    # Check if model wants to call a function\n    if response.candidates[0].content.parts[0].function_call:\n        function_call = response.candidates[0].content.parts[0].function_call\n        print(f"Model wants to call: {function_call.name}")\n        print(f"With arguments: {function_call.args}")\n        \n        # Here you would implement the actual API call\n        # For this example, we\'ll simulate the response\n        weather_data = {\n            "temperature": 22,\n            "condition": "sunny",\n            "humidity": 65\n        }\n        \n        return weather_data\n    \n    return response.text\n\n# Example usage\nrequest = "What\'s the weather like in San Francisco?"\nresult = handle_function_calling(request)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"production-features",children:"Production Features"}),"\n",(0,r.jsx)(n.h3,{id:"1-context-caching",children:"1. Context Caching"}),"\n",(0,r.jsx)(n.p,{children:"Optimize costs by caching frequently used content:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def create_document_cache(documents):\n    """Create a cache for frequently referenced documents."""\n    # Prepare document parts\n    pdf_parts = [\n        Part.from_uri(\n            file_uri=doc_uri,\n            mime_type="application/pdf"\n        ) for doc_uri in documents\n    ]\n    \n    # Create cache\n    cached_content = client.caches.create(\n        model="gemini-2.5-flash",\n        config=CreateCachedContentConfig(\n            system_instruction="You are an expert document analyst.",\n            contents=pdf_parts,\n            ttl="3600s",  # 1 hour cache\n        )\n    )\n    \n    return cached_content\n\ndef query_cached_documents(cached_content, query):\n    """Query documents using cached content."""\n    response = client.models.generate_content(\n        model="gemini-2.5-flash",\n        contents=query,\n        config=GenerateContentConfig(\n            cached_content=cached_content.name,\n        )\n    )\n    \n    return response.text\n\n# Example: Cache and query company documents\ndocuments = [\n    "gs://your-bucket/policy-doc1.pdf",\n    "gs://your-bucket/policy-doc2.pdf"\n]\n\ncache = create_document_cache(documents)\nanswer = query_cached_documents(cache, "What is our remote work policy?")\n\n# Clean up cache when done\nclient.caches.delete(name=cache.name)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-batch-processing",children:"2. Batch Processing"}),"\n",(0,r.jsx)(n.p,{children:"Process large volumes of requests efficiently:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\n\ndef create_batch_job(input_data_uri, output_bucket):\n    """Create a batch processing job for large-scale inference."""\n    batch_job = client.batches.create(\n        model=MODEL_ID,\n        src=input_data_uri,  # JSONL file in Cloud Storage\n        config=CreateBatchJobConfig(dest=output_bucket),\n    )\n    \n    return batch_job\n\ndef wait_for_batch_completion(batch_job):\n    """Wait for batch job to complete."""\n    while batch_job.state == "JOB_STATE_RUNNING":\n        print("Batch job still running...")\n        time.sleep(30)\n        batch_job = client.batches.get(name=batch_job.name)\n    \n    if batch_job.state == "JOB_STATE_SUCCEEDED":\n        print("Batch job completed successfully!")\n        return batch_job.dest.gcs_uri\n    else:\n        print(f"Batch job failed: {batch_job.error}")\n        return None\n\n# Example: Process thousands of customer reviews\ninput_data = "gs://your-bucket/customer-reviews.jsonl"\noutput_bucket = "gs://your-bucket/batch-results/"\n\nbatch_job = create_batch_job(input_data, output_bucket)\nresults_location = wait_for_batch_completion(batch_job)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3-text-embeddings",children:"3. Text Embeddings"}),"\n",(0,r.jsx)(n.p,{children:"Generate embeddings for semantic search and retrieval:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def generate_embeddings(texts, output_dim=768):\n    """Generate embeddings for a list of texts."""\n    response = client.models.embed_content(\n        model="gemini-embedding-001",\n        contents=texts,\n        config=EmbedContentConfig(output_dimensionality=output_dim)\n    )\n    \n    return response.embeddings\n\ndef build_knowledge_base(documents):\n    """Build a searchable knowledge base using embeddings."""\n    # Generate embeddings for all documents\n    embeddings = generate_embeddings(documents)\n    \n    # Store embeddings with metadata\n    knowledge_base = []\n    for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n        knowledge_base.append({\n            "id": i,\n            "text": doc,\n            "embedding": embedding.values\n        })\n    \n    return knowledge_base\n\n# Example: Create a FAQ search system\nfaq_documents = [\n    "How do I reset my password?",\n    "What are your business hours?",\n    "How do I contact customer support?",\n    "What is your return policy?"\n]\n\nknowledge_base = build_knowledge_base(faq_documents)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsx)(n.h3,{id:"1-error-handling-and-resilience",children:"1. Error Handling and Resilience"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\nfrom google.api_core import exceptions\n\ndef resilient_generate_content(prompt, max_retries=3):\n    """Generate content with error handling and retries."""\n    for attempt in range(max_retries):\n        try:\n            response = client.models.generate_content(\n                model=MODEL_ID,\n                contents=prompt\n            )\n            return response.text\n        \n        except exceptions.ResourceExhausted:\n            # Rate limit exceeded\n            wait_time = (2 ** attempt) + 1\n            print(f"Rate limit hit. Waiting {wait_time}s before retry...")\n            time.sleep(wait_time)\n        \n        except exceptions.InvalidArgument as e:\n            print(f"Invalid request: {e}")\n            break\n        \n        except Exception as e:\n            print(f"Unexpected error on attempt {attempt + 1}: {e}")\n            if attempt == max_retries - 1:\n                raise\n    \n    return None\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-cost-optimization",children:"2. Cost Optimization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def cost_efficient_generation(prompt, use_cache=True):\n    """Generate content with cost optimization strategies."""\n    config = GenerateContentConfig(\n        # Use lower cost settings for non-critical tasks\n        temperature=0.3,\n        max_output_tokens=512,  # Limit output length\n        candidate_count=1,      # Single response\n    )\n    \n    if use_cache:\n        # Implement your caching logic here\n        pass\n    \n    response = client.models.generate_content(\n        model="gemini-2.5-flash",  # Use cost-effective model\n        contents=prompt,\n        config=config\n    )\n    \n    return response.text\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3-monitoring-and-logging",children:"3. Monitoring and Logging"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import logging\nfrom datetime import datetime\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef monitored_generation(prompt, user_id=None):\n    """Generate content with monitoring and logging."""\n    start_time = datetime.now()\n    \n    try:\n        response = client.models.generate_content(\n            model=MODEL_ID,\n            contents=prompt\n        )\n        \n        # Log successful request\n        duration = (datetime.now() - start_time).total_seconds()\n        logger.info(f"Generation successful - User: {user_id}, Duration: {duration}s")\n        \n        return response.text\n    \n    except Exception as e:\n        # Log error\n        logger.error(f"Generation failed - User: {user_id}, Error: {str(e)}")\n        raise\n'})}),"\n",(0,r.jsx)(n.h2,{id:"enterprise-integration-patterns",children:"Enterprise Integration Patterns"}),"\n",(0,r.jsx)(n.h3,{id:"1-api-gateway-integration",children:"1. API Gateway Integration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n@app.route('/api/generate', methods=['POST'])\ndef api_generate():\n    \"\"\"API endpoint for text generation.\"\"\"\n    try:\n        data = request.json\n        prompt = data.get('prompt')\n        \n        if not prompt:\n            return jsonify({'error': 'Prompt is required'}), 400\n        \n        response = client.models.generate_content(\n            model=MODEL_ID,\n            contents=prompt\n        )\n        \n        return jsonify({\n            'response': response.text,\n            'model': MODEL_ID,\n            'timestamp': datetime.now().isoformat()\n        })\n    \n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8080)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-microservices-architecture",children:"2. Microservices Architecture"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class GeminiService:\n    """Service class for Gemini operations."""\n    \n    def __init__(self, project_id, location="us-central1"):\n        self.client = genai.Client(\n            vertexai=True, \n            project=project_id, \n            location=location\n        )\n        self.model_id = "gemini-2.5-flash"\n    \n    async def generate_async(self, prompt):\n        """Asynchronous content generation."""\n        # Implement async generation logic\n        pass\n    \n    def generate_with_validation(self, prompt, expected_format=None):\n        """Generate content with output validation."""\n        response = self.client.models.generate_content(\n            model=self.model_id,\n            contents=prompt\n        )\n        \n        # Validate response format if specified\n        if expected_format:\n            # Implement validation logic\n            pass\n        \n        return response.text\n    \n    def health_check(self):\n        """Service health check."""\n        try:\n            self.client.models.generate_content(\n                model=self.model_id,\n                contents="Health check"\n            )\n            return True\n        except:\n            return False\n'})}),"\n",(0,r.jsx)(n.h2,{id:"security-and-compliance",children:"Security and Compliance"}),"\n",(0,r.jsx)(n.h3,{id:"1-data-privacy",children:"1. Data Privacy"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def secure_generation(prompt, user_id):\n    """Generate content with privacy safeguards."""\n    # Remove or mask PII from prompts\n    cleaned_prompt = remove_pii(prompt)\n    \n    response = client.models.generate_content(\n        model=MODEL_ID,\n        contents=cleaned_prompt,\n        config=GenerateContentConfig(\n            # Enable safety filters\n            safety_settings=[\n                SafetySetting(\n                    category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n                    threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n                )\n            ]\n        )\n    )\n    \n    # Log access for audit trail\n    log_api_access(user_id, "generation", datetime.now())\n    \n    return response.text\n\ndef remove_pii(text):\n    """Remove personally identifiable information."""\n    # Implement PII detection and removal\n    return text\n\ndef log_api_access(user_id, action, timestamp):\n    """Log API access for compliance."""\n    # Implement audit logging\n    pass\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-access-control",children:"2. Access Control"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from functools import wraps\n\ndef require_auth(f):\n    """Decorator for API authentication."""\n    @wraps(f)\n    def decorated_function(*args, **kwargs):\n        # Implement authentication logic\n        token = request.headers.get(\'Authorization\')\n        if not validate_token(token):\n            return jsonify({\'error\': \'Unauthorized\'}), 401\n        return f(*args, **kwargs)\n    return decorated_function\n\ndef validate_token(token):\n    """Validate authentication token."""\n    # Implement token validation\n    return True\n\n@app.route(\'/api/secure-generate\', methods=[\'POST\'])\n@require_auth\ndef secure_generate_endpoint():\n    """Secured generation endpoint."""\n    # Implementation here\n    pass\n'})}),"\n",(0,r.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"1-connection-pooling",children:"1. Connection Pooling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import threading\n\nclass GeminiClientPool:\n    """Connection pool for Gemini clients."""\n    \n    def __init__(self, project_id, location, pool_size=5):\n        self.clients = []\n        self.lock = threading.Lock()\n        \n        for _ in range(pool_size):\n            client = genai.Client(\n                vertexai=True,\n                project=project_id,\n                location=location\n            )\n            self.clients.append(client)\n    \n    def get_client(self):\n        """Get an available client from the pool."""\n        with self.lock:\n            if self.clients:\n                return self.clients.pop()\n            else:\n                # Create new client if pool is empty\n                return genai.Client(vertexai=True)\n    \n    def return_client(self, client):\n        """Return client to the pool."""\n        with self.lock:\n            self.clients.append(client)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-caching-layer",children:"2. Caching Layer"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import redis\nimport json\nimport hashlib\n\nclass ResponseCache:\n    """Redis-based cache for API responses."""\n    \n    def __init__(self, redis_host=\'localhost\', redis_port=6379):\n        self.redis_client = redis.Redis(host=redis_host, port=redis_port)\n        self.ttl = 3600  # 1 hour\n    \n    def get_cache_key(self, prompt, model_id):\n        """Generate cache key for prompt."""\n        content = f"{prompt}:{model_id}"\n        return hashlib.md5(content.encode()).hexdigest()\n    \n    def get(self, prompt, model_id):\n        """Get cached response."""\n        key = self.get_cache_key(prompt, model_id)\n        cached = self.redis_client.get(key)\n        return json.loads(cached) if cached else None\n    \n    def set(self, prompt, model_id, response):\n        """Cache response."""\n        key = self.get_cache_key(prompt, model_id)\n        self.redis_client.setex(\n            key, \n            self.ttl, \n            json.dumps(response)\n        )\n\n# Usage with cache\ncache = ResponseCache()\n\ndef cached_generation(prompt):\n    """Generate content with caching."""\n    # Check cache first\n    cached_response = cache.get(prompt, MODEL_ID)\n    if cached_response:\n        return cached_response[\'text\']\n    \n    # Generate new response\n    response = client.models.generate_content(\n        model=MODEL_ID,\n        contents=prompt\n    )\n    \n    # Cache the response\n    cache.set(prompt, MODEL_ID, {\'text\': response.text})\n    \n    return response.text\n'})}),"\n",(0,r.jsx)(n.h2,{id:"deployment-and-devops",children:"Deployment and DevOps"}),"\n",(0,r.jsx)(n.h3,{id:"1-docker-configuration",children:"1. Docker Configuration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-dockerfile",children:'# Dockerfile\nFROM python:3.11-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Set environment variables\nENV PYTHONPATH=/app\nENV GOOGLE_CLOUD_PROJECT=your-project-id\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n    CMD python health_check.py\n\nEXPOSE 8080\n\nCMD ["python", "main.py"]\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-kubernetes-deployment",children:"2. Kubernetes Deployment"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# k8s/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gemini-service\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: gemini-service\n  template:\n    metadata:\n      labels:\n        app: gemini-service\n    spec:\n      containers:\n      - name: gemini-service\n        image: gcr.io/your-project/gemini-service:latest\n        ports:\n        - containerPort: 8080\n        env:\n        - name: GOOGLE_CLOUD_PROJECT\n          value: "your-project-id"\n        - name: LOCATION\n          value: "us-central1"\n        resources:\n          requests:\n            memory: "512Mi"\n            cpu: "250m"\n          limits:\n            memory: "1Gi"\n            cpu: "500m"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3-terraform-infrastructure",children:"3. Terraform Infrastructure"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-hcl",children:'# terraform/main.tf\nresource "google_project_service" "vertex_ai" {\n  project = var.project_id\n  service = "aiplatform.googleapis.com"\n}\n\nresource "google_service_account" "gemini_service" {\n  account_id   = "gemini-service"\n  display_name = "Gemini Service Account"\n  project      = var.project_id\n}\n\nresource "google_project_iam_member" "gemini_ai_user" {\n  project = var.project_id\n  role    = "roles/aiplatform.user"\n  member  = "serviceAccount:${google_service_account.gemini_service.email}"\n}\n\nresource "google_cloud_run_service" "gemini_api" {\n  name     = "gemini-api"\n  location = var.region\n  project  = var.project_id\n\n  template {\n    spec {\n      containers {\n        image = "gcr.io/${var.project_id}/gemini-service:latest"\n        \n        env {\n          name  = "GOOGLE_CLOUD_PROJECT"\n          value = var.project_id\n        }\n        \n        resources {\n          limits = {\n            cpu    = "2000m"\n            memory = "2Gi"\n          }\n        }\n      }\n      \n      service_account_name = google_service_account.gemini_service.email\n    }\n  }\n\n  traffic {\n    percent         = 100\n    latest_revision = true\n  }\n}\n'})}),"\n",(0,r.jsx)(n.h2,{id:"monitoring-and-observability",children:"Monitoring and Observability"}),"\n",(0,r.jsx)(n.h3,{id:"1-metrics-and-logging",children:"1. Metrics and Logging"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from google.cloud import monitoring_v3\nfrom google.cloud import logging\n\ndef setup_monitoring():\n    """Set up monitoring and logging."""\n    # Initialize Cloud Monitoring\n    monitoring_client = monitoring_v3.MetricServiceClient()\n    \n    # Initialize Cloud Logging\n    logging_client = logging.Client()\n    logging_client.setup_logging()\n    \n    return monitoring_client, logging_client\n\ndef track_generation_metrics(duration, tokens_used, model_id):\n    """Track generation metrics."""\n    # Send custom metrics to Cloud Monitoring\n    monitoring_client, _ = setup_monitoring()\n    \n    # Create metric data\n    series = monitoring_v3.TimeSeries()\n    series.metric.type = "custom.googleapis.com/gemini/generation_duration"\n    series.resource.type = "global"\n    \n    point = monitoring_v3.Point()\n    point.value.double_value = duration\n    point.interval.end_time.seconds = int(time.time())\n    \n    series.points = [point]\n    \n    # Send to monitoring\n    project_name = f"projects/{PROJECT_ID}"\n    monitoring_client.create_time_series(\n        name=project_name, \n        time_series=[series]\n    )\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-health-checks",children:"2. Health Checks"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def health_check():\n    """Comprehensive health check."""\n    checks = {\n        "vertex_ai": check_vertex_ai_connection(),\n        "model_availability": check_model_availability(),\n        "cache": check_cache_connection(),\n        "dependencies": check_dependencies()\n    }\n    \n    all_healthy = all(checks.values())\n    \n    return {\n        "status": "healthy" if all_healthy else "unhealthy",\n        "checks": checks,\n        "timestamp": datetime.now().isoformat()\n    }\n\ndef check_vertex_ai_connection():\n    """Check Vertex AI connectivity."""\n    try:\n        client.models.generate_content(\n            model=MODEL_ID,\n            contents="health check"\n        )\n        return True\n    except:\n        return False\n'})}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,r.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Authentication Errors"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Ensure proper service account setup\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'path/to/service-account.json'\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Rate Limiting"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Implement exponential backoff\ndef retry_with_backoff(func, max_retries=3):\n    for i in range(max_retries):\n        try:\n            return func()\n        except exceptions.ResourceExhausted:\n            time.sleep(2 ** i)\n    raise Exception("Max retries exceeded")\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Memory Issues"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Process large files in chunks\ndef process_large_document(file_uri):\n    # Split document into smaller parts\n    # Process each part separately\n    pass\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.p,{children:"Vertex AI with Gemini models provides a powerful platform for building enterprise AI applications. Key benefits include:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unified API"}),": Single interface for all AI capabilities"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multimodal Processing"}),": Handle text, images, audio, and video"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Enterprise Features"}),": Security, compliance, and scalability"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost Optimization"}),": Caching, batching, and efficient processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Production Ready"}),": Monitoring, logging, and deployment tools"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Explore Advanced Features"}),": Dive deeper into function calling and agent patterns"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Build Production Applications"}),": Implement monitoring and security best practices"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Optimize Performance"}),": Use caching and batch processing for scale"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integrate with Workflows"}),": Connect with existing business processes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stay Updated"}),": Follow ",(0,r.jsx)(n.a,{href:"https://cloud.google.com/vertex-ai/docs",children:"Google Cloud AI documentation"})," for latest features"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["For more examples and advanced use cases, visit the ",(0,r.jsx)(n.a,{href:"https://github.com/GoogleCloudPlatform/generative-ai",children:"Google Cloud Generative AI GitHub repository"}),"."]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},7814:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var i=t(9729);const r={},o=i.createContext(r);function s(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);