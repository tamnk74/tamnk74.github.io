"use strict";(self.webpackChunkdocs_site=self.webpackChunkdocs_site||[]).push([[9713],{4093:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>u,frontMatter:()=>i,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"devops/ecommerce-backup-restore-strategy","title":"Ecommerce Platform Backup & Restore Strategy","description":"Overview","source":"@site/docs/devops/ecommerce-backup-restore-strategy.md","sourceDirName":"devops","slug":"/devops/ecommerce-backup-restore-strategy","permalink":"/fullstack-dev/docs/devops/ecommerce-backup-restore-strategy","draft":false,"unlisted":false,"editUrl":"https://github.com/tamnk74/fullstack-dev/tree/main/docs-site/docs/devops/ecommerce-backup-restore-strategy.md","tags":[],"version":"current","frontMatter":{}}');var s=t(5813),r=t(5741);const i={},c="Ecommerce Platform Backup & Restore Strategy",o={},l=[{value:"Overview",id:"overview",level:2},{value:"Table of Contents",id:"table-of-contents",level:2},{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"Current Tech Stack",id:"current-tech-stack",level:3},{value:"Backup Architecture Diagram",id:"backup-architecture-diagram",level:3},{value:"Backup Requirements",id:"backup-requirements",level:2},{value:"Recovery Objectives",id:"recovery-objectives",level:3},{value:"Business Impact Classification",id:"business-impact-classification",level:3},{value:"Data Classification",id:"data-classification",level:2},{value:"Ecommerce Data Categories",id:"ecommerce-data-categories",level:3},{value:"Backup Infrastructure",id:"backup-infrastructure",level:2},{value:"Google Cloud Infrastructure Setup",id:"google-cloud-infrastructure-setup",level:3},{value:"Database Backup Strategy",id:"database-backup-strategy",level:2},{value:"PostgreSQL Microservices Backup",id:"postgresql-microservices-backup",level:3},{value:"Application Data Backup",id:"application-data-backup",level:2},{value:"Redis Cache Backup Strategy",id:"redis-cache-backup-strategy",level:3},{value:"Kubernetes Cluster Backup",id:"kubernetes-cluster-backup",level:2},{value:"Velero Backup Configuration",id:"velero-backup-configuration",level:3},{value:"File Storage Backup",id:"file-storage-backup",level:2},{value:"Product Images and Static Assets",id:"product-images-and-static-assets",level:3},{value:"Configuration &amp; Secrets Backup",id:"configuration--secrets-backup",level:2},{value:"Kubernetes Secrets and ConfigMaps",id:"kubernetes-secrets-and-configmaps",level:3},{value:"Disaster Recovery Plan",id:"disaster-recovery-plan",level:2},{value:"Multi-Region DR Architecture",id:"multi-region-dr-architecture",level:3},{value:"Automated DR Orchestration",id:"automated-dr-orchestration",level:3},{value:"Monitoring &amp; Alerting",id:"monitoring--alerting",level:2},{value:"Backup Health Monitoring",id:"backup-health-monitoring",level:3},{value:"Recovery Procedures",id:"recovery-procedures",level:2},{value:"Automated Recovery Playbooks",id:"automated-recovery-playbooks",level:3},{value:"Testing &amp; Validation",id:"testing--validation",level:2},{value:"Backup Testing Framework",id:"backup-testing-framework",level:3},{value:"Implementation Roadmap",id:"implementation-roadmap",level:2},{value:"Phase 1: Core Backup Infrastructure (Weeks 1-2)",id:"phase-1-core-backup-infrastructure-weeks-1-2",level:3},{value:"Phase 2: Application-Level Backups (Weeks 3-4)",id:"phase-2-application-level-backups-weeks-3-4",level:3},{value:"Phase 3: Disaster Recovery (Weeks 5-6)",id:"phase-3-disaster-recovery-weeks-5-6",level:3},{value:"Phase 4: Testing &amp; Validation (Weeks 7-8)",id:"phase-4-testing--validation-weeks-7-8",level:3},{value:"Phase 5: Optimization &amp; Enhancement (Weeks 9-10)",id:"phase-5-optimization--enhancement-weeks-9-10",level:3},{value:"Cost Optimization",id:"cost-optimization",level:2},{value:"Backup Storage Lifecycle",id:"backup-storage-lifecycle",level:3},{value:"Compliance &amp; Security",id:"compliance--security",level:2},{value:"Data Protection Compliance",id:"data-protection-compliance",level:3},{value:"Security Measures",id:"security-measures",level:3},{value:"Related Documentation",id:"related-documentation",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"ecommerce-platform-backup--restore-strategy",children:"Ecommerce Platform Backup & Restore Strategy"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsxs)(n.p,{children:["This comprehensive backup and restore strategy is specifically designed for your ecommerce platform built with ",(0,s.jsx)(n.strong,{children:"NestJS microservices"}),", ",(0,s.jsx)(n.strong,{children:"Next.js frontend"}),", and deployed on ",(0,s.jsx)(n.strong,{children:"Google Kubernetes Engine (GKE)"}),". This strategy ensures business continuity, data protection, and rapid recovery capabilities for your production ecommerce environment."]}),"\n",(0,s.jsx)(n.h2,{id:"table-of-contents",children:"Table of Contents"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#architecture-overview",children:"Architecture Overview"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#backup-requirements",children:"Backup Requirements"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#data-classification",children:"Data Classification"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#backup-infrastructure",children:"Backup Infrastructure"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#database-backup-strategy",children:"Database Backup Strategy"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#application-data-backup",children:"Application Data Backup"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#kubernetes-cluster-backup",children:"Kubernetes Cluster Backup"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#file-storage-backup",children:"File Storage Backup"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#configuration--secrets-backup",children:"Configuration & Secrets Backup"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#disaster-recovery-plan",children:"Disaster Recovery Plan"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#monitoring--alerting",children:"Monitoring & Alerting"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#recovery-procedures",children:"Recovery Procedures"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#testing--validation",children:"Testing & Validation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#compliance--security",children:"Compliance & Security"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#implementation-roadmap",children:"Implementation Roadmap"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,s.jsx)(n.h3,{id:"current-tech-stack",children:"Current Tech Stack"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Frontend"}),": Next.js 15+ with App Router, React 19+, Tailwind CSS"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Backend"}),": NestJS microservices with TypeORM"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Infrastructure"}),": Google Kubernetes Engine (GKE)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Databases"}),": Cloud SQL (PostgreSQL), Redis (Memorystore)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Storage"}),": Cloud Storage, Filestore"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Search"}),": Elasticsearch"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Message Queues"}),": Kafka/RabbitMQ"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitoring"}),": Datadog"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CI/CD"}),": GitHub Actions, ArgoCD"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"backup-architecture-diagram",children:"Backup Architecture Diagram"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:'graph TB\n    subgraph "Production Environment"\n        A[Next.js Frontend] --\x3e B[NestJS API Gateway]\n        B --\x3e C[User Service]\n        B --\x3e D[Product Service]\n        B --\x3e E[Order Service]\n        B --\x3e F[Payment Service]\n        B --\x3e G[Inventory Service]\n\n        C --\x3e H[PostgreSQL]\n        D --\x3e H\n        E --\x3e H\n        F --\x3e H\n        G --\x3e H\n\n        A --\x3e I[Redis Cache]\n        B --\x3e I\n\n        J[Product Images] --\x3e K[Cloud Storage]\n        L[Static Assets] --\x3e K\n        M[Search Index] --\x3e N[Elasticsearch]\n    end\n\n    subgraph "Backup Infrastructure"\n        O[Backup Orchestrator]\n        P[Cloud SQL Backups]\n        Q[Redis Snapshots]\n        R[File Storage Backups]\n        S[Configuration Backups]\n        T[Kubernetes Backups]\n        U[Elasticsearch Backups]\n    end\n\n    subgraph "Recovery Environments"\n        V[Staging Environment]\n        W[DR Environment]\n        X[Test Restoration]\n    end\n\n    H --\x3e P\n    I --\x3e Q\n    K --\x3e R\n    N --\x3e U\n    O --\x3e P\n    O --\x3e Q\n    O --\x3e R\n    O --\x3e S\n    O --\x3e T\n    O --\x3e U\n\n    P --\x3e V\n    P --\x3e W\n    P --\x3e X\n'})}),"\n",(0,s.jsx)(n.h2,{id:"backup-requirements",children:"Backup Requirements"}),"\n",(0,s.jsx)(n.h3,{id:"recovery-objectives",children:"Recovery Objectives"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Component"}),(0,s.jsx)(n.th,{children:"RTO (Recovery Time Objective)"}),(0,s.jsx)(n.th,{children:"RPO (Recovery Point Objective)"}),(0,s.jsx)(n.th,{children:"Backup Frequency"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Customer Database"}),(0,s.jsx)(n.td,{children:"15 minutes"}),(0,s.jsx)(n.td,{children:"5 minutes"}),(0,s.jsx)(n.td,{children:"Continuous + Hourly"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Product Catalog"}),(0,s.jsx)(n.td,{children:"30 minutes"}),(0,s.jsx)(n.td,{children:"15 minutes"}),(0,s.jsx)(n.td,{children:"Every 4 hours"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Order Database"}),(0,s.jsx)(n.td,{children:"5 minutes"}),(0,s.jsx)(n.td,{children:"1 minute"}),(0,s.jsx)(n.td,{children:"Continuous + Every 15 min"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Payment Data"}),(0,s.jsx)(n.td,{children:"5 minutes"}),(0,s.jsx)(n.td,{children:"30 seconds"}),(0,s.jsx)(n.td,{children:"Continuous + Every 5 min"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Inventory Data"}),(0,s.jsx)(n.td,{children:"10 minutes"}),(0,s.jsx)(n.td,{children:"5 minutes"}),(0,s.jsx)(n.td,{children:"Every hour"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"User Sessions (Redis)"}),(0,s.jsx)(n.td,{children:"5 minutes"}),(0,s.jsx)(n.td,{children:"15 minutes"}),(0,s.jsx)(n.td,{children:"Every 30 minutes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Product Images"}),(0,s.jsx)(n.td,{children:"2 hours"}),(0,s.jsx)(n.td,{children:"1 hour"}),(0,s.jsx)(n.td,{children:"Daily"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Static Assets"}),(0,s.jsx)(n.td,{children:"4 hours"}),(0,s.jsx)(n.td,{children:"4 hours"}),(0,s.jsx)(n.td,{children:"Daily"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Search Indices"}),(0,s.jsx)(n.td,{children:"1 hour"}),(0,s.jsx)(n.td,{children:"30 minutes"}),(0,s.jsx)(n.td,{children:"Every 2 hours"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Configuration"}),(0,s.jsx)(n.td,{children:"30 minutes"}),(0,s.jsx)(n.td,{children:"1 hour"}),(0,s.jsx)(n.td,{children:"On change + Daily"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"business-impact-classification",children:"Business Impact Classification"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Critical"}),": Customer data, active orders, payment transactions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High"}),": Product catalog, inventory, user authentication"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Medium"}),": Search indices, cached data, static content"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Low"}),": Logs, analytics data, temporary files"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"data-classification",children:"Data Classification"}),"\n",(0,s.jsx)(n.h3,{id:"ecommerce-data-categories",children:"Ecommerce Data Categories"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// Data classification for backup prioritization\nexport enum DataClassification {\n  CRITICAL = 'critical', // Financial, PII, active transactions\n  SENSITIVE = 'sensitive', // Customer data, order history\n  INTERNAL = 'internal', // Product info, inventory\n  PUBLIC = 'public', // Static content, cached data\n}\n\nexport interface BackupPolicy {\n  classification: DataClassification;\n  retentionDays: number;\n  encryptionRequired: boolean;\n  compressionLevel: number;\n  replicationRegions: string[];\n  complianceRequirements: string[];\n}\n\nexport const ECOMMERCE_BACKUP_POLICIES: Record<DataClassification, BackupPolicy> = {\n  [DataClassification.CRITICAL]: {\n    classification: DataClassification.CRITICAL,\n    retentionDays: 2555, // 7 years for financial data\n    encryptionRequired: true,\n    compressionLevel: 9,\n    replicationRegions: ['us-central1', 'us-east1', 'europe-west1'],\n    complianceRequirements: ['PCI-DSS', 'GDPR', 'SOX'],\n  },\n  [DataClassification.SENSITIVE]: {\n    classification: DataClassification.SENSITIVE,\n    retentionDays: 1095, // 3 years for customer data\n    encryptionRequired: true,\n    compressionLevel: 7,\n    replicationRegions: ['us-central1', 'us-east1'],\n    complianceRequirements: ['GDPR', 'CCPA'],\n  },\n  [DataClassification.INTERNAL]: {\n    classification: DataClassification.INTERNAL,\n    retentionDays: 365, // 1 year for business data\n    encryptionRequired: true,\n    compressionLevel: 5,\n    replicationRegions: ['us-central1'],\n    complianceRequirements: [],\n  },\n  [DataClassification.PUBLIC]: {\n    classification: DataClassification.PUBLIC,\n    retentionDays: 90, // 3 months for public data\n    encryptionRequired: false,\n    compressionLevel: 3,\n    replicationRegions: ['us-central1'],\n    complianceRequirements: [],\n  },\n};\n"})}),"\n",(0,s.jsx)(n.h2,{id:"backup-infrastructure",children:"Backup Infrastructure"}),"\n",(0,s.jsx)(n.h3,{id:"google-cloud-infrastructure-setup",children:"Google Cloud Infrastructure Setup"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-hcl",children:'# terraform/modules/backup-infrastructure/main.tf\n\n# Backup storage buckets with lifecycle policies\nresource "google_storage_bucket" "backup_critical" {\n  name     = "${var.project_id}-ecommerce-backups-critical"\n  location = var.region\n\n  versioning {\n    enabled = true\n  }\n\n  encryption {\n    default_kms_key_name = google_kms_crypto_key.backup_key.id\n  }\n\n  lifecycle_rule {\n    condition {\n      age = 30\n    }\n    action {\n      type          = "SetStorageClass"\n      storage_class = "NEARLINE"\n    }\n  }\n\n  lifecycle_rule {\n    condition {\n      age = 90\n    }\n    action {\n      type          = "SetStorageClass"\n      storage_class = "COLDLINE"\n    }\n  }\n\n  lifecycle_rule {\n    condition {\n      age = 365\n    }\n    action {\n      type          = "SetStorageClass"\n      storage_class = "ARCHIVE"\n    }\n  }\n}\n\n# Cross-region replication for critical backups\nresource "google_storage_bucket" "backup_critical_replica" {\n  name     = "${var.project_id}-ecommerce-backups-critical-replica"\n  location = var.backup_region\n\n  versioning {\n    enabled = true\n  }\n\n  encryption {\n    default_kms_key_name = google_kms_crypto_key.backup_key.id\n  }\n}\n\n# KMS key for backup encryption\nresource "google_kms_crypto_key" "backup_key" {\n  name     = "ecommerce-backup-key"\n  key_ring = google_kms_key_ring.backup_keyring.id\n\n  lifecycle {\n    prevent_destroy = true\n  }\n}\n\nresource "google_kms_key_ring" "backup_keyring" {\n  name     = "ecommerce-backup-keyring"\n  location = var.region\n}\n\n# Cloud SQL backup configuration\nresource "google_sql_database_instance" "ecommerce_primary" {\n  name             = "ecommerce-primary"\n  database_version = "POSTGRES_14"\n  region           = var.region\n\n  settings {\n    tier = var.db_tier\n\n    backup_configuration {\n      enabled                        = true\n      start_time                    = "02:00"\n      point_in_time_recovery_enabled = true\n      location                      = var.region\n      backup_retention_settings {\n        retained_backups = 30\n        retention_unit   = "COUNT"\n      }\n      transaction_log_retention_days = 7\n    }\n\n    replica_configuration {\n      failover_target = true\n    }\n  }\n\n  replica_configuration {\n    failover_target = false\n  }\n}\n\n# Read replica for backup offloading\nresource "google_sql_database_instance" "ecommerce_replica" {\n  name                 = "ecommerce-replica"\n  master_instance_name = google_sql_database_instance.ecommerce_primary.name\n  region              = var.backup_region\n  database_version    = "POSTGRES_14"\n\n  settings {\n    tier = var.db_tier\n\n    backup_configuration {\n      enabled = false # Backups handled by master\n    }\n  }\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"database-backup-strategy",children:"Database Backup Strategy"}),"\n",(0,s.jsx)(n.h3,{id:"postgresql-microservices-backup",children:"PostgreSQL Microservices Backup"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// src/shared/backup/ecommerce-database-backup.service.ts\nimport { Injectable, Logger } from '@nestjs/common';\nimport { ConfigService } from '@nestjs/config';\nimport { Cron } from '@nestjs/schedule';\nimport { DataClassification, ECOMMERCE_BACKUP_POLICIES } from './backup-policies';\n\ninterface DatabaseBackupConfig {\n  serviceName: string;\n  database: string;\n  classification: DataClassification;\n  tables: string[];\n  customBackupScript?: string;\n}\n\n@Injectable()\nexport class EcommerceDatabaseBackupService {\n  private readonly logger = new Logger(EcommerceDatabaseBackupService.name);\n\n  private readonly serviceConfigs: DatabaseBackupConfig[] = [\n    {\n      serviceName: 'user-service',\n      database: 'users_db',\n      classification: DataClassification.SENSITIVE,\n      tables: ['users', 'user_profiles', 'user_preferences', 'user_sessions'],\n    },\n    {\n      serviceName: 'product-service',\n      database: 'products_db',\n      classification: DataClassification.INTERNAL,\n      tables: ['products', 'categories', 'product_variants', 'product_reviews'],\n    },\n    {\n      serviceName: 'order-service',\n      database: 'orders_db',\n      classification: DataClassification.CRITICAL,\n      tables: ['orders', 'order_items', 'order_status_history', 'order_payments'],\n    },\n    {\n      serviceName: 'payment-service',\n      database: 'payments_db',\n      classification: DataClassification.CRITICAL,\n      tables: ['payment_methods', 'transactions', 'payment_logs'],\n    },\n    {\n      serviceName: 'inventory-service',\n      database: 'inventory_db',\n      classification: DataClassification.INTERNAL,\n      tables: ['inventory', 'stock_movements', 'reservations'],\n    },\n  ];\n\n  constructor(private readonly configService: ConfigService) {}\n\n  // Critical data backup every 5 minutes\n  @Cron('*/5 * * * *')\n  async backupCriticalData(): Promise<void> {\n    const criticalServices = this.serviceConfigs.filter(\n      (config) => config.classification === DataClassification.CRITICAL,\n    );\n\n    for (const config of criticalServices) {\n      await this.createIncrementalBackup(config);\n    }\n  }\n\n  // Sensitive data backup every 15 minutes\n  @Cron('*/15 * * * *')\n  async backupSensitiveData(): Promise<void> {\n    const sensitiveServices = this.serviceConfigs.filter(\n      (config) => config.classification === DataClassification.SENSITIVE,\n    );\n\n    for (const config of sensitiveServices) {\n      await this.createIncrementalBackup(config);\n    }\n  }\n\n  // Full backup daily at 2 AM\n  @Cron('0 2 * * *')\n  async createFullBackups(): Promise<void> {\n    this.logger.log('Starting full ecommerce database backups');\n\n    for (const config of this.serviceConfigs) {\n      await this.createFullBackup(config);\n    }\n  }\n\n  private async createFullBackup(config: DatabaseBackupConfig): Promise<void> {\n    const startTime = Date.now();\n    const backupId = `${config.serviceName}_full_${Date.now()}`;\n\n    try {\n      this.logger.log(`Creating full backup for ${config.serviceName}`);\n\n      // Use Cloud SQL backup for production\n      await this.triggerCloudSQLBackup(config.database, backupId);\n\n      // Create application-level backup for specific tables\n      await this.createApplicationBackup(config, backupId, 'full');\n\n      // Validate backup integrity\n      await this.validateBackup(backupId);\n\n      const duration = Date.now() - startTime;\n      this.logger.log(`Full backup completed for ${config.serviceName} in ${duration}ms`);\n    } catch (error) {\n      this.logger.error(`Full backup failed for ${config.serviceName}: ${error.message}`);\n      await this.sendBackupAlert(config.serviceName, 'full_backup_failed', error.message);\n      throw error;\n    }\n  }\n\n  private async createIncrementalBackup(config: DatabaseBackupConfig): Promise<void> {\n    const startTime = Date.now();\n    const backupId = `${config.serviceName}_incremental_${Date.now()}`;\n\n    try {\n      // Get changes since last backup\n      const lastBackupTime = await this.getLastBackupTimestamp(config.serviceName);\n\n      // Create incremental backup with WAL files\n      await this.createWALBackup(config, backupId, lastBackupTime);\n\n      // For critical data, also create a point-in-time snapshot\n      if (config.classification === DataClassification.CRITICAL) {\n        await this.createPointInTimeSnapshot(config, backupId);\n      }\n\n      const duration = Date.now() - startTime;\n      this.logger.log(`Incremental backup completed for ${config.serviceName} in ${duration}ms`);\n    } catch (error) {\n      this.logger.error(`Incremental backup failed for ${config.serviceName}: ${error.message}`);\n      await this.sendBackupAlert(config.serviceName, 'incremental_backup_failed', error.message);\n      throw error;\n    }\n  }\n\n  private async triggerCloudSQLBackup(database: string, backupId: string): Promise<void> {\n    // Use Google Cloud SQL Admin API to trigger backup\n    const { execSync } = require('child_process');\n\n    const command = `gcloud sql backups create --instance=${database} --description=\"${backupId}\"`;\n    execSync(command);\n  }\n\n  private async createApplicationBackup(\n    config: DatabaseBackupConfig,\n    backupId: string,\n    type: 'full' | 'incremental',\n  ): Promise<void> {\n    const policy = ECOMMERCE_BACKUP_POLICIES[config.classification];\n\n    // Create encrypted backup with compression\n    const backupCommand = this.buildBackupCommand(config, backupId, type, policy);\n\n    // Execute backup\n    const { execSync } = require('child_process');\n    execSync(backupCommand);\n\n    // Upload to multiple regions for critical data\n    if (config.classification === DataClassification.CRITICAL) {\n      await this.replicateToMultipleRegions(backupId, policy.replicationRegions);\n    }\n  }\n\n  private buildBackupCommand(config: DatabaseBackupConfig, backupId: string, type: string, policy: any): string {\n    const host = this.configService.get('DB_HOST');\n    const port = this.configService.get('DB_PORT');\n    const username = this.configService.get('DB_USERNAME');\n\n    let command = `pg_dump -h ${host} -p ${port} -U ${username} -d ${config.database}`;\n\n    // Add table-specific options\n    if (config.tables.length > 0) {\n      const tableArgs = config.tables.map((table) => `-t ${table}`).join(' ');\n      command += ` ${tableArgs}`;\n    }\n\n    // Add compression based on classification\n    command += ` --compress=${policy.compressionLevel}`;\n\n    // Encryption for sensitive data\n    if (policy.encryptionRequired) {\n      command += ` | gpg --symmetric --cipher-algo AES256`;\n    }\n\n    return command;\n  }\n\n  private async validateBackup(backupId: string): Promise<void> {\n    // Implement backup validation logic\n    // 1. Verify file integrity\n    // 2. Test restore to staging environment\n    // 3. Validate data consistency\n  }\n\n  private async sendBackupAlert(serviceName: string, alertType: string, message: string): Promise<void> {\n    // Send alert to monitoring system (Datadog, Slack, etc.)\n    this.logger.error(`BACKUP ALERT [${serviceName}] ${alertType}: ${message}`);\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"application-data-backup",children:"Application Data Backup"}),"\n",(0,s.jsx)(n.h3,{id:"redis-cache-backup-strategy",children:"Redis Cache Backup Strategy"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// src/shared/backup/redis-backup.service.ts\nimport { Injectable, Logger } from '@nestjs/common';\nimport Redis from 'ioredis';\nimport { Cron } from '@nestjs/schedule';\n\n@Injectable()\nexport class RedisBackupService {\n  private readonly logger = new Logger(RedisBackupService.name);\n  private readonly redis: Redis;\n\n  constructor() {\n    this.redis = new Redis({\n      host: process.env.REDIS_HOST,\n      port: parseInt(process.env.REDIS_PORT || '6379'),\n      password: process.env.REDIS_PASSWORD,\n    });\n  }\n\n  // Backup user sessions every 30 minutes\n  @Cron('*/30 * * * *')\n  async backupUserSessions(): Promise<void> {\n    await this.backupRedisPattern('session:*', 'user-sessions');\n  }\n\n  // Backup shopping carts every 15 minutes\n  @Cron('*/15 * * * *')\n  async backupShoppingCarts(): Promise<void> {\n    await this.backupRedisPattern('cart:*', 'shopping-carts');\n  }\n\n  // Backup product cache every hour\n  @Cron('0 * * * *')\n  async backupProductCache(): Promise<void> {\n    await this.backupRedisPattern('product:*', 'product-cache');\n  }\n\n  private async backupRedisPattern(pattern: string, backupType: string): Promise<void> {\n    try {\n      const keys = await this.redis.keys(pattern);\n      const backupData: Record<string, any> = {};\n\n      // Batch process keys to avoid memory issues\n      const batchSize = 1000;\n      for (let i = 0; i < keys.length; i += batchSize) {\n        const batch = keys.slice(i, i + batchSize);\n        const pipeline = this.redis.pipeline();\n\n        batch.forEach((key) => pipeline.get(key));\n        const results = await pipeline.exec();\n\n        batch.forEach((key, index) => {\n          if (results && results[index] && results[index][1]) {\n            backupData[key] = results[index][1];\n          }\n        });\n      }\n\n      // Upload to Cloud Storage\n      await this.uploadRedisBackup(backupData, backupType);\n\n      this.logger.log(`Redis backup completed for ${backupType}: ${keys.length} keys`);\n    } catch (error) {\n      this.logger.error(`Redis backup failed for ${backupType}: ${error.message}`);\n      throw error;\n    }\n  }\n\n  private async uploadRedisBackup(data: Record<string, any>, backupType: string): Promise<void> {\n    const { Storage } = require('@google-cloud/storage');\n    const storage = new Storage();\n\n    const bucketName = process.env.BACKUP_BUCKET;\n    const fileName = `redis-backups/${backupType}/${Date.now()}.json`;\n\n    const file = storage.bucket(bucketName).file(fileName);\n    await file.save(JSON.stringify(data), {\n      metadata: {\n        contentType: 'application/json',\n      },\n    });\n  }\n\n  async restoreRedisData(backupFile: string): Promise<void> {\n    // Implementation for Redis data restoration\n    try {\n      const { Storage } = require('@google-cloud/storage');\n      const storage = new Storage();\n\n      const bucketName = process.env.BACKUP_BUCKET;\n      const file = storage.bucket(bucketName).file(backupFile);\n\n      const [contents] = await file.download();\n      const backupData = JSON.parse(contents.toString());\n\n      // Restore data in batches\n      const pipeline = this.redis.pipeline();\n      Object.entries(backupData).forEach(([key, value]) => {\n        pipeline.set(key, value);\n      });\n\n      await pipeline.exec();\n      this.logger.log(`Redis data restored from ${backupFile}`);\n    } catch (error) {\n      this.logger.error(`Redis restore failed: ${error.message}`);\n      throw error;\n    }\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"kubernetes-cluster-backup",children:"Kubernetes Cluster Backup"}),"\n",(0,s.jsx)(n.h3,{id:"velero-backup-configuration",children:"Velero Backup Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# k8s/backup/velero-backup-config.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: velero-backup-config\n  namespace: velero\ndata:\n  backup-locations.yaml: |\n    apiVersion: velero.io/v1\n    kind: BackupStorageLocation\n    metadata:\n      name: gcp-backup-location\n      namespace: velero\n    spec:\n      provider: gcp\n      objectStorage:\n        bucket: ecommerce-k8s-backups\n        prefix: velero\n      config:\n        serviceAccount: velero-backup-sa@project.iam.gserviceaccount.com\n\n---\napiVersion: velero.io/v1\nkind: Schedule\nmetadata:\n  name: ecommerce-daily-backup\n  namespace: velero\nspec:\n  schedule: \'0 3 * * *\' # Daily at 3 AM\n  template:\n    includeClusterResources: true\n    includedNamespaces:\n      - ecommerce-prod\n      - ecommerce-staging\n      - monitoring\n      - ingress-nginx\n    excludedResources:\n      - pods\n      - replicasets\n      - secrets\n    storageLocation: gcp-backup-location\n    ttl: 720h # 30 days retention\n\n---\napiVersion: velero.io/v1\nkind: Schedule\nmetadata:\n  name: ecommerce-critical-backup\n  namespace: velero\nspec:\n  schedule: \'*/15 * * * *\' # Every 15 minutes\n  template:\n    includeClusterResources: false\n    includedNamespaces:\n      - ecommerce-prod\n    includedResources:\n      - persistentvolumes\n      - persistentvolumeclaims\n      - configmaps\n      - services\n    storageLocation: gcp-backup-location\n    ttl: 168h # 7 days retention\n\n---\n# Backup script for manual triggers\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: backup-scripts\n  namespace: ecommerce-prod\ndata:\n  manual-backup.sh: |\n    #!/bin/bash\n\n    # Manual backup script for emergency situations\n    BACKUP_NAME="manual-backup-$(date +%Y%m%d-%H%M%S)"\n\n    echo "Creating manual backup: $BACKUP_NAME"\n\n    # Create immediate backup\n    velero backup create $BACKUP_NAME \\\n      --include-namespaces ecommerce-prod \\\n      --include-cluster-resources=true \\\n      --wait\n\n    # Check backup status\n    velero backup describe $BACKUP_NAME\n\n    # Export backup logs\n    velero backup logs $BACKUP_NAME > /tmp/${BACKUP_NAME}.log\n\n    echo "Manual backup completed: $BACKUP_NAME"\n\n  restore-from-backup.sh: |\n    #!/bin/bash\n\n    BACKUP_NAME=$1\n\n    if [ -z "$BACKUP_NAME" ]; then\n      echo "Usage: $0 <backup-name>"\n      exit 1\n    fi\n\n    echo "Restoring from backup: $BACKUP_NAME"\n\n    # Create restore\n    RESTORE_NAME="restore-$(date +%Y%m%d-%H%M%S)"\n    velero restore create $RESTORE_NAME \\\n      --from-backup $BACKUP_NAME \\\n      --wait\n\n    # Check restore status\n    velero restore describe $RESTORE_NAME\n\n    echo "Restore completed: $RESTORE_NAME"\n'})}),"\n",(0,s.jsx)(n.h2,{id:"file-storage-backup",children:"File Storage Backup"}),"\n",(0,s.jsx)(n.h3,{id:"product-images-and-static-assets",children:"Product Images and Static Assets"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// src/shared/backup/file-storage-backup.service.ts\nimport { Injectable, Logger } from '@nestjs/common';\nimport { Storage } from '@google-cloud/storage';\nimport { Cron } from '@nestjs/schedule';\n\n@Injectable()\nexport class FileStorageBackupService {\n  private readonly logger = new Logger(FileStorageBackupService.name);\n  private readonly storage = new Storage();\n\n  // Daily backup of product images\n  @Cron('0 4 * * *')\n  async backupProductImages(): Promise<void> {\n    await this.syncBucketToBackup('ecommerce-product-images', 'ecommerce-backups-product-images', 'product-images');\n  }\n\n  // Daily backup of static assets\n  @Cron('0 5 * * *')\n  async backupStaticAssets(): Promise<void> {\n    await this.syncBucketToBackup('ecommerce-static-assets', 'ecommerce-backups-static-assets', 'static-assets');\n  }\n\n  // Weekly backup of user uploaded content\n  @Cron('0 6 * * 0')\n  async backupUserContent(): Promise<void> {\n    await this.syncBucketToBackup('ecommerce-user-content', 'ecommerce-backups-user-content', 'user-content');\n  }\n\n  private async syncBucketToBackup(sourceBucket: string, backupBucket: string, contentType: string): Promise<void> {\n    try {\n      this.logger.log(`Starting backup sync for ${contentType}`);\n\n      const [sourceFiles] = await this.storage.bucket(sourceBucket).getFiles();\n      const backupBucketRef = this.storage.bucket(backupBucket);\n\n      let copiedFiles = 0;\n      let skippedFiles = 0;\n\n      for (const file of sourceFiles) {\n        try {\n          const backupFileName = `${new Date().toISOString().split('T')[0]}/${file.name}`;\n          const backupFile = backupBucketRef.file(backupFileName);\n\n          // Check if file already exists in backup\n          const [exists] = await backupFile.exists();\n          if (!exists) {\n            await file.copy(backupFile);\n            copiedFiles++;\n          } else {\n            skippedFiles++;\n          }\n        } catch (error) {\n          this.logger.error(`Failed to backup file ${file.name}: ${error.message}`);\n        }\n      }\n\n      this.logger.log(`Backup sync completed for ${contentType}: ${copiedFiles} copied, ${skippedFiles} skipped`);\n    } catch (error) {\n      this.logger.error(`Backup sync failed for ${contentType}: ${error.message}`);\n      throw error;\n    }\n  }\n\n  async restoreFilesFromBackup(backupBucket: string, targetBucket: string, backupDate: string): Promise<void> {\n    try {\n      this.logger.log(`Starting file restore from ${backupDate}`);\n\n      const [backupFiles] = await this.storage.bucket(backupBucket).getFiles({ prefix: backupDate });\n\n      const targetBucketRef = this.storage.bucket(targetBucket);\n      let restoredFiles = 0;\n\n      for (const file of backupFiles) {\n        try {\n          const originalFileName = file.name.replace(`${backupDate}/`, '');\n          const targetFile = targetBucketRef.file(originalFileName);\n\n          await file.copy(targetFile);\n          restoredFiles++;\n        } catch (error) {\n          this.logger.error(`Failed to restore file ${file.name}: ${error.message}`);\n        }\n      }\n\n      this.logger.log(`File restore completed: ${restoredFiles} files restored`);\n    } catch (error) {\n      this.logger.error(`File restore failed: ${error.message}`);\n      throw error;\n    }\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"configuration--secrets-backup",children:"Configuration & Secrets Backup"}),"\n",(0,s.jsx)(n.h3,{id:"kubernetes-secrets-and-configmaps",children:"Kubernetes Secrets and ConfigMaps"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# scripts/backup-k8s-configs.sh\n\n# Backup Kubernetes configurations and secrets\nBACKUP_DATE=$(date +%Y%m%d-%H%M%S)\nBACKUP_DIR="/tmp/k8s-config-backup-$BACKUP_DATE"\nBUCKET_NAME="ecommerce-config-backups"\n\nmkdir -p "$BACKUP_DIR"\n\necho "Starting Kubernetes configuration backup..."\n\n# Export namespaces\nkubectl get namespaces -o yaml > "$BACKUP_DIR/namespaces.yaml"\n\n# Export secrets (base64 encoded, will be encrypted before storage)\nfor namespace in ecommerce-prod ecommerce-staging monitoring; do\n  mkdir -p "$BACKUP_DIR/secrets/$namespace"\n  kubectl get secrets -n "$namespace" -o yaml > "$BACKUP_DIR/secrets/$namespace/secrets.yaml"\ndone\n\n# Export configmaps\nfor namespace in ecommerce-prod ecommerce-staging monitoring; do\n  mkdir -p "$BACKUP_DIR/configmaps/$namespace"\n  kubectl get configmaps -n "$namespace" -o yaml > "$BACKUP_DIR/configmaps/$namespace/configmaps.yaml"\ndone\n\n# Export custom resources\nkubectl get crd -o yaml > "$BACKUP_DIR/crds.yaml"\n\n# Export ingress configurations\nkubectl get ingress --all-namespaces -o yaml > "$BACKUP_DIR/ingress.yaml"\n\n# Export service accounts\nkubectl get serviceaccounts --all-namespaces -o yaml > "$BACKUP_DIR/serviceaccounts.yaml"\n\n# Export RBAC\nkubectl get clusterroles -o yaml > "$BACKUP_DIR/clusterroles.yaml"\nkubectl get clusterrolebindings -o yaml > "$BACKUP_DIR/clusterrolebindings.yaml"\nkubectl get roles --all-namespaces -o yaml > "$BACKUP_DIR/roles.yaml"\nkubectl get rolebindings --all-namespaces -o yaml > "$BACKUP_DIR/rolebindings.yaml"\n\n# Encrypt the backup\ntar -czf "$BACKUP_DIR.tar.gz" -C "/tmp" "k8s-config-backup-$BACKUP_DATE"\ngpg --symmetric --cipher-algo AES256 --output "$BACKUP_DIR.tar.gz.gpg" "$BACKUP_DIR.tar.gz"\n\n# Upload to Google Cloud Storage\ngsutil cp "$BACKUP_DIR.tar.gz.gpg" "gs://$BUCKET_NAME/k8s-configs/$BACKUP_DATE.tar.gz.gpg"\n\n# Cleanup local files\nrm -rf "$BACKUP_DIR" "$BACKUP_DIR.tar.gz" "$BACKUP_DIR.tar.gz.gpg"\n\necho "Kubernetes configuration backup completed: $BACKUP_DATE"\n'})}),"\n",(0,s.jsx)(n.h2,{id:"disaster-recovery-plan",children:"Disaster Recovery Plan"}),"\n",(0,s.jsx)(n.h3,{id:"multi-region-dr-architecture",children:"Multi-Region DR Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# terraform/disaster-recovery/main.tf\n# Disaster Recovery infrastructure in secondary region\n\nresource "google_container_cluster" "dr_cluster" {\n  name     = "ecommerce-dr-cluster"\n  location = var.dr_region\n\n  # Minimal node configuration for cost optimization\n  initial_node_count       = 1\n  remove_default_node_pool = true\n\n  workload_identity_config {\n    workload_pool = "${var.project_id}.svc.id.goog"\n  }\n}\n\nresource "google_container_node_pool" "dr_primary_nodes" {\n  name       = "dr-primary-nodes"\n  location   = var.dr_region\n  cluster    = google_container_cluster.dr_cluster.name\n\n  initial_node_count = 1\n\n  autoscaling {\n    min_node_count = 1\n    max_node_count = 10\n  }\n\n  node_config {\n    machine_type = "e2-standard-2"\n\n    oauth_scopes = [\n      "https://www.googleapis.com/auth/cloud-platform"\n    ]\n  }\n}\n\n# DR database (read replica promotion target)\nresource "google_sql_database_instance" "dr_database" {\n  name             = "ecommerce-dr-database"\n  database_version = "POSTGRES_14"\n  region           = var.dr_region\n\n  settings {\n    tier = "db-custom-2-4096"\n\n    backup_configuration {\n      enabled = false  # Managed by primary\n    }\n  }\n}\n\n# DR Redis instance\nresource "google_redis_instance" "dr_redis" {\n  name           = "ecommerce-dr-redis"\n  tier           = "STANDARD_HA"\n  memory_size_gb = 4\n  region         = var.dr_region\n\n  replica_count = 1\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"automated-dr-orchestration",children:"Automated DR Orchestration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// src/shared/disaster-recovery/dr-orchestrator.service.ts\nimport { Injectable, Logger } from '@nestjs/common';\n\nexport interface DRScenario {\n  name: string;\n  triggerConditions: string[];\n  recoverySteps: DRStep[];\n  estimatedRTO: number; // minutes\n  estimatedRPO: number; // minutes\n}\n\nexport interface DRStep {\n  name: string;\n  action: string;\n  timeout: number;\n  rollbackAction?: string;\n  dependencies?: string[];\n}\n\n@Injectable()\nexport class DisasterRecoveryOrchestrator {\n  private readonly logger = new Logger(DisasterRecoveryOrchestrator.name);\n\n  private readonly drScenarios: DRScenario[] = [\n    {\n      name: 'database_failure',\n      triggerConditions: ['db_connection_lost', 'db_corruption_detected'],\n      estimatedRTO: 15,\n      estimatedRPO: 5,\n      recoverySteps: [\n        {\n          name: 'switch_to_replica',\n          action: 'promote_read_replica',\n          timeout: 300000, // 5 minutes\n          rollbackAction: 'demote_replica',\n        },\n        {\n          name: 'update_connection_strings',\n          action: 'update_k8s_secrets',\n          timeout: 120000, // 2 minutes\n          dependencies: ['switch_to_replica'],\n        },\n        {\n          name: 'restart_services',\n          action: 'rolling_restart_pods',\n          timeout: 600000, // 10 minutes\n          dependencies: ['update_connection_strings'],\n        },\n      ],\n    },\n    {\n      name: 'region_failure',\n      triggerConditions: ['region_unavailable', 'multi_service_failure'],\n      estimatedRTO: 60,\n      estimatedRPO: 15,\n      recoverySteps: [\n        {\n          name: 'activate_dr_cluster',\n          action: 'scale_dr_k8s_cluster',\n          timeout: 900000, // 15 minutes\n        },\n        {\n          name: 'restore_from_backup',\n          action: 'restore_latest_backup',\n          timeout: 1800000, // 30 minutes\n          dependencies: ['activate_dr_cluster'],\n        },\n        {\n          name: 'switch_dns',\n          action: 'update_dns_records',\n          timeout: 300000, // 5 minutes\n          dependencies: ['restore_from_backup'],\n        },\n        {\n          name: 'validate_services',\n          action: 'run_health_checks',\n          timeout: 600000, // 10 minutes\n          dependencies: ['switch_dns'],\n        },\n      ],\n    },\n  ];\n\n  async executeDisasterRecovery(scenarioName: string): Promise<void> {\n    const scenario = this.drScenarios.find((s) => s.name === scenarioName);\n    if (!scenario) {\n      throw new Error(`Unknown DR scenario: ${scenarioName}`);\n    }\n\n    this.logger.log(`Starting disaster recovery for scenario: ${scenarioName}`);\n\n    try {\n      await this.notifyStakeholders('dr_started', scenario);\n\n      for (const step of scenario.recoverySteps) {\n        await this.executeRecoveryStep(step, scenario);\n      }\n\n      await this.validateRecovery(scenario);\n      await this.notifyStakeholders('dr_completed', scenario);\n    } catch (error) {\n      this.logger.error(`DR execution failed: ${error.message}`);\n      await this.notifyStakeholders('dr_failed', scenario, error);\n      throw error;\n    }\n  }\n\n  private async executeRecoveryStep(step: DRStep, scenario: DRScenario): Promise<void> {\n    this.logger.log(`Executing recovery step: ${step.name}`);\n\n    try {\n      switch (step.action) {\n        case 'promote_read_replica':\n          await this.promoteReadReplica();\n          break;\n        case 'update_k8s_secrets':\n          await this.updateKubernetesSecrets();\n          break;\n        case 'rolling_restart_pods':\n          await this.rollingRestartPods();\n          break;\n        case 'scale_dr_k8s_cluster':\n          await this.scaleDRCluster();\n          break;\n        case 'restore_latest_backup':\n          await this.restoreLatestBackup();\n          break;\n        case 'update_dns_records':\n          await this.updateDNSRecords();\n          break;\n        case 'run_health_checks':\n          await this.runHealthChecks();\n          break;\n        default:\n          throw new Error(`Unknown recovery action: ${step.action}`);\n      }\n\n      this.logger.log(`Recovery step completed: ${step.name}`);\n    } catch (error) {\n      this.logger.error(`Recovery step failed: ${step.name} - ${error.message}`);\n\n      if (step.rollbackAction) {\n        await this.executeRollback(step);\n      }\n\n      throw error;\n    }\n  }\n\n  private async promoteReadReplica(): Promise<void> {\n    // Promote read replica to master\n    const { execSync } = require('child_process');\n    execSync('gcloud sql instances promote-replica ecommerce-replica');\n  }\n\n  private async updateKubernetesSecrets(): Promise<void> {\n    // Update database connection secrets in Kubernetes\n    execSync(\n      `kubectl patch secret db-credentials -p '{\"data\":{\"host\":\"${Buffer.from('new-db-host').toString('base64')}\"}}'`,\n    );\n  }\n\n  private async rollingRestartPods(): Promise<void> {\n    // Restart all application pods to pick up new configuration\n    const deployments = ['user-service', 'product-service', 'order-service', 'payment-service'];\n\n    for (const deployment of deployments) {\n      execSync(`kubectl rollout restart deployment/${deployment} -n ecommerce-prod`);\n      execSync(`kubectl rollout status deployment/${deployment} -n ecommerce-prod --timeout=300s`);\n    }\n  }\n\n  private async notifyStakeholders(event: string, scenario: DRScenario, error?: Error): Promise<void> {\n    // Send notifications via Slack, email, SMS\n    const message = `DR Event: ${event} - Scenario: ${scenario.name}${error ? ` - Error: ${error.message}` : ''}`;\n    this.logger.log(`Notification: ${message}`);\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"monitoring--alerting",children:"Monitoring & Alerting"}),"\n",(0,s.jsx)(n.h3,{id:"backup-health-monitoring",children:"Backup Health Monitoring"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// src/shared/monitoring/backup-monitoring.service.ts\nimport { Injectable, Logger } from '@nestjs/common';\nimport { Cron } from '@nestjs/schedule';\n\n@Injectable()\nexport class BackupMonitoringService {\n  private readonly logger = new Logger(BackupMonitoringService.name);\n\n  // Monitor backup health every 30 minutes\n  @Cron('*/30 * * * *')\n  async monitorBackupHealth(): Promise<void> {\n    await Promise.all([\n      this.checkDatabaseBackups(),\n      this.checkFileBackups(),\n      this.checkRedisBackups(),\n      this.checkKubernetesBackups(),\n    ]);\n  }\n\n  private async checkDatabaseBackups(): Promise<void> {\n    const services = ['user-service', 'product-service', 'order-service', 'payment-service'];\n\n    for (const service of services) {\n      const lastBackup = await this.getLastBackupTime(service);\n      const timeSinceBackup = Date.now() - lastBackup.getTime();\n\n      // Alert if backup is older than expected interval\n      if (timeSinceBackup > this.getExpectedInterval(service)) {\n        await this.sendAlert('backup_overdue', service, {\n          lastBackup: lastBackup.toISOString(),\n          timeSinceBackup: Math.round(timeSinceBackup / 1000 / 60), // minutes\n        });\n      }\n    }\n  }\n\n  private async sendAlert(type: string, service: string, data: any): Promise<void> {\n    // Integration with monitoring systems\n    this.logger.error(`BACKUP ALERT [${type}] ${service}: ${JSON.stringify(data)}`);\n  }\n\n  private getExpectedInterval(service: string): number {\n    const intervals = {\n      'user-service': 15 * 60 * 1000, // 15 minutes\n      'product-service': 60 * 60 * 1000, // 1 hour\n      'order-service': 5 * 60 * 1000, // 5 minutes\n      'payment-service': 5 * 60 * 1000, // 5 minutes\n    };\n\n    return intervals[service] || 60 * 60 * 1000; // Default 1 hour\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"recovery-procedures",children:"Recovery Procedures"}),"\n",(0,s.jsx)(n.h3,{id:"automated-recovery-playbooks",children:"Automated Recovery Playbooks"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# scripts/recovery-playbooks/order-service-recovery.sh\n\n# Order Service Recovery Playbook\n# This script handles recovery of the critical order service\n\nset -e\n\nORDER_SERVICE_RECOVERY_PLAYBOOK() {\n  local BACKUP_ID=$1\n  local TARGET_ENV=$2\n\n  echo "=== ORDER SERVICE RECOVERY PLAYBOOK ==="\n  echo "Backup ID: $BACKUP_ID"\n  echo "Target Environment: $TARGET_ENV"\n  echo "Started at: $(date)"\n\n  # Step 1: Validate backup\n  echo "Step 1: Validating backup..."\n  if ! validate_backup "$BACKUP_ID"; then\n    echo "ERROR: Backup validation failed"\n    exit 1\n  fi\n\n  # Step 2: Stop order service\n  echo "Step 2: Stopping order service..."\n  kubectl scale deployment order-service --replicas=0 -n "$TARGET_ENV"\n\n  # Step 3: Restore database\n  echo "Step 3: Restoring order database..."\n  restore_database "orders_db" "$BACKUP_ID"\n\n  # Step 4: Restore Redis cache\n  echo "Step 4: Restoring order cache..."\n  restore_redis_pattern "order:*" "$BACKUP_ID"\n\n  # Step 5: Start order service\n  echo "Step 5: Starting order service..."\n  kubectl scale deployment order-service --replicas=3 -n "$TARGET_ENV"\n\n  # Step 6: Wait for service to be ready\n  echo "Step 6: Waiting for service readiness..."\n  kubectl wait --for=condition=available deployment/order-service -n "$TARGET_ENV" --timeout=300s\n\n  # Step 7: Validate service health\n  echo "Step 7: Validating service health..."\n  if ! validate_service_health "order-service" "$TARGET_ENV"; then\n    echo "ERROR: Service health validation failed"\n    exit 1\n  fi\n\n  echo "=== ORDER SERVICE RECOVERY COMPLETED ==="\n  echo "Completed at: $(date)"\n}\n\nvalidate_backup() {\n  local backup_id=$1\n  # Implementation for backup validation\n  return 0\n}\n\nrestore_database() {\n  local database=$1\n  local backup_id=$2\n  # Implementation for database restoration\n  echo "Restoring database $database from backup $backup_id"\n}\n\nrestore_redis_pattern() {\n  local pattern=$1\n  local backup_id=$2\n  # Implementation for Redis restoration\n  echo "Restoring Redis pattern $pattern from backup $backup_id"\n}\n\nvalidate_service_health() {\n  local service=$1\n  local namespace=$2\n  # Implementation for service health validation\n  local health_check_url="http://$service.$namespace.svc.cluster.local:3000/health"\n  curl -f "$health_check_url" > /dev/null 2>&1\n}\n\n# Execute the playbook if called directly\nif [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then\n  ORDER_SERVICE_RECOVERY_PLAYBOOK "$1" "$2"\nfi\n'})}),"\n",(0,s.jsx)(n.h2,{id:"testing--validation",children:"Testing & Validation"}),"\n",(0,s.jsx)(n.h3,{id:"backup-testing-framework",children:"Backup Testing Framework"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// src/shared/testing/backup-testing.service.ts\nimport { Injectable, Logger } from '@nestjs/common';\nimport { Cron } from '@nestjs/schedule';\n\ninterface BackupTestResult {\n  testName: string;\n  success: boolean;\n  duration: number;\n  errors: string[];\n  metrics: Record<string, any>;\n}\n\n@Injectable()\nexport class BackupTestingService {\n  private readonly logger = new Logger(BackupTestingService.name);\n\n  // Run backup tests weekly\n  @Cron('0 6 * * 0')\n  async runWeeklyBackupTests(): Promise<void> {\n    const testResults: BackupTestResult[] = [];\n\n    testResults.push(await this.testDatabaseRestore());\n    testResults.push(await this.testRedisRestore());\n    testResults.push(await this.testFileRestore());\n    testResults.push(await this.testKubernetesRestore());\n    testResults.push(await this.testDisasterRecovery());\n\n    await this.generateTestReport(testResults);\n  }\n\n  private async testDatabaseRestore(): Promise<BackupTestResult> {\n    const startTime = Date.now();\n    const testName = 'database_restore_test';\n    const errors: string[] = [];\n\n    try {\n      // Create test database\n      const testDbName = `test_restore_${Date.now()}`;\n\n      // Get latest backup\n      const latestBackup = await this.getLatestBackup('orders_db');\n\n      // Restore to test database\n      await this.restoreToTestDatabase(latestBackup, testDbName);\n\n      // Validate data integrity\n      const integrityCheck = await this.validateDataIntegrity(testDbName);\n      if (!integrityCheck.valid) {\n        errors.push(...integrityCheck.errors);\n      }\n\n      // Cleanup test database\n      await this.cleanupTestDatabase(testDbName);\n\n      return {\n        testName,\n        success: errors.length === 0,\n        duration: Date.now() - startTime,\n        errors,\n        metrics: {\n          backupSize: latestBackup.size,\n          restoreTime: Date.now() - startTime,\n          recordCount: integrityCheck.recordCount,\n        },\n      };\n    } catch (error) {\n      return {\n        testName,\n        success: false,\n        duration: Date.now() - startTime,\n        errors: [error.message],\n        metrics: {},\n      };\n    }\n  }\n\n  private async testDisasterRecovery(): Promise<BackupTestResult> {\n    const startTime = Date.now();\n    const testName = 'disaster_recovery_test';\n    const errors: string[] = [];\n\n    try {\n      // Simulate a disaster scenario in test environment\n      await this.simulateDisaster('test-environment');\n\n      // Execute DR procedures\n      await this.executeDRProcedures('test-environment');\n\n      // Validate recovery\n      const recoveryValidation = await this.validateRecovery('test-environment');\n      if (!recoveryValidation.valid) {\n        errors.push(...recoveryValidation.errors);\n      }\n\n      return {\n        testName,\n        success: errors.length === 0,\n        duration: Date.now() - startTime,\n        errors,\n        metrics: {\n          rto: recoveryValidation.rto,\n          rpo: recoveryValidation.rpo,\n          servicesRecovered: recoveryValidation.servicesRecovered,\n        },\n      };\n    } catch (error) {\n      return {\n        testName,\n        success: false,\n        duration: Date.now() - startTime,\n        errors: [error.message],\n        metrics: {},\n      };\n    }\n  }\n\n  private async generateTestReport(results: BackupTestResult[]): Promise<void> {\n    const report = {\n      timestamp: new Date().toISOString(),\n      summary: {\n        totalTests: results.length,\n        passedTests: results.filter((r) => r.success).length,\n        failedTests: results.filter((r) => !r.success).length,\n        totalDuration: results.reduce((sum, r) => sum + r.duration, 0),\n      },\n      results,\n    };\n\n    // Save report and send notifications\n    await this.saveTestReport(report);\n    await this.notifyTestResults(report);\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"implementation-roadmap",children:"Implementation Roadmap"}),"\n",(0,s.jsx)(n.h3,{id:"phase-1-core-backup-infrastructure-weeks-1-2",children:"Phase 1: Core Backup Infrastructure (Weeks 1-2)"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Set up Google Cloud Storage buckets with lifecycle policies"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Configure Cloud SQL automated backups"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement database backup service for PostgreSQL"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Set up Redis backup automation"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Create backup monitoring and alerting"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"phase-2-application-level-backups-weeks-3-4",children:"Phase 2: Application-Level Backups (Weeks 3-4)"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement file storage backup for product images"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Set up static asset backup automation"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Create Kubernetes configuration backup scripts"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement secrets and ConfigMap backup"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Set up cross-region replication for critical data"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"phase-3-disaster-recovery-weeks-5-6",children:"Phase 3: Disaster Recovery (Weeks 5-6)"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Deploy DR infrastructure in secondary region"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement DR orchestration service"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Create automated failover procedures"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Set up DNS failover configuration"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Test regional disaster recovery scenarios"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"phase-4-testing--validation-weeks-7-8",children:"Phase 4: Testing & Validation (Weeks 7-8)"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement backup testing framework"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Create automated restore validation"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Set up weekly backup integrity tests"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Create disaster recovery testing procedures"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Establish backup compliance reporting"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"phase-5-optimization--enhancement-weeks-9-10",children:"Phase 5: Optimization & Enhancement (Weeks 9-10)"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Optimize backup storage costs"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement intelligent backup scheduling"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Add backup deduplication"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Create backup analytics dashboard"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Fine-tune recovery procedures based on testing"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"cost-optimization",children:"Cost Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"backup-storage-lifecycle",children:"Backup Storage Lifecycle"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-hcl",children:'# Storage lifecycle for cost optimization\nresource "google_storage_bucket" "backup_storage" {\n  name     = "ecommerce-backups-optimized"\n  location = "US"\n\n  lifecycle_rule {\n    condition {\n      age = 7\n    }\n    action {\n      type          = "SetStorageClass"\n      storage_class = "NEARLINE"  # 30% cost reduction\n    }\n  }\n\n  lifecycle_rule {\n    condition {\n      age = 30\n    }\n    action {\n      type          = "SetStorageClass"\n      storage_class = "COLDLINE"  # 50% cost reduction\n    }\n  }\n\n  lifecycle_rule {\n    condition {\n      age = 365\n    }\n    action {\n      type          = "SetStorageClass"\n      storage_class = "ARCHIVE"   # 68% cost reduction\n    }\n  }\n\n  lifecycle_rule {\n    condition {\n      age = 2555  # 7 years\n    }\n    action {\n      type = "Delete"\n    }\n  }\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"compliance--security",children:"Compliance & Security"}),"\n",(0,s.jsx)(n.h3,{id:"data-protection-compliance",children:"Data Protection Compliance"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"PCI DSS"}),": Payment data encrypted at rest and in transit"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GDPR"}),": Customer data retention and deletion policies"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"SOX"}),": Financial data backup retention (7 years)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Encryption"}),": AES-256 encryption for all sensitive backups"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Access Control"}),": RBAC for backup access and restoration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audit Logging"}),": Complete audit trail for all backup operations"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"security-measures",children:"Security Measures"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# Security policies for backup access\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: backup-service-account\n  namespace: ecommerce-prod\n  annotations:\n    iam.gke.io/gcp-service-account: ecommerce-backup-sa@project.iam.gserviceaccount.com\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: ecommerce-prod\n  name: backup-role\nrules:\n  - apiGroups: ['']\n    resources: ['secrets', 'configmaps']\n    verbs: ['get', 'list']\n  - apiGroups: ['apps']\n    resources: ['deployments']\n    verbs: ['get', 'list', 'patch']\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: backup-role-binding\n  namespace: ecommerce-prod\nsubjects:\n  - kind: ServiceAccount\n    name: backup-service-account\n    namespace: ecommerce-prod\nroleRef:\n  kind: Role\n  name: backup-role\n  apiGroup: rbac.authorization.k8s.io\n"})}),"\n",(0,s.jsx)(n.h2,{id:"related-documentation",children:"Related Documentation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"/fullstack-dev/docs/devops/devsecops-toolset",children:"DevSecOps Toolset"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"/fullstack-dev/docs/architecture-practices/architecture-execution/infrastructure-monitoring",children:"Infrastructure Monitoring Guide"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"/fullstack-dev/docs/architecture-practices/architecture-execution/database-migrations",children:"Database Migration Management"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"/fullstack-dev/docs/devops/gcp/microservices-terraform-guide",children:"GCP Microservices Terraform Guide"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"/fullstack-dev/docs/architecture-practices/architecture-execution/security-scanning",children:"Security Scanning Implementation"})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This comprehensive backup and restore strategy ensures your ecommerce platform can recover quickly from any disaster while maintaining data integrity and compliance with industry standards."})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},5741:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>c});var a=t(9729);const s={},r=a.createContext(s);function i(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);