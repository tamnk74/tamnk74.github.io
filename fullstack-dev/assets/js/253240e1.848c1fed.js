"use strict";(self.webpackChunkdocs_site=self.webpackChunkdocs_site||[]).push([[2913],{3081:e=>{e.exports=JSON.parse('{"permalink":"/fullstack-dev/blog/rag-ecommerce-recommendations-part4-generation-personalization","editUrl":"https://github.com/tamnk74/fullstack-dev/tree/main/docs-site/blog/2025-10-08-rag-ecommerce-recommendations-part4-generation-personalization.md","source":"@site/blog/2025-10-08-rag-ecommerce-recommendations-part4-generation-personalization.md","title":"Building Intelligent E-commerce Recommendations with RAG: Part 4 - Generation and Personalization Engine","description":"Welcome to Part 4 of our RAG-powered e-commerce recommendation series! We\'ve built our data pipeline, generated embeddings, and implemented sophisticated retrieval systems. Now we\'ll create the crown jewel: an intelligent generation and personalization engine that transforms raw product matches into compelling, personalized recommendations that drive engagement and conversions.","date":"2025-10-08T00:00:00.000Z","tags":[{"inline":false,"label":"RAG","permalink":"/fullstack-dev/blog/tags/rag","description":"Retrieval-Augmented Generation"},{"inline":false,"label":"E-commerce","permalink":"/fullstack-dev/blog/tags/ecommerce","description":"E-commerce development and strategies"},{"inline":false,"label":"Recommendations","permalink":"/fullstack-dev/blog/tags/recommendations","description":"Recommendation systems and algorithms"},{"inline":false,"label":"Large Language Models","permalink":"/fullstack-dev/blog/tags/llm","description":"Large language models and applications"},{"inline":false,"label":"Personalization","permalink":"/fullstack-dev/blog/tags/personalization","description":"User personalization and customization"},{"inline":false,"label":"OpenAI","permalink":"/fullstack-dev/blog/tags/openai","description":"OpenAI APIs and models"},{"inline":false,"label":"Anthropic","permalink":"/fullstack-dev/blog/tags/anthropic","description":"Anthropic AI models and Claude"},{"inline":false,"label":"Prompt Engineering","permalink":"/fullstack-dev/blog/tags/prompt-engineering","description":"Prompt design and optimization"},{"inline":false,"label":"Generation","permalink":"/fullstack-dev/blog/tags/generation","description":"Content and text generation"}],"readingTime":20.19,"hasTruncateMarker":true,"authors":[{"name":"Tam Nguyen","title":"Full Stack Developer, Next.js for Production Creator","url":"https://github.com/tamnk74","page":{"permalink":"/fullstack-dev/blog/authors/tam"},"socials":{"github":"https://github.com/tamnk74"},"imageURL":"https://github.com/tamnk74.png","key":"tam"}],"frontMatter":{"slug":"rag-ecommerce-recommendations-part4-generation-personalization","title":"Building Intelligent E-commerce Recommendations with RAG: Part 4 - Generation and Personalization Engine","authors":["tam"],"tags":["rag","ecommerce","recommendations","llm","personalization","openai","anthropic","prompt-engineering","generation"],"date":"2025-10-08T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Building Intelligent E-commerce Recommendations with RAG: Part 3 - Retrieval System Implementation","permalink":"/fullstack-dev/blog/rag-ecommerce-recommendations-part3-retrieval-system"},"nextItem":{"title":"Building Smart Location-Aware Applications: Complete Geo-targeting Implementation with Node.js and React","permalink":"/fullstack-dev/blog/geo-targeting-implementation-nodejs-react"}}')},5800:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>d,frontMatter:()=>i,metadata:()=>r,toc:()=>l});var r=t(3081),o=t(5813),a=t(7814);const i={slug:"rag-ecommerce-recommendations-part4-generation-personalization",title:"Building Intelligent E-commerce Recommendations with RAG: Part 4 - Generation and Personalization Engine",authors:["tam"],tags:["rag","ecommerce","recommendations","llm","personalization","openai","anthropic","prompt-engineering","generation"],date:new Date("2025-10-08T00:00:00.000Z")},s="Building Intelligent E-commerce Recommendations with RAG: Part 4 - Generation and Personalization Engine",c={authorsImageUrls:[void 0]},l=[{value:"Generation Engine Architecture",id:"generation-engine-architecture",level:2},{value:"LLM Integration Layer",id:"llm-integration-layer",level:2},{value:"Prompt Engineering Framework",id:"prompt-engineering-framework",level:2},{value:"Personalization Engine",id:"personalization-engine",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"<strong>What We&#39;ve Accomplished</strong>",id:"what-weve-accomplished",level:3},{value:"<strong>Before Part 5</strong>",id:"before-part-5",level:3}];function p(e){const n={code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.p,{children:"Welcome to Part 4 of our RAG-powered e-commerce recommendation series! We've built our data pipeline, generated embeddings, and implemented sophisticated retrieval systems. Now we'll create the crown jewel: an intelligent generation and personalization engine that transforms raw product matches into compelling, personalized recommendations that drive engagement and conversions."}),"\n",(0,o.jsx)(n.p,{children:"This generation engine leverages large language models to understand context, craft personalized explanations, and create recommendations that feel natural and relevant to each individual user. We'll explore advanced prompt engineering, personalization algorithms, and A/B testing frameworks."}),"\n",(0,o.jsx)(n.h2,{id:"generation-engine-architecture",children:"Generation Engine Architecture"}),"\n",(0,o.jsx)(n.p,{children:"Our generation engine combines retrieval results with user context to create personalized, explainable recommendations:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          Input Processing                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Retrieved   \u2502  \u2502    User     \u2502  \u2502   Context   \u2502  \u2502   Intent    \u2502 \u2502\n\u2502  \u2502  Products   \u2502  \u2502   Profile   \u2502  \u2502    Data     \u2502  \u2502 Detection   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       Prompt Engineering                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Template  \u2502  \u2502 Personality \u2502  \u2502   Context   \u2502  \u2502   Output    \u2502 \u2502\n\u2502  \u2502  Selection  \u2502  \u2502   Matching  \u2502  \u2502 Injection   \u2502  \u2502 Formatting  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        LLM Generation                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   OpenAI    \u2502  \u2502  Anthropic  \u2502  \u2502    Local    \u2502  \u2502   Custom    \u2502 \u2502\n\u2502  \u2502   Models    \u2502  \u2502   Claude    \u2502  \u2502   Models    \u2502  \u2502   Models    \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Post-Processing                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Response    \u2502  \u2502 Validation  \u2502  \u2502 Formatting  \u2502  \u2502  Quality    \u2502 \u2502\n\u2502  \u2502 Parsing     \u2502  \u2502   Logic     \u2502  \u2502   Polish    \u2502  \u2502 Assurance   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(n.h2,{id:"llm-integration-layer",children:"LLM Integration Layer"}),"\n",(0,o.jsx)(n.p,{children:"Let's start by implementing a flexible LLM integration layer that supports multiple providers:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# generation/llm_client.py\nimport asyncio\nimport openai\nimport anthropic\nfrom typing import List, Dict, Any, Optional, Union\nimport json\nimport logging\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport time\n\nlogger = logging.getLogger(__name__)\n\nclass ModelProvider(Enum):\n    OPENAI = "openai"\n    ANTHROPIC = "anthropic"\n    LOCAL = "local"\n    AZURE_OPENAI = "azure_openai"\n\n@dataclass\nclass GenerationConfig:\n    temperature: float = 0.7\n    max_tokens: int = 1000\n    top_p: float = 0.9\n    frequency_penalty: float = 0.0\n    presence_penalty: float = 0.0\n    stop_sequences: Optional[List[str]] = None\n\n@dataclass\nclass GenerationRequest:\n    prompt: str\n    config: GenerationConfig\n    model: str\n    system_prompt: Optional[str] = None\n    context: Optional[Dict[str, Any]] = None\n\n@dataclass\nclass GenerationResponse:\n    content: str\n    model: str\n    provider: ModelProvider\n    token_usage: Dict[str, int]\n    latency: float\n    cost: float\n\nclass BaseLLMClient(ABC):\n    """Abstract base class for LLM clients"""\n    \n    @abstractmethod\n    async def generate(self, request: GenerationRequest) -> GenerationResponse:\n        pass\n    \n    @abstractmethod\n    def calculate_cost(self, token_usage: Dict[str, int], model: str) -> float:\n        pass\n\nclass OpenAIClient(BaseLLMClient):\n    """OpenAI API client implementation"""\n    \n    def __init__(self, api_key: str, organization: Optional[str] = None):\n        self.client = openai.AsyncOpenAI(\n            api_key=api_key,\n            organization=organization\n        )\n        self.pricing = {\n            "gpt-4": {"input": 0.03, "output": 0.06},\n            "gpt-4-turbo": {"input": 0.01, "output": 0.03},\n            "gpt-3.5-turbo": {"input": 0.001, "output": 0.002},\n            "gpt-3.5-turbo-16k": {"input": 0.003, "output": 0.004}\n        }\n    \n    async def generate(self, request: GenerationRequest) -> GenerationResponse:\n        """Generate response using OpenAI API"""\n        start_time = time.time()\n        \n        try:\n            messages = []\n            \n            # Add system prompt if provided\n            if request.system_prompt:\n                messages.append({\n                    "role": "system",\n                    "content": request.system_prompt\n                })\n            \n            # Add main prompt\n            messages.append({\n                "role": "user",\n                "content": request.prompt\n            })\n            \n            # Make API call\n            response = await self.client.chat.completions.create(\n                model=request.model,\n                messages=messages,\n                temperature=request.config.temperature,\n                max_tokens=request.config.max_tokens,\n                top_p=request.config.top_p,\n                frequency_penalty=request.config.frequency_penalty,\n                presence_penalty=request.config.presence_penalty,\n                stop=request.config.stop_sequences\n            )\n            \n            latency = time.time() - start_time\n            token_usage = {\n                "prompt_tokens": response.usage.prompt_tokens,\n                "completion_tokens": response.usage.completion_tokens,\n                "total_tokens": response.usage.total_tokens\n            }\n            \n            cost = self.calculate_cost(token_usage, request.model)\n            \n            return GenerationResponse(\n                content=response.choices[0].message.content,\n                model=request.model,\n                provider=ModelProvider.OPENAI,\n                token_usage=token_usage,\n                latency=latency,\n                cost=cost\n            )\n            \n        except Exception as e:\n            logger.error(f"OpenAI generation failed: {e}")\n            raise\n    \n    def calculate_cost(self, token_usage: Dict[str, int], model: str) -> float:\n        """Calculate cost based on token usage"""\n        pricing = self.pricing.get(model, self.pricing["gpt-3.5-turbo"])\n        \n        input_cost = token_usage["prompt_tokens"] * pricing["input"] / 1000\n        output_cost = token_usage["completion_tokens"] * pricing["output"] / 1000\n        \n        return input_cost + output_cost\n\nclass AnthropicClient(BaseLLMClient):\n    """Anthropic Claude API client implementation"""\n    \n    def __init__(self, api_key: str):\n        self.client = anthropic.AsyncAnthropic(api_key=api_key)\n        self.pricing = {\n            "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},\n            "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},\n            "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125}\n        }\n    \n    async def generate(self, request: GenerationRequest) -> GenerationResponse:\n        """Generate response using Anthropic API"""\n        start_time = time.time()\n        \n        try:\n            # Prepare prompt for Claude\n            full_prompt = request.prompt\n            if request.system_prompt:\n                full_prompt = f"{request.system_prompt}\\n\\n{request.prompt}"\n            \n            # Make API call\n            response = await self.client.messages.create(\n                model=request.model,\n                max_tokens=request.config.max_tokens,\n                temperature=request.config.temperature,\n                top_p=request.config.top_p,\n                stop_sequences=request.config.stop_sequences or [],\n                messages=[\n                    {"role": "user", "content": full_prompt}\n                ]\n            )\n            \n            latency = time.time() - start_time\n            token_usage = {\n                "prompt_tokens": response.usage.input_tokens,\n                "completion_tokens": response.usage.output_tokens,\n                "total_tokens": response.usage.input_tokens + response.usage.output_tokens\n            }\n            \n            cost = self.calculate_cost(token_usage, request.model)\n            \n            return GenerationResponse(\n                content=response.content[0].text,\n                model=request.model,\n                provider=ModelProvider.ANTHROPIC,\n                token_usage=token_usage,\n                latency=latency,\n                cost=cost\n            )\n            \n        except Exception as e:\n            logger.error(f"Anthropic generation failed: {e}")\n            raise\n    \n    def calculate_cost(self, token_usage: Dict[str, int], model: str) -> float:\n        """Calculate cost based on token usage"""\n        pricing = self.pricing.get(model, self.pricing["claude-3-haiku-20240307"])\n        \n        input_cost = token_usage["prompt_tokens"] * pricing["input"] / 1000\n        output_cost = token_usage["completion_tokens"] * pricing["output"] / 1000\n        \n        return input_cost + output_cost\n\nclass LLMClientManager:\n    """Manages multiple LLM clients with fallback and load balancing"""\n    \n    def __init__(self):\n        self.clients: Dict[ModelProvider, BaseLLMClient] = {}\n        self.model_mapping: Dict[str, ModelProvider] = {}\n        self.fallback_order = [ModelProvider.OPENAI, ModelProvider.ANTHROPIC]\n    \n    def register_client(self, provider: ModelProvider, client: BaseLLMClient):\n        """Register an LLM client"""\n        self.clients[provider] = client\n        logger.info(f"Registered {provider.value} client")\n    \n    def register_model(self, model_name: str, provider: ModelProvider):\n        """Register a model with its provider"""\n        self.model_mapping[model_name] = provider\n    \n    async def generate(self, request: GenerationRequest) -> GenerationResponse:\n        """Generate response with fallback support"""\n        provider = self.model_mapping.get(request.model)\n        \n        if not provider:\n            raise ValueError(f"Unknown model: {request.model}")\n        \n        # Try primary provider\n        if provider in self.clients:\n            try:\n                return await self.clients[provider].generate(request)\n            except Exception as e:\n                logger.warning(f"Primary provider {provider.value} failed: {e}")\n        \n        # Try fallback providers\n        for fallback_provider in self.fallback_order:\n            if fallback_provider != provider and fallback_provider in self.clients:\n                try:\n                    logger.info(f"Trying fallback provider: {fallback_provider.value}")\n                    fallback_request = self._adapt_request_for_provider(request, fallback_provider)\n                    return await self.clients[fallback_provider].generate(fallback_request)\n                except Exception as e:\n                    logger.warning(f"Fallback provider {fallback_provider.value} failed: {e}")\n                    continue\n        \n        raise Exception("All LLM providers failed")\n    \n    def _adapt_request_for_provider(self, request: GenerationRequest, \n                                   provider: ModelProvider) -> GenerationRequest:\n        """Adapt request for different provider"""\n        # Map models between providers\n        model_mappings = {\n            ModelProvider.OPENAI: {\n                "claude-3-opus-20240229": "gpt-4",\n                "claude-3-sonnet-20240229": "gpt-3.5-turbo-16k",\n                "claude-3-haiku-20240307": "gpt-3.5-turbo"\n            },\n            ModelProvider.ANTHROPIC: {\n                "gpt-4": "claude-3-opus-20240229",\n                "gpt-3.5-turbo": "claude-3-haiku-20240307",\n                "gpt-3.5-turbo-16k": "claude-3-sonnet-20240229"\n            }\n        }\n        \n        adapted_model = model_mappings.get(provider, {}).get(request.model, request.model)\n        \n        return GenerationRequest(\n            prompt=request.prompt,\n            config=request.config,\n            model=adapted_model,\n            system_prompt=request.system_prompt,\n            context=request.context\n        )\n'})}),"\n",(0,o.jsx)(n.h2,{id:"prompt-engineering-framework",children:"Prompt Engineering Framework"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# generation/prompt_engineer.py\nfrom typing import Dict, Any, List, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport json\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass RecommendationType(Enum):\n    PRODUCT_DISCOVERY = "product_discovery"\n    SIMILAR_PRODUCTS = "similar_products"\n    PERSONALIZED_SUGGESTIONS = "personalized_suggestions"\n    TRENDING_ITEMS = "trending_items"\n    BUNDLE_RECOMMENDATIONS = "bundle_recommendations"\n    REORDER_SUGGESTIONS = "reorder_suggestions"\n\nclass PersonalityType(Enum):\n    PROFESSIONAL = "professional"\n    FRIENDLY = "friendly"\n    ENTHUSIASTIC = "enthusiastic"\n    MINIMALIST = "minimalist"\n    EXPERT = "expert"\n\n@dataclass\nclass PromptTemplate:\n    template: str\n    required_variables: List[str]\n    optional_variables: List[str]\n    output_format: str\n    personality: PersonalityType\n\nclass PromptEngineer:\n    """Advanced prompt engineering for e-commerce recommendations"""\n    \n    def __init__(self):\n        self.templates = self._initialize_templates()\n        self.personality_traits = self._initialize_personalities()\n    \n    def create_recommendation_prompt(self, \n                                   recommendation_type: RecommendationType,\n                                   user_context: Dict[str, Any],\n                                   products: List[Dict[str, Any]],\n                                   personality: PersonalityType = PersonalityType.FRIENDLY,\n                                   additional_context: Optional[Dict[str, Any]] = None) -> str:\n        """Create a comprehensive recommendation prompt"""\n        \n        template = self.templates[recommendation_type][personality]\n        personality_traits = self.personality_traits[personality]\n        \n        # Prepare product information\n        product_info = self._format_products(products)\n        \n        # Prepare user context\n        user_info = self._format_user_context(user_context)\n        \n        # Prepare additional context\n        context_info = self._format_additional_context(additional_context or {})\n        \n        # Build the prompt\n        prompt = template.template.format(\n            personality_traits=personality_traits,\n            user_context=user_info,\n            products=product_info,\n            additional_context=context_info,\n            output_format=template.output_format\n        )\n        \n        return prompt\n    \n    def create_explanation_prompt(self,\n                                 recommended_product: Dict[str, Any],\n                                 user_context: Dict[str, Any],\n                                 reasoning_factors: List[str],\n                                 personality: PersonalityType = PersonalityType.FRIENDLY) -> str:\n        """Create a prompt for generating explanations"""\n        \n        personality_traits = self.personality_traits[personality]\n        \n        prompt = f"""\n{personality_traits}\n\nI need you to explain why this product is recommended for this specific user.\n\nUSER PROFILE:\n{self._format_user_context(user_context)}\n\nRECOMMENDED PRODUCT:\n{self._format_single_product(recommended_product)}\n\nREASONING FACTORS:\n{self._format_reasoning_factors(reasoning_factors)}\n\nPlease provide a personalized explanation that:\n1. Highlights why this product matches their needs\n2. Mentions specific benefits relevant to their profile\n3. Addresses any potential concerns\n4. Keeps the tone {personality.value}\n\nFormat your response as a natural, conversational explanation (2-3 sentences).\n"""\n        \n        return prompt\n    \n    def create_comparison_prompt(self,\n                               products: List[Dict[str, Any]],\n                               comparison_criteria: List[str],\n                               user_context: Dict[str, Any],\n                               personality: PersonalityType = PersonalityType.EXPERT) -> str:\n        """Create a prompt for product comparisons"""\n        \n        personality_traits = self.personality_traits[personality]\n        \n        prompt = f"""\n{personality_traits}\n\nI need you to compare these products for a specific user.\n\nUSER PROFILE:\n{self._format_user_context(user_context)}\n\nPRODUCTS TO COMPARE:\n{self._format_products_for_comparison(products)}\n\nCOMPARISON CRITERIA:\n{\', \'.join(comparison_criteria)}\n\nPlease provide a detailed comparison that:\n1. Compares each product across the specified criteria\n2. Highlights strengths and weaknesses of each\n3. Provides a recommendation based on the user\'s profile\n4. Uses a {personality.value} tone\n\nFormat your response as structured comparison with clear recommendations.\n"""\n        \n        return prompt\n    \n    def _initialize_templates(self) -> Dict[RecommendationType, Dict[PersonalityType, PromptTemplate]]:\n        """Initialize prompt templates for different recommendation types and personalities"""\n        templates = {}\n        \n        # Product Discovery Templates\n        templates[RecommendationType.PRODUCT_DISCOVERY] = {\n            PersonalityType.FRIENDLY: PromptTemplate(\n                template="""\n{personality_traits}\n\nI\'m helping a customer discover new products they might love. Here\'s what I know about them:\n\nUSER PROFILE:\n{user_context}\n\nAVAILABLE PRODUCTS:\n{products}\n\nADDITIONAL CONTEXT:\n{additional_context}\n\nPlease recommend 3-5 products from the list that would be perfect for this user. For each recommendation:\n\n1. **Product Name & Brief Description**\n2. **Why it\'s perfect for them** (personalized reasoning)\n3. **Key benefits** they\'ll appreciate\n4. **Price & value consideration**\n\n{output_format}\n""",\n                required_variables=["user_context", "products"],\n                optional_variables=["additional_context"],\n                output_format="JSON format with product recommendations",\n                personality=PersonalityType.FRIENDLY\n            ),\n            \n            PersonalityType.PROFESSIONAL: PromptTemplate(\n                template="""\n{personality_traits}\n\nBased on the user profile and available product catalog, I need to provide data-driven product recommendations.\n\nUSER ANALYSIS:\n{user_context}\n\nPRODUCT CATALOG:\n{products}\n\nCONTEXTUAL FACTORS:\n{additional_context}\n\nPlease provide analytical product recommendations with:\n\n1. **Product Selection Rationale**\n2. **Compatibility Assessment**\n3. **Value Proposition Analysis**\n4. **Risk/Benefit Evaluation**\n\n{output_format}\n""",\n                required_variables=["user_context", "products"],\n                optional_variables=["additional_context"],\n                output_format="Structured analytical format",\n                personality=PersonalityType.PROFESSIONAL\n            )\n        }\n        \n        # Similar Products Templates\n        templates[RecommendationType.SIMILAR_PRODUCTS] = {\n            PersonalityType.FRIENDLY: PromptTemplate(\n                template="""\n{personality_traits}\n\nA customer is interested in similar products to what they\'re currently viewing. Help me suggest alternatives!\n\nUSER PROFILE:\n{user_context}\n\nSIMILAR PRODUCTS:\n{products}\n\nContext: {additional_context}\n\nPlease recommend the best alternatives, explaining:\n1. **How each product is similar** to their interest\n2. **What makes each unique** or better\n3. **Price comparison** and value\n4. **Why they might prefer** each option\n\n{output_format}\n""",\n                required_variables=["user_context", "products"],\n                optional_variables=["additional_context"],\n                output_format="Friendly recommendation list",\n                personality=PersonalityType.FRIENDLY\n            )\n        }\n        \n        # Add more templates for other recommendation types...\n        \n        return templates\n    \n    def _initialize_personalities(self) -> Dict[PersonalityType, str]:\n        """Initialize personality traits for different tones"""\n        return {\n            PersonalityType.FRIENDLY: """\nYou are a helpful, warm, and enthusiastic shopping assistant. You love helping people find products they\'ll genuinely enjoy. You speak conversationally, use occasional emojis, and always consider the human element in your recommendations. You\'re knowledgeable but not overwhelming, and you always keep the customer\'s best interests at heart.\n""",\n            \n            PersonalityType.PROFESSIONAL: """\nYou are a professional product consultant with deep expertise in e-commerce and consumer behavior. You provide data-driven, analytical recommendations backed by clear reasoning. Your tone is authoritative but approachable, and you focus on providing maximum value through informed decision-making.\n""",\n            \n            PersonalityType.ENTHUSIASTIC: """\nYou are an excited product expert who absolutely loves discovering amazing finds for customers! You\'re genuinely thrilled about every recommendation and can\'t wait to share why each product is fantastic. Your enthusiasm is contagious, and you help customers get excited about their potential purchases.\n""",\n            \n            PersonalityType.MINIMALIST: """\nYou are a concise, no-nonsense product advisor. You provide clear, direct recommendations without unnecessary fluff. Every word serves a purpose, and you focus on the most important information customers need to make decisions quickly and confidently.\n""",\n            \n            PersonalityType.EXPERT: """\nYou are a seasoned industry expert with deep knowledge across multiple product categories. You provide detailed, technical insights while remaining accessible. Your recommendations are backed by expertise, and you help customers understand the nuances that matter most.\n"""\n        }\n    \n    def _format_products(self, products: List[Dict[str, Any]]) -> str:\n        """Format products for prompt inclusion"""\n        formatted_products = []\n        \n        for i, product in enumerate(products, 1):\n            payload = product.get(\'payload\', product)\n            \n            product_text = f"""\nProduct {i}:\n- ID: {payload.get(\'id\', \'N/A\')}\n- Title: {payload.get(\'title\', \'N/A\')}\n- Category: {payload.get(\'category\', \'N/A\')}\n- Brand: {payload.get(\'brand\', \'N/A\')}\n- Price: ${payload.get(\'price\', \'N/A\')}\n- Rating: {payload.get(\'rating\', \'N/A\')}/5 ({payload.get(\'review_count\', 0)} reviews)\n- Description: {payload.get(\'description\', \'N/A\')[:200]}...\n"""\n            if product.get(\'score\'):\n                product_text += f"- Relevance Score: {product[\'score\']:.3f}\\n"\n            \n            formatted_products.append(product_text)\n        \n        return "\\n".join(formatted_products)\n    \n    def _format_user_context(self, user_context: Dict[str, Any]) -> str:\n        """Format user context for prompt inclusion"""\n        context_parts = []\n        \n        # Demographics\n        demographics = user_context.get(\'demographics\', {})\n        if demographics:\n            context_parts.append(f"Demographics: {demographics}")\n        \n        # Preferences\n        preferences = user_context.get(\'preferences\', {})\n        if preferences:\n            pref_text = []\n            if preferences.get(\'preferred_categories\'):\n                pref_text.append(f"Preferred categories: {\', \'.join(preferences[\'preferred_categories\'])}")\n            if preferences.get(\'preferred_brands\'):\n                pref_text.append(f"Preferred brands: {\', \'.join(preferences[\'preferred_brands\'])}")\n            if preferences.get(\'price_sensitivity\'):\n                price_level = "budget-conscious" if preferences[\'price_sensitivity\'] < 0.3 else \\\n                             "premium-oriented" if preferences[\'price_sensitivity\'] > 0.7 else "balanced"\n                pref_text.append(f"Price preference: {price_level}")\n            \n            if pref_text:\n                context_parts.append("Preferences: " + "; ".join(pref_text))\n        \n        # Behavioral patterns\n        behavioral = user_context.get(\'behavioral_features\', {})\n        if behavioral:\n            behavior_text = []\n            purchase_freq = behavioral.get(\'purchase_frequency\', 0)\n            if purchase_freq > 10:\n                behavior_text.append("frequent buyer")\n            elif purchase_freq > 5:\n                behavior_text.append("regular customer")\n            else:\n                behavior_text.append("occasional shopper")\n            \n            if behavioral.get(\'avg_session_duration\', 0) > 300:\n                behavior_text.append("detailed researcher")\n            \n            if behavior_text:\n                context_parts.append("Shopping behavior: " + ", ".join(behavior_text))\n        \n        # Purchase history summary\n        if user_context.get(\'recent_purchases\'):\n            recent = user_context[\'recent_purchases\'][:3]  # Last 3 purchases\n            context_parts.append(f"Recent purchases: {\', \'.join([p.get(\'title\', \'Unknown\') for p in recent])}")\n        \n        return "\\n".join(context_parts) if context_parts else "New customer with limited profile data"\n    \n    def _format_additional_context(self, context: Dict[str, Any]) -> str:\n        """Format additional context information"""\n        context_parts = []\n        \n        if context.get(\'season\'):\n            context_parts.append(f"Season: {context[\'season\']}")\n        \n        if context.get(\'current_trends\'):\n            context_parts.append(f"Current trends: {\', \'.join(context[\'current_trends\'])}")\n        \n        if context.get(\'inventory_status\'):\n            context_parts.append(f"Inventory considerations: {context[\'inventory_status\']}")\n        \n        if context.get(\'promotion_info\'):\n            context_parts.append(f"Active promotions: {context[\'promotion_info\']}")\n        \n        return "\\n".join(context_parts) if context_parts else "No additional context"\n    \n    def _format_single_product(self, product: Dict[str, Any]) -> str:\n        """Format a single product for detailed analysis"""\n        payload = product.get(\'payload\', product)\n        \n        return f"""\nTitle: {payload.get(\'title\', \'N/A\')}\nCategory: {payload.get(\'category\', \'N/A\')}\nBrand: {payload.get(\'brand\', \'N/A\')}\nPrice: ${payload.get(\'price\', \'N/A\')}\nRating: {payload.get(\'rating\', \'N/A\')}/5 ({payload.get(\'review_count\', 0)} reviews)\nDescription: {payload.get(\'description\', \'N/A\')}\nKey Features: {\', \'.join(payload.get(\'features\', []))}\n"""\n    \n    def _format_reasoning_factors(self, factors: List[str]) -> str:\n        """Format reasoning factors for explanation"""\n        return "\\n".join([f"- {factor}" for factor in factors])\n    \n    def _format_products_for_comparison(self, products: List[Dict[str, Any]]) -> str:\n        """Format products specifically for comparison"""\n        formatted = []\n        \n        for i, product in enumerate(products, 1):\n            payload = product.get(\'payload\', product)\n            formatted.append(f"""\nOption {i}: {payload.get(\'title\', \'N/A\')}\n- Brand: {payload.get(\'brand\', \'N/A\')}\n- Price: ${payload.get(\'price\', \'N/A\')}\n- Rating: {payload.get(\'rating\', \'N/A\')}/5\n- Key Features: {\', \'.join(payload.get(\'features\', [])[:3])}\n""")\n        \n        return "\\n".join(formatted)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"personalization-engine",children:"Personalization Engine"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# generation/personalization_engine.py\nimport numpy as np\nfrom typing import Dict, Any, List, Optional, Tuple\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport logging\nfrom datetime import datetime, timedelta\nimport json\n\nlogger = logging.getLogger(__name__)\n\nclass PersonalizationStrategy(Enum):\n    COLLABORATIVE_FILTERING = \"collaborative_filtering\"\n    CONTENT_BASED = \"content_based\"\n    HYBRID = \"hybrid\"\n    CONTEXTUAL = \"contextual\"\n    DEEP_LEARNING = \"deep_learning\"\n\n@dataclass\nclass PersonalizationConfig:\n    strategy: PersonalizationStrategy\n    weights: Dict[str, float]\n    min_confidence: float = 0.5\n    diversity_factor: float = 0.3\n    novelty_factor: float = 0.2\n    serendipity_factor: float = 0.1\n\n@dataclass\nclass PersonalizedRecommendation:\n    product_id: str\n    score: float\n    reasoning: List[str]\n    confidence: float\n    personalization_factors: Dict[str, float]\n    explanation: str\n    metadata: Dict[str, Any]\n\nclass PersonalizationEngine:\n    \"\"\"Advanced personalization engine for recommendation generation\"\"\"\n    \n    def __init__(self, config: PersonalizationConfig):\n        self.config = config\n        self.user_profiles = {}\n        self.item_profiles = {}\n        self.interaction_matrix = {}\n        \n    async def personalize_recommendations(self,\n                                        user_id: str,\n                                        candidate_products: List[Dict[str, Any]],\n                                        user_context: Dict[str, Any],\n                                        additional_context: Optional[Dict[str, Any]] = None) -> List[PersonalizedRecommendation]:\n        \"\"\"Generate personalized recommendations for a specific user\"\"\"\n        \n        # Get user profile\n        user_profile = await self._get_user_profile(user_id, user_context)\n        \n        # Calculate personalization scores for each candidate\n        personalized_products = []\n        \n        for product in candidate_products:\n            personalization_score = await self._calculate_personalization_score(\n                user_profile, product, additional_context or {}\n            )\n            \n            if personalization_score.confidence >= self.config.min_confidence:\n                personalized_products.append(personalization_score)\n        \n        # Apply diversity and novelty filters\n        final_recommendations = self._apply_diversification(\n            personalized_products, user_profile\n        )\n        \n        # Sort by personalized score\n        final_recommendations.sort(key=lambda x: x.score, reverse=True)\n        \n        return final_recommendations\n    \n    async def _get_user_profile(self, user_id: str, context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Get or create comprehensive user profile\"\"\"\n        \n        if user_id in self.user_profiles:\n            profile = self.user_profiles[user_id]\n            # Update with latest context\n            profile.update(context)\n        else:\n            # Create new profile\n            profile = {\n                'user_id': user_id,\n                'preferences': context.get('preferences', {}),\n                'behavioral_features': context.get('behavioral_features', {}),\n                'demographic_features': context.get('demographics', {}),\n                'interaction_history': context.get('interaction_history', []),\n                'purchase_history': context.get('purchase_history', []),\n                'created_at': datetime.utcnow(),\n                'last_updated': datetime.utcnow()\n            }\n            self.user_profiles[user_id] = profile\n        \n        # Enrich profile with derived features\n        profile = await self._enrich_user_profile(profile)\n        \n        return profile\n    \n    async def _enrich_user_profile(self, profile: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Enrich user profile with derived features\"\"\"\n        \n        # Calculate user preferences vector\n        profile['preference_vector'] = self._calculate_preference_vector(profile)\n        \n        # Calculate user clusters/segments\n        profile['user_segment'] = self._determine_user_segment(profile)\n        \n        # Calculate temporal patterns\n        profile['temporal_patterns'] = self._analyze_temporal_patterns(profile)\n        \n        # Calculate price sensitivity\n        profile['price_sensitivity'] = self._calculate_price_sensitivity(profile)\n        \n        # Calculate brand affinity\n        profile['brand_affinity'] = self._calculate_brand_affinity(profile)\n        \n        return profile\n    \n    async def _calculate_personalization_score(self,\n                                             user_profile: Dict[str, Any],\n                                             product: Dict[str, Any],\n                                             context: Dict[str, Any]) -> PersonalizedRecommendation:\n        \"\"\"Calculate comprehensive personalization score\"\"\"\n        \n        product_payload = product.get('payload', product)\n        base_score = product.get('score', 0.5)\n        \n        # Calculate different personalization factors\n        factors = {}\n        reasoning = []\n        \n        # 1. Content-based similarity\n        content_score = self._calculate_content_similarity(user_profile, product_payload)\n        factors['content_similarity'] = content_score\n        if content_score > 0.7:\n            reasoning.append(f\"Matches your interest in {product_payload.get('category', 'this category')}\")\n        \n        # 2. Collaborative filtering score\n        collaborative_score = self._calculate_collaborative_score(user_profile, product_payload)\n        factors['collaborative_filtering'] = collaborative_score\n        if collaborative_score > 0.6:\n            reasoning.append(\"Popular among users with similar preferences\")\n        \n        # 3. Behavioral alignment\n        behavioral_score = self._calculate_behavioral_alignment(user_profile, product_payload)\n        factors['behavioral_alignment'] = behavioral_score\n        if behavioral_score > 0.6:\n            reasoning.append(\"Matches your shopping behavior patterns\")\n        \n        # 4. Contextual relevance\n        contextual_score = self._calculate_contextual_relevance(user_profile, product_payload, context)\n        factors['contextual_relevance'] = contextual_score\n        if contextual_score > 0.6:\n            reasoning.append(\"Relevant to your current context\")\n        \n        # 5. Price alignment\n        price_score = self._calculate_price_alignment(user_profile, product_payload)\n        factors['price_alignment'] = price_score\n        if price_score > 0.7:\n            reasoning.append(\"Within your preferred price range\")\n        \n        # 6. Brand preference\n        brand_score = self._calculate_brand_preference(user_profile, product_payload)\n        factors['brand_preference'] = brand_score\n        if brand_score > 0.7:\n            reasoning.append(f\"From {product_payload.get('brand', 'a brand')} you've shown interest in\")\n        \n        # 7. Temporal relevance\n        temporal_score = self._calculate_temporal_relevance(user_profile, product_payload, context)\n        factors['temporal_relevance'] = temporal_score\n        if temporal_score > 0.6:\n            reasoning.append(\"Timely for your current needs\")\n        \n        # 8. Social proof\n        social_score = self._calculate_social_proof(product_payload)\n        factors['social_proof'] = social_score\n        if social_score > 0.8:\n            reasoning.append(\"Highly rated by other customers\")\n        \n        # Calculate weighted personalization score\n        personalization_weights = {\n            'content_similarity': 0.20,\n            'collaborative_filtering': 0.15,\n            'behavioral_alignment': 0.15,\n            'contextual_relevance': 0.10,\n            'price_alignment': 0.15,\n            'brand_preference': 0.10,\n            'temporal_relevance': 0.10,\n            'social_proof': 0.05\n        }\n        \n        personalization_score = sum(\n            factors[factor] * weight \n            for factor, weight in personalization_weights.items()\n        )\n        \n        # Combine with base relevance score\n        final_score = 0.6 * base_score + 0.4 * personalization_score\n        \n        # Calculate confidence based on data availability\n        confidence = self._calculate_confidence(user_profile, factors)\n        \n        # Generate explanation\n        explanation = self._generate_explanation(reasoning, product_payload, user_profile)\n        \n        return PersonalizedRecommendation(\n            product_id=product_payload.get('id'),\n            score=final_score,\n            reasoning=reasoning,\n            confidence=confidence,\n            personalization_factors=factors,\n            explanation=explanation,\n            metadata={\n                'base_score': base_score,\n                'personalization_score': personalization_score,\n                'weights_used': personalization_weights\n            }\n        )\n    \n    def _calculate_preference_vector(self, profile: Dict[str, Any]) -> np.ndarray:\n        \"\"\"Calculate user preference vector from interaction history\"\"\"\n        # Simplified implementation - in production, use more sophisticated methods\n        categories = ['electronics', 'clothing', 'books', 'home', 'sports', 'beauty']\n        vector = np.zeros(len(categories))\n        \n        purchase_history = profile.get('purchase_history', [])\n        for purchase in purchase_history:\n            category = purchase.get('category', '').lower()\n            if category in categories:\n                idx = categories.index(category)\n                vector[idx] += 1\n        \n        # Normalize\n        if np.sum(vector) > 0:\n            vector = vector / np.sum(vector)\n        \n        return vector\n    \n    def _determine_user_segment(self, profile: Dict[str, Any]) -> str:\n        \"\"\"Determine user segment based on behavior and preferences\"\"\"\n        behavioral = profile.get('behavioral_features', {})\n        \n        purchase_freq = behavioral.get('purchase_frequency', 0)\n        avg_order_value = behavioral.get('avg_order_value', 0)\n        session_duration = behavioral.get('avg_session_duration', 0)\n        \n        if purchase_freq > 10 and avg_order_value > 100:\n            return \"premium_frequent\"\n        elif purchase_freq > 10:\n            return \"frequent_budget\"\n        elif avg_order_value > 100:\n            return \"premium_occasional\"\n        elif session_duration > 300:\n            return \"researcher\"\n        else:\n            return \"casual\"\n    \n    def _analyze_temporal_patterns(self, profile: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Analyze user's temporal shopping patterns\"\"\"\n        # Simplified implementation\n        return {\n            'preferred_shopping_time': 'evening',\n            'seasonal_preferences': ['winter_clothing', 'summer_outdoor'],\n            'frequency_pattern': 'monthly'\n        }\n    \n    def _calculate_price_sensitivity(self, profile: Dict[str, Any]) -> float:\n        \"\"\"Calculate user's price sensitivity\"\"\"\n        purchase_history = profile.get('purchase_history', [])\n        if not purchase_history:\n            return 0.5  # Default neutral sensitivity\n        \n        prices = [p.get('price', 0) for p in purchase_history]\n        avg_price = np.mean(prices) if prices else 0\n        \n        # Normalize to 0-1 scale (simplified)\n        return min(avg_price / 200, 1.0)  # Assuming 200 is high price threshold\n    \n    def _calculate_brand_affinity(self, profile: Dict[str, Any]) -> Dict[str, float]:\n        \"\"\"Calculate user's brand preferences\"\"\"\n        purchase_history = profile.get('purchase_history', [])\n        brand_counts = {}\n        \n        for purchase in purchase_history:\n            brand = purchase.get('brand', 'unknown')\n            brand_counts[brand] = brand_counts.get(brand, 0) + 1\n        \n        total_purchases = len(purchase_history)\n        brand_affinity = {}\n        \n        for brand, count in brand_counts.items():\n            brand_affinity[brand] = count / total_purchases if total_purchases > 0 else 0\n        \n        return brand_affinity\n    \n    def _calculate_content_similarity(self, user_profile: Dict[str, Any], \n                                    product: Dict[str, Any]) -> float:\n        \"\"\"Calculate content-based similarity\"\"\"\n        user_preferences = user_profile.get('preferences', {})\n        \n        score = 0.0\n        \n        # Category preference\n        preferred_categories = user_preferences.get('preferred_categories', [])\n        product_category = product.get('category', '')\n        if product_category in preferred_categories:\n            score += 0.4\n        \n        # Brand preference\n        brand_affinity = user_profile.get('brand_affinity', {})\n        product_brand = product.get('brand', '')\n        brand_score = brand_affinity.get(product_brand, 0)\n        score += 0.3 * brand_score\n        \n        # Price alignment\n        price_sensitivity = user_profile.get('price_sensitivity', 0.5)\n        product_price = product.get('price', 0)\n        # Simplified price scoring\n        if product_price < 50 and price_sensitivity < 0.5:\n            score += 0.3\n        elif product_price > 100 and price_sensitivity > 0.7:\n            score += 0.3\n        elif 50 <= product_price <= 100:\n            score += 0.2\n        \n        return min(score, 1.0)\n    \n    def _calculate_collaborative_score(self, user_profile: Dict[str, Any],\n                                     product: Dict[str, Any]) -> float:\n        \"\"\"Calculate collaborative filtering score\"\"\"\n        # Simplified implementation - in production, use matrix factorization\n        user_segment = user_profile.get('user_segment', 'casual')\n        \n        # Mock collaborative scores based on user segment\n        collaborative_scores = {\n            'premium_frequent': 0.8,\n            'frequent_budget': 0.6,\n            'premium_occasional': 0.7,\n            'researcher': 0.65,\n            'casual': 0.5\n        }\n        \n        return collaborative_scores.get(user_segment, 0.5)\n    \n    def _calculate_behavioral_alignment(self, user_profile: Dict[str, Any],\n                                      product: Dict[str, Any]) -> float:\n        \"\"\"Calculate behavioral pattern alignment\"\"\"\n        behavioral = user_profile.get('behavioral_features', {})\n        \n        # Shopping frequency alignment\n        purchase_freq = behavioral.get('purchase_frequency', 0)\n        product_type = product.get('category', '').lower()\n        \n        if purchase_freq > 10 and product_type in ['consumables', 'electronics']:\n            return 0.8\n        elif purchase_freq < 5 and product_type in ['luxury', 'furniture']:\n            return 0.7\n        else:\n            return 0.5\n    \n    def _calculate_contextual_relevance(self, user_profile: Dict[str, Any],\n                                      product: Dict[str, Any],\n                                      context: Dict[str, Any]) -> float:\n        \"\"\"Calculate contextual relevance score\"\"\"\n        score = 0.5  # Base score\n        \n        # Seasonal relevance\n        current_season = context.get('season', 'unknown')\n        product_category = product.get('category', '').lower()\n        \n        seasonal_relevance = {\n            'winter': ['coats', 'heating', 'warm_clothing'],\n            'summer': ['swimwear', 'outdoor', 'cooling'],\n            'spring': ['gardening', 'light_clothing'],\n            'fall': ['jackets', 'school_supplies']\n        }\n        \n        if current_season in seasonal_relevance:\n            relevant_categories = seasonal_relevance[current_season]\n            if any(cat in product_category for cat in relevant_categories):\n                score += 0.3\n        \n        # Time-based relevance\n        current_hour = datetime.now().hour\n        if 9 <= current_hour <= 17 and 'office' in product_category:\n            score += 0.2\n        elif 18 <= current_hour <= 22 and 'entertainment' in product_category:\n            score += 0.2\n        \n        return min(score, 1.0)\n    \n    def _calculate_price_alignment(self, user_profile: Dict[str, Any],\n                                 product: Dict[str, Any]) -> float:\n        \"\"\"Calculate price alignment with user preferences\"\"\"\n        user_price_sensitivity = user_profile.get('price_sensitivity', 0.5)\n        product_price = product.get('price', 0)\n        \n        # Calculate price score based on user's typical spending\n        purchase_history = user_profile.get('purchase_history', [])\n        if purchase_history:\n            avg_user_price = np.mean([p.get('price', 0) for p in purchase_history])\n        else:\n            avg_user_price = 50  # Default assumption\n        \n        price_ratio = product_price / avg_user_price if avg_user_price > 0 else 1\n        \n        if user_price_sensitivity < 0.3:  # Budget-conscious\n            return max(0, 1 - (price_ratio - 0.8) / 0.5) if price_ratio > 0.8 else 1.0\n        elif user_price_sensitivity > 0.7:  # Premium-oriented\n            return min(1, price_ratio / 1.5) if price_ratio < 1.5 else 1.0\n        else:  # Balanced\n            return max(0, 1 - abs(price_ratio - 1) / 0.5)\n    \n    def _calculate_brand_preference(self, user_profile: Dict[str, Any],\n                                  product: Dict[str, Any]) -> float:\n        \"\"\"Calculate brand preference alignment\"\"\"\n        brand_affinity = user_profile.get('brand_affinity', {})\n        product_brand = product.get('brand', '')\n        \n        return brand_affinity.get(product_brand, 0.3)  # Default slight preference\n    \n    def _calculate_temporal_relevance(self, user_profile: Dict[str, Any],\n                                    product: Dict[str, Any],\n                                    context: Dict[str, Any]) -> float:\n        \"\"\"Calculate temporal relevance\"\"\"\n        temporal_patterns = user_profile.get('temporal_patterns', {})\n        \n        # Check if product aligns with user's shopping patterns\n        current_time = datetime.now().hour\n        preferred_time = temporal_patterns.get('preferred_shopping_time', 'evening')\n        \n        if preferred_time == 'morning' and 6 <= current_time <= 12:\n            return 0.8\n        elif preferred_time == 'evening' and 18 <= current_time <= 23:\n            return 0.8\n        else:\n            return 0.5\n    \n    def _calculate_social_proof(self, product: Dict[str, Any]) -> float:\n        \"\"\"Calculate social proof score\"\"\"\n        rating = product.get('rating', 0)\n        review_count = product.get('review_count', 0)\n        \n        # Normalize rating (0-5 to 0-1)\n        rating_score = rating / 5 if rating > 0 else 0\n        \n        # Review count bonus (logarithmic scale)\n        review_bonus = min(np.log(review_count + 1) / np.log(1000), 0.3) if review_count > 0 else 0\n        \n        return min(rating_score + review_bonus, 1.0)\n    \n    def _calculate_confidence(self, user_profile: Dict[str, Any], \n                            factors: Dict[str, float]) -> float:\n        \"\"\"Calculate confidence in personalization\"\"\"\n        # Base confidence on amount of user data available\n        data_points = 0\n        \n        if user_profile.get('purchase_history'):\n            data_points += len(user_profile['purchase_history'])\n        \n        if user_profile.get('interaction_history'):\n            data_points += min(len(user_profile['interaction_history']), 50)\n        \n        if user_profile.get('preferences'):\n            data_points += 10  # Explicit preferences are valuable\n        \n        # Normalize to 0-1 scale\n        data_confidence = min(data_points / 100, 1.0)\n        \n        # Factor in score variance (more consistent scores = higher confidence)\n        score_variance = np.var(list(factors.values()))\n        variance_penalty = min(score_variance, 0.3)\n        \n        return max(data_confidence - variance_penalty, 0.1)\n    \n    def _generate_explanation(self, reasoning: List[str], \n                            product: Dict[str, Any],\n                            user_profile: Dict[str, Any]) -> str:\n        \"\"\"Generate human-readable explanation\"\"\"\n        if not reasoning:\n            return f\"This {product.get('category', 'product')} might interest you\"\n        \n        primary_reason = reasoning[0]\n        \n        if len(reasoning) == 1:\n            return primary_reason\n        elif len(reasoning) == 2:\n            return f\"{primary_reason} and {reasoning[1].lower()}\"\n        else:\n            return f\"{primary_reason}, {reasoning[1].lower()}, and {len(reasoning)-2} other factors\"\n    \n    def _apply_diversification(self, recommendations: List[PersonalizedRecommendation],\n                             user_profile: Dict[str, Any]) -> List[PersonalizedRecommendation]:\n        \"\"\"Apply diversification to avoid echo chamber effect\"\"\"\n        if len(recommendations) <= 3:\n            return recommendations\n        \n        diversified = []\n        category_counts = {}\n        brand_counts = {}\n        \n        # Sort by score first\n        recommendations.sort(key=lambda x: x.score, reverse=True)\n        \n        for rec in recommendations:\n            # Get product info (simplified access)\n            category = rec.metadata.get('category', 'unknown')\n            brand = rec.metadata.get('brand', 'unknown')\n            \n            # Apply diversity constraints\n            max_per_category = max(len(recommendations) // 4, 2)\n            max_per_brand = max(len(recommendations) // 6, 1)\n            \n            if (category_counts.get(category, 0) < max_per_category and\n                brand_counts.get(brand, 0) < max_per_brand):\n                \n                diversified.append(rec)\n                category_counts[category] = category_counts.get(category, 0) + 1\n                brand_counts[brand] = brand_counts.get(brand, 0) + 1\n        \n        return diversified\n"})}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(n.p,{children:["In ",(0,o.jsx)(n.strong,{children:"Part 5"})," (final part) of this series, we'll focus on production deployment, monitoring, and optimization. We'll cover:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Production Architecture"}),": Scaling strategies and infrastructure design"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Performance Monitoring"}),": Tracking recommendation quality and system performance"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"A/B Testing Framework"}),": Measuring and optimizing recommendation effectiveness"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cost Optimization"}),": Strategies for managing LLM and infrastructure costs"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Continuous Improvement"}),": Feedback loops and model updates"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"what-weve-accomplished",children:(0,o.jsx)(n.strong,{children:"What We've Accomplished"})}),"\n",(0,o.jsx)(n.p,{children:"In this post, we've built an intelligent generation and personalization engine:"}),"\n",(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"Flexible LLM Integration"}),": Support for OpenAI, Anthropic, and local models with fallback\n\u2705 ",(0,o.jsx)(n.strong,{children:"Advanced Prompt Engineering"}),": Sophisticated prompts for different personalities and use cases\n\u2705 ",(0,o.jsx)(n.strong,{children:"Comprehensive Personalization"}),": Multi-factor personalization with confidence scoring\n\u2705 ",(0,o.jsx)(n.strong,{children:"Explainable Recommendations"}),": Clear reasoning and human-readable explanations\n\u2705 ",(0,o.jsx)(n.strong,{children:"Diversification Logic"}),": Preventing echo chambers and ensuring variety"]}),"\n",(0,o.jsx)(n.h3,{id:"before-part-5",children:(0,o.jsx)(n.strong,{children:"Before Part 5"})}),"\n",(0,o.jsx)(n.p,{children:"Make sure you have:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Integrated your chosen LLM provider"}),"\n",(0,o.jsx)(n.li,{children:"Tested prompt templates with your product data"}),"\n",(0,o.jsx)(n.li,{children:"Implemented basic personalization logic"}),"\n",(0,o.jsx)(n.li,{children:"Prepared production infrastructure planning"}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"The generation engine transforms raw search results into intelligent, personalized experiences. In our final post, we'll make it production-ready and continuously improving!"})})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},7814:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>s});var r=t(9729);const o={},a=r.createContext(o);function i(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);