"use strict";(self.webpackChunkdocs_site=self.webpackChunkdocs_site||[]).push([[113],{5741:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>i});var t=r(9729);const a={},s=t.createContext(a);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(s.Provider,{value:n},e.children)}},6831:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"backend/kafka-implementation","title":"Kafka Implementation Guide for E-commerce Microservices","description":"Overview","source":"@site/docs/backend/kafka-implementation.md","sourceDirName":"backend","slug":"/backend/kafka-implementation","permalink":"/fullstack-dev/docs/backend/kafka-implementation","draft":false,"unlisted":false,"editUrl":"https://github.com/tamnk74/fullstack-dev/tree/main/docs-site/docs/backend/kafka-implementation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"C4 Model - E-commerce Web Application","permalink":"/fullstack-dev/docs/backend/"},"next":{"title":"OpenTelemetry Implementation Guide","permalink":"/fullstack-dev/docs/backend/opentelemetry-implementation"}}');var a=r(5813),s=r(5741);const o={},i="Kafka Implementation Guide for E-commerce Microservices",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"Table of Contents",id:"table-of-contents",level:2},{value:"Event-Driven Architecture Design",id:"event-driven-architecture-design",level:2},{value:"Core Events",id:"core-events",level:3},{value:"Event Sourcing Pattern",id:"event-sourcing-pattern",level:3},{value:"Kafka Configuration",id:"kafka-configuration",level:2},{value:"1. Shared Kafka Configuration",id:"1-shared-kafka-configuration",level:3},{value:"2. Kafka Service Base Classes",id:"2-kafka-service-base-classes",level:3},{value:"Service Implementation",id:"service-implementation",level:2},{value:"1. Order Service Producer",id:"1-order-service-producer",level:3},{value:"2. Order Service Integration",id:"2-order-service-integration",level:3},{value:"3. Notification Service Consumer",id:"3-notification-service-consumer",level:3},{value:"4. Product Service with Inventory Management",id:"4-product-service-with-inventory-management",level:3},{value:"Kubernetes Deployment",id:"kubernetes-deployment",level:2},{value:"1. Kafka Cluster Deployment",id:"1-kafka-cluster-deployment",level:3},{value:"2. Topic Configuration",id:"2-topic-configuration",level:3},{value:"3. Service Deployment with Kafka Integration",id:"3-service-deployment-with-kafka-integration",level:3},{value:"4. Kafka Manager (Optional)",id:"4-kafka-manager-optional",level:3},{value:"Monitoring and Observability",id:"monitoring-and-observability",level:2},{value:"1. Kafka Monitoring with Prometheus",id:"1-kafka-monitoring-with-prometheus",level:3},{value:"2. Application Metrics Service",id:"2-application-metrics-service",level:3},{value:"3. Datadog Integration",id:"3-datadog-integration",level:3},{value:"Error Handling and Retry Logic",id:"error-handling-and-retry-logic",level:2},{value:"1. Retry Decorator",id:"1-retry-decorator",level:3},{value:"2. Dead Letter Queue Implementation",id:"2-dead-letter-queue-implementation",level:3},{value:"3. Enhanced Consumer with Error Handling",id:"3-enhanced-consumer-with-error-handling",level:3},{value:"Testing Strategy",id:"testing-strategy",level:2},{value:"1. Unit Tests for Kafka Services",id:"1-unit-tests-for-kafka-services",level:3},{value:"2. Integration Tests",id:"2-integration-tests",level:3},{value:"3. Testcontainers for Local Testing",id:"3-testcontainers-for-local-testing",level:3},{value:"Deployment Scripts",id:"deployment-scripts",level:2},{value:"1. Main Deployment Script",id:"1-main-deployment-script",level:3},{value:"2. Topic Management Script",id:"2-topic-management-script",level:3},{value:"3. Health Check Script",id:"3-health-check-script",level:3},{value:"Environment Configuration",id:"environment-configuration",level:2},{value:"1. Environment Variables",id:"1-environment-variables",level:3},{value:"2. ConfigMap for Kubernetes",id:"2-configmap-for-kubernetes",level:3},{value:"Best Practices and Recommendations",id:"best-practices-and-recommendations",level:2},{value:"1. Message Design",id:"1-message-design",level:3},{value:"2. Consumer Design",id:"2-consumer-design",level:3},{value:"3. Producer Design",id:"3-producer-design",level:3},{value:"4. Operations",id:"4-operations",level:3},{value:"5. Security",id:"5-security",level:3},{value:"Performance Optimization Guidelines",id:"performance-optimization-guidelines",level:2},{value:"1. Producer Optimization",id:"1-producer-optimization",level:3},{value:"2. Consumer Optimization",id:"2-consumer-optimization",level:3},{value:"3. Partitioning Strategy",id:"3-partitioning-strategy",level:3},{value:"Security Implementation",id:"security-implementation",level:2},{value:"1. SASL/SSL Configuration",id:"1-saslssl-configuration",level:3},{value:"2. Access Control Lists (ACLs)",id:"2-access-control-lists-acls",level:3},{value:"Data Governance and Compliance",id:"data-governance-and-compliance",level:2},{value:"1. Schema Registry Implementation",id:"1-schema-registry-implementation",level:3},{value:"2. GDPR Compliance Implementation",id:"2-gdpr-compliance-implementation",level:3},{value:"Disaster Recovery and Business Continuity",id:"disaster-recovery-and-business-continuity",level:2},{value:"1. Cross-Region Replication",id:"1-cross-region-replication",level:3},{value:"2. Backup and Recovery Scripts",id:"2-backup-and-recovery-scripts",level:3},{value:"CI/CD Integration with GitHub Actions",id:"cicd-integration-with-github-actions",level:2},{value:"1. Kafka Testing Pipeline",id:"1-kafka-testing-pipeline",level:3},{value:"2. ArgoCD Deployment Configuration",id:"2-argocd-deployment-configuration",level:3},{value:"Troubleshooting Guide",id:"troubleshooting-guide",level:2},{value:"1. Common Issues and Solutions",id:"1-common-issues-and-solutions",level:3},{value:"2. Debug Commands and Tools",id:"2-debug-commands-and-tools",level:3}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"kafka-implementation-guide-for-e-commerce-microservices",children:"Kafka Implementation Guide for E-commerce Microservices"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"This guide provides step-by-step implementation of Apache Kafka for your e-commerce platform using NestJS microservices, deployed on Google Kubernetes Engine (GKE)."}),"\n",(0,a.jsx)(n.h2,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   User Service  \u2502    \u2502  Order Service  \u2502    \u2502 Payment Service \u2502\n\u2502                 \u2502    \u2502                 \u2502    \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                      \u2502                      \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Apache Kafka      \u2502\n                    \u2502   Message Broker    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502                      \u2502                      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Product Service \u2502    \u2502 Notification    \u2502    \u2502 Analytics       \u2502\n\u2502                 \u2502    \u2502 Service         \u2502    \u2502 Service         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,a.jsx)(n.h2,{id:"table-of-contents",children:"Table of Contents"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#event-driven-architecture-design",children:"Event-Driven Architecture Design"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#kafka-configuration",children:"Kafka Configuration"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#service-implementation",children:"Service Implementation"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#kubernetes-deployment",children:"Kubernetes Deployment"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#monitoring-and-observability",children:"Monitoring and Observability"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#error-handling-and-retry-logic",children:"Error Handling and Retry Logic"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#testing-strategy",children:"Testing Strategy"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"#deployment-scripts",children:"Deployment Scripts"})}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"event-driven-architecture-design",children:"Event-Driven Architecture Design"}),"\n",(0,a.jsx)(n.h3,{id:"core-events",children:"Core Events"}),"\n",(0,a.jsx)(n.p,{children:"Create shared types for consistent event handling across services:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// shared/types/src/events/base.event.ts\nexport interface BaseEvent {\n  id: string;\n  type: string;\n  aggregateId: string;\n  version: number;\n  timestamp: Date;\n  metadata?: Record<string, any>;\n}\n\n// shared/types/src/events/user.events.ts\nexport interface UserEvents {\n  USER_CREATED: {\n    userId: string;\n    email: string;\n    firstName: string;\n    lastName: string;\n    profile: UserProfile;\n  };\n  USER_UPDATED: {\n    userId: string;\n    changes: Partial<UserProfile>;\n  };\n  USER_DELETED: {\n    userId: string;\n  };\n}\n\n// shared/types/src/events/order.events.ts\nexport interface OrderEvents {\n  ORDER_CREATED: {\n    orderId: string;\n    userId: string;\n    items: OrderItem[];\n    totalAmount: number;\n    shippingAddress: Address;\n  };\n  ORDER_CONFIRMED: {\n    orderId: string;\n    userId: string;\n    estimatedDelivery: Date;\n  };\n  ORDER_SHIPPED: {\n    orderId: string;\n    trackingNumber: string;\n    carrier: string;\n  };\n  ORDER_DELIVERED: {\n    orderId: string;\n    deliveredAt: Date;\n  };\n  ORDER_CANCELLED: {\n    orderId: string;\n    reason: string;\n    refundAmount: number;\n  };\n}\n\n// shared/types/src/events/payment.events.ts\nexport interface PaymentEvents {\n  PAYMENT_INITIATED: {\n    paymentId: string;\n    orderId: string;\n    amount: number;\n    currency: string;\n    method: PaymentMethod;\n  };\n  PAYMENT_COMPLETED: {\n    paymentId: string;\n    orderId: string;\n    transactionId: string;\n    amount: number;\n  };\n  PAYMENT_FAILED: {\n    paymentId: string;\n    orderId: string;\n    errorCode: string;\n    errorMessage: string;\n  };\n  REFUND_PROCESSED: {\n    refundId: string;\n    paymentId: string;\n    amount: number;\n  };\n}\n\n// shared/types/src/events/product.events.ts\nexport interface ProductEvents {\n  PRODUCT_CREATED: {\n    productId: string;\n    name: string;\n    price: number;\n    category: string;\n  };\n  INVENTORY_UPDATED: {\n    productId: string;\n    quantity: number;\n    operation: 'add' | 'subtract' | 'set';\n  };\n  PRODUCT_VIEWED: {\n    productId: string;\n    userId?: string;\n    sessionId: string;\n  };\n}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"event-sourcing-pattern",children:"Event Sourcing Pattern"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// shared/types/src/models/event-store.model.ts\nexport interface EventStore {\n  id: string;\n  aggregateId: string;\n  aggregateType: string;\n  eventType: string;\n  eventData: any;\n  version: number;\n  timestamp: Date;\n  metadata?: Record<string, any>;\n}\n\n// shared/services/src/event-sourcing/event-store.service.ts\nimport { Injectable } from '@nestjs/common';\nimport { BaseEvent } from '../types';\n\n@Injectable()\nexport class EventStoreService {\n  async saveEvent(event: BaseEvent): Promise<void> {\n    // Save to your preferred database (PostgreSQL, MongoDB, etc.)\n    // This ensures event persistence and replay capability\n  }\n\n  async getEvents(aggregateId: string): Promise<BaseEvent[]> {\n    // Retrieve all events for an aggregate\n  }\n\n  async getEventsSince(timestamp: Date): Promise<BaseEvent[]> {\n    // Get events for replay scenarios\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"kafka-configuration",children:"Kafka Configuration"}),"\n",(0,a.jsx)(n.h3,{id:"1-shared-kafka-configuration",children:"1. Shared Kafka Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// shared/kafka/src/config/kafka.config.ts\nimport { Transport } from '@nestjs/microservices';\n\nexport interface KafkaServiceConfig {\n  clientId: string;\n  groupId: string;\n  brokers: string[];\n}\n\nexport const createKafkaConfig = (serviceConfig: KafkaServiceConfig) => ({\n  transport: Transport.KAFKA,\n  options: {\n    client: {\n      clientId: serviceConfig.clientId,\n      brokers: serviceConfig.brokers,\n      retry: {\n        initialRetryTime: 100,\n        retries: 8,\n      },\n      connectionTimeout: 3000,\n      requestTimeout: 30000,\n    },\n    consumer: {\n      groupId: serviceConfig.groupId,\n      allowAutoTopicCreation: false,\n      sessionTimeout: 30000,\n      heartbeatInterval: 3000,\n      maxWaitTimeInMs: 5000,\n      retry: {\n        retries: 5,\n      },\n    },\n    producer: {\n      allowAutoTopicCreation: false,\n      transactionTimeout: 30000,\n      retry: {\n        retries: 5,\n      },\n    },\n  },\n});\n\nexport const kafkaTopics = {\n  USER_EVENTS: 'user-events',\n  ORDER_EVENTS: 'order-events',\n  PAYMENT_EVENTS: 'payment-events',\n  PRODUCT_EVENTS: 'product-events',\n  NOTIFICATION_EVENTS: 'notification-events',\n  ANALYTICS_EVENTS: 'analytics-events',\n} as const;\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-kafka-service-base-classes",children:"2. Kafka Service Base Classes"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// shared/kafka/src/services/kafka-producer.service.ts\nimport { Injectable, Logger, OnModuleDestroy, OnModuleInit } from '@nestjs/common';\nimport { ClientKafka } from '@nestjs/microservices';\nimport { BaseEvent } from '@ecommerce/types';\n\n@Injectable()\nexport abstract class KafkaProducerService implements OnModuleInit, OnModuleDestroy {\n  protected readonly logger = new Logger(this.constructor.name);\n  protected abstract kafkaClient: ClientKafka;\n\n  async onModuleInit() {\n    await this.kafkaClient.connect();\n    this.logger.log('Kafka producer connected');\n  }\n\n  async onModuleDestroy() {\n    await this.kafkaClient.close();\n    this.logger.log('Kafka producer disconnected');\n  }\n\n  async publishEvent<T extends BaseEvent>(topic: string, event: T): Promise<void> {\n    try {\n      await this.kafkaClient.emit(topic, {\n        key: event.aggregateId,\n        value: JSON.stringify(event),\n        headers: {\n          eventType: event.type,\n          version: event.version.toString(),\n          timestamp: event.timestamp.toISOString(),\n        },\n      });\n\n      this.logger.debug(`Event published to ${topic}:`, event.type);\n    } catch (error) {\n      this.logger.error(`Failed to publish event to ${topic}:`, error);\n      throw error;\n    }\n  }\n}\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// shared/kafka/src/services/kafka-consumer.service.ts\nimport { Logger } from '@nestjs/common';\nimport { KafkaContext } from '@nestjs/microservices';\n\nexport abstract class KafkaConsumerService {\n  protected readonly logger = new Logger(this.constructor.name);\n\n  protected async handleMessage<T>(\n    payload: any,\n    context: KafkaContext,\n    processor: (event: T) => Promise<void>,\n  ): Promise<void> {\n    const message = context.getMessage();\n    const { partition, offset } = message;\n\n    try {\n      const event = this.parseEvent<T>(payload);\n      await processor(event);\n\n      this.logger.debug(`Processed message from partition ${partition}, offset ${offset}`);\n    } catch (error) {\n      this.logger.error(`Failed to process message:`, error);\n      throw error;\n    }\n  }\n\n  private parseEvent<T>(payload: any): T {\n    try {\n      return typeof payload === 'string' ? JSON.parse(payload) : payload;\n    } catch (error) {\n      throw new Error(`Invalid event format: ${error.message}`);\n    }\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"service-implementation",children:"Service Implementation"}),"\n",(0,a.jsx)(n.h3,{id:"1-order-service-producer",children:"1. Order Service Producer"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// services/order-service/src/kafka/order-producer.service.ts\nimport { Injectable } from '@nestjs/common';\nimport { ClientKafka, ClientProxyFactory } from '@nestjs/microservices';\nimport { KafkaProducerService, kafkaTopics, createKafkaConfig } from '@ecommerce/kafka';\nimport { OrderEvents } from '@ecommerce/types';\n\n@Injectable()\nexport class OrderProducerService extends KafkaProducerService {\n  protected kafkaClient: ClientKafka;\n\n  constructor() {\n    super();\n    this.kafkaClient = ClientProxyFactory.create(\n      createKafkaConfig({\n        clientId: 'order-service-producer',\n        groupId: 'order-service-group',\n        brokers: [process.env.KAFKA_BROKER || 'localhost:9092'],\n      }),\n    ) as ClientKafka;\n  }\n\n  async publishOrderCreated(event: OrderEvents['ORDER_CREATED']): Promise<void> {\n    await this.publishEvent(kafkaTopics.ORDER_EVENTS, {\n      id: crypto.randomUUID(),\n      type: 'ORDER_CREATED',\n      aggregateId: event.orderId,\n      version: 1,\n      timestamp: new Date(),\n      ...event,\n    });\n  }\n\n  async publishOrderConfirmed(event: OrderEvents['ORDER_CONFIRMED']): Promise<void> {\n    await this.publishEvent(kafkaTopics.ORDER_EVENTS, {\n      id: crypto.randomUUID(),\n      type: 'ORDER_CONFIRMED',\n      aggregateId: event.orderId,\n      version: 1,\n      timestamp: new Date(),\n      ...event,\n    });\n  }\n\n  async publishOrderShipped(event: OrderEvents['ORDER_SHIPPED']): Promise<void> {\n    await this.publishEvent(kafkaTopics.ORDER_EVENTS, {\n      id: crypto.randomUUID(),\n      type: 'ORDER_SHIPPED',\n      aggregateId: event.orderId,\n      version: 1,\n      timestamp: new Date(),\n      ...event,\n    });\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-order-service-integration",children:"2. Order Service Integration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// services/order-service/src/orders/orders.service.ts\nimport { Injectable } from '@nestjs/common';\nimport { OrderProducerService } from '../kafka/order-producer.service';\nimport { CreateOrderDto } from './dto/create-order.dto';\n\n@Injectable()\nexport class OrdersService {\n  constructor(\n    private readonly orderProducer: OrderProducerService, // ... other dependencies\n  ) {}\n\n  async createOrder(createOrderDto: CreateOrderDto, userId: string): Promise<Order> {\n    // 1. Validate order data\n    // 2. Check inventory availability\n    // 3. Calculate totals\n\n    const order = await this.orderRepository.save({\n      ...createOrderDto,\n      userId,\n      status: 'PENDING',\n      createdAt: new Date(),\n    });\n\n    // Publish order created event\n    await this.orderProducer.publishOrderCreated({\n      orderId: order.id,\n      userId: order.userId,\n      items: order.items,\n      totalAmount: order.totalAmount,\n      shippingAddress: order.shippingAddress,\n    });\n\n    return order;\n  }\n\n  async confirmOrder(orderId: string): Promise<void> {\n    const order = await this.orderRepository.findOne({ where: { id: orderId } });\n    if (!order) {\n      throw new NotFoundException('Order not found');\n    }\n\n    order.status = 'CONFIRMED';\n    order.estimatedDelivery = this.calculateDeliveryDate();\n    await this.orderRepository.save(order);\n\n    await this.orderProducer.publishOrderConfirmed({\n      orderId: order.id,\n      userId: order.userId,\n      estimatedDelivery: order.estimatedDelivery,\n    });\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3-notification-service-consumer",children:"3. Notification Service Consumer"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// services/notification-service/src/kafka/notification-consumer.controller.ts\nimport { Controller } from '@nestjs/common';\nimport { EventPattern, Payload, Ctx, KafkaContext } from '@nestjs/microservices';\nimport { KafkaConsumerService, kafkaTopics } from '@ecommerce/kafka';\nimport { OrderEvents, PaymentEvents, UserEvents } from '@ecommerce/types';\nimport { NotificationService } from '../services/notification.service';\n\n@Controller()\nexport class NotificationConsumerController extends KafkaConsumerService {\n  constructor(private readonly notificationService: NotificationService) {\n    super();\n  }\n\n  @EventPattern(kafkaTopics.ORDER_EVENTS)\n  async handleOrderEvent(@Payload() payload: any, @Ctx() context: KafkaContext): Promise<void> {\n    await this.handleMessage(payload, context, async (event: any) => {\n      switch (event.type) {\n        case 'ORDER_CREATED':\n          await this.notificationService.sendOrderConfirmation(event);\n          break;\n        case 'ORDER_SHIPPED':\n          await this.notificationService.sendShippingNotification(event);\n          break;\n        case 'ORDER_DELIVERED':\n          await this.notificationService.sendDeliveryNotification(event);\n          break;\n      }\n    });\n  }\n\n  @EventPattern(kafkaTopics.PAYMENT_EVENTS)\n  async handlePaymentEvent(@Payload() payload: any, @Ctx() context: KafkaContext): Promise<void> {\n    await this.handleMessage(payload, context, async (event: any) => {\n      switch (event.type) {\n        case 'PAYMENT_COMPLETED':\n          await this.notificationService.sendPaymentConfirmation(event);\n          break;\n        case 'PAYMENT_FAILED':\n          await this.notificationService.sendPaymentFailure(event);\n          break;\n      }\n    });\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"4-product-service-with-inventory-management",children:"4. Product Service with Inventory Management"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// services/product-service/src/kafka/inventory-consumer.controller.ts\nimport { Controller } from '@nestjs/common';\nimport { EventPattern, Payload, Ctx, KafkaContext } from '@nestjs/microservices';\nimport { KafkaConsumerService, kafkaTopics } from '@ecommerce/kafka';\nimport { OrderEvents } from '@ecommerce/types';\nimport { InventoryService } from '../services/inventory.service';\nimport { ProductProducerService } from './product-producer.service';\n\n@Controller()\nexport class InventoryConsumerController extends KafkaConsumerService {\n  constructor(\n    private readonly inventoryService: InventoryService,\n    private readonly productProducer: ProductProducerService,\n  ) {\n    super();\n  }\n\n  @EventPattern(kafkaTopics.ORDER_EVENTS)\n  async handleOrderEvent(@Payload() payload: any, @Ctx() context: KafkaContext): Promise<void> {\n    await this.handleMessage(payload, context, async (event: any) => {\n      switch (event.type) {\n        case 'ORDER_CREATED':\n          await this.reserveInventory(event);\n          break;\n        case 'ORDER_CANCELLED':\n          await this.releaseInventory(event);\n          break;\n      }\n    });\n  }\n\n  private async reserveInventory(event: OrderEvents['ORDER_CREATED']): Promise<void> {\n    for (const item of event.items) {\n      const updated = await this.inventoryService.reserveQuantity(item.productId, item.quantity);\n\n      if (updated) {\n        await this.productProducer.publishInventoryUpdated({\n          productId: item.productId,\n          quantity: -item.quantity,\n          operation: 'subtract',\n        });\n      }\n    }\n  }\n\n  private async releaseInventory(event: OrderEvents['ORDER_CANCELLED']): Promise<void> {\n    // Implementation for releasing reserved inventory\n    for (const item of event.items) {\n      await this.inventoryService.releaseReservedQuantity(item.productId, item.quantity);\n\n      await this.productProducer.publishInventoryUpdated({\n        productId: item.productId,\n        quantity: item.quantity,\n        operation: 'add',\n      });\n    }\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"kubernetes-deployment",children:"Kubernetes Deployment"}),"\n",(0,a.jsx)(n.h3,{id:"1-kafka-cluster-deployment",children:"1. Kafka Cluster Deployment"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# k8s/kafka/namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: kafka\n---\n# k8s/kafka/zookeeper.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zookeeper\n  namespace: kafka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: zookeeper\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n        runAsNonRoot: true\n      containers:\n        - name: zookeeper\n          image: confluentinc/cp-zookeeper:7.4.0\n          env:\n            - name: ZOOKEEPER_CLIENT_PORT\n              value: '2181'\n            - name: ZOOKEEPER_TICK_TIME\n              value: '2000'\n            - name: ZOOKEEPER_DATA_DIR\n              value: '/var/lib/zookeeper/data'\n            - name: ZOOKEEPER_LOG_DIR\n              value: '/var/lib/zookeeper/logs'\n            - name: ZOOKEEPER_SERVER_ID\n              value: '1'\n          ports:\n            - containerPort: 2181\n          resources:\n            requests:\n              memory: '512Mi'\n              cpu: '250m'\n            limits:\n              memory: '1Gi'\n              cpu: '500m'\n          volumeMounts:\n            - name: zookeeper-data\n              mountPath: /var/lib/zookeeper/data\n            - name: zookeeper-logs\n              mountPath: /var/lib/zookeeper/logs\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n            runAsNonRoot: true\n            runAsUser: 1000\n            runAsGroup: 1000\n            capabilities:\n              drop:\n                - ALL\n          readinessProbe:\n            exec:\n              command:\n                - sh\n                - -c\n                - 'echo ruok | nc localhost 2181 | grep imok'\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            timeoutSeconds: 3\n            failureThreshold: 3\n          livenessProbe:\n            exec:\n              command:\n                - sh\n                - -c\n                - 'echo ruok | nc localhost 2181 | grep imok'\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 3\n            failureThreshold: 3\n          livenessProbe:\n            tcpSocket:\n              port: 2181\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 3\n            failureThreshold: 3\n      volumes:\n        - name: zookeeper-data\n          persistentVolumeClaim:\n            claimName: zookeeper-data-pvc\n        - name: zookeeper-logs\n          persistentVolumeClaim:\n            claimName: zookeeper-logs-pvc\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: zookeeper-service\n  namespace: kafka\nspec:\n  selector:\n    app: zookeeper\n  ports:\n    - port: 2181\n      targetPort: 2181\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: zookeeper-data-pvc\n  namespace: kafka\n  annotations:\n    volume.beta.kubernetes.io/storage-class: \"standard-rwo\"\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: standard-rwo\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: zookeeper-logs-pvc\n  namespace: kafka\n  annotations:\n    volume.beta.kubernetes.io/storage-class: \"standard-rwo\"\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n  storageClassName: standard-rwo\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# k8s/kafka/kafka-broker.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-broker\n  namespace: kafka\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kafka-broker\n  template:\n    metadata:\n      labels:\n        app: kafka-broker\n    spec:\n      containers:\n        - name: kafka-broker\n          image: confluentinc/cp-kafka:7.4.0\n          env:\n            - name: KAFKA_ZOOKEEPER_CONNECT\n              value: 'zookeeper-service:2181'\n            - name: KAFKA_ADVERTISED_LISTENERS\n              value: 'PLAINTEXT://kafka-service:9092'\n            - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\n              value: '1'\n            - name: KAFKA_AUTO_CREATE_TOPICS_ENABLE\n              value: 'false'\n            - name: KAFKA_LOG_RETENTION_HOURS\n              value: '168'\n            - name: KAFKA_LOG_RETENTION_BYTES\n              value: '1073741824'\n            - name: KAFKA_NUM_PARTITIONS\n              value: '3'\n            - name: KAFKA_DEFAULT_REPLICATION_FACTOR\n              value: '1'\n          ports:\n            - containerPort: 9092\n          resources:\n            requests:\n              memory: '1Gi'\n              cpu: '500m'\n            limits:\n              memory: '2Gi'\n              cpu: '1000m'\n          volumeMounts:\n            - name: kafka-data\n              mountPath: /var/lib/kafka/data\n      volumes:\n        - name: kafka-data\n          persistentVolumeClaim:\n            claimName: kafka-pvc\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-service\n  namespace: kafka\nspec:\n  selector:\n    app: kafka-broker\n  ports:\n    - port: 9092\n      targetPort: 9092\n  type: ClusterIP\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: kafka-pvc\n  namespace: kafka\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 50Gi\n  storageClassName: standard-rwo\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-topic-configuration",children:"2. Topic Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# k8s/kafka/topics-job.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: kafka-topics-setup\n  namespace: kafka\nspec:\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n        - name: kafka-topics\n          image: confluentinc/cp-kafka:7.4.0\n          command: [\'/bin/sh\']\n          args:\n            - -c\n            - |\n              # Wait for Kafka to be ready\n              until kafka-topics --bootstrap-server kafka-service:9092 --list; do\n                echo "Waiting for Kafka..."\n                sleep 5\n              done\n\n              # Create topics\n              kafka-topics --bootstrap-server kafka-service:9092 --create --topic notification.email --partitions 1 --replication-factor 1 --if-not-exists\n\n              echo "Topics created successfully"\n\n              # List created topics\n              kafka-topics --bootstrap-server kafka-service:9092 --list\n'})}),"\n",(0,a.jsx)(n.h3,{id:"3-service-deployment-with-kafka-integration",children:"3. Service Deployment with Kafka Integration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# k8s/services/order-service.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order-service\n  namespace: ecommerce\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: order-service\n  template:\n    metadata:\n      labels:\n        app: order-service\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '3000'\n        prometheus.io/path: '/metrics'\n    spec:\n      containers:\n        - name: order-service\n          image: gcr.io/YOUR_PROJECT/order-service:latest\n          env:\n            - name: KAFKA_BROKER\n              value: 'kafka-service.kafka.svc.cluster.local:9092'\n            - name: KAFKA_CLIENT_ID\n              value: 'order-service'\n            - name: KAFKA_GROUP_ID\n              value: 'order-service-group'\n            - name: DATABASE_URL\n              valueFrom:\n                secretKeyRef:\n                  name: db-secrets\n                  key: order-db-url\n            - name: NODE_ENV\n              value: 'production'\n          ports:\n            - containerPort: 3000\n              name: http\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 3000\n            initialDelaySeconds: 10\n            periodSeconds: 5\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 3000\n            initialDelaySeconds: 30\n            periodSeconds: 10\n          resources:\n            requests:\n              memory: '512Mi'\n              cpu: '250m'\n            limits:\n              memory: '1Gi'\n              cpu: '500m'\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: order-service\n  namespace: ecommerce\nspec:\n  selector:\n    app: order-service\n  ports:\n    - port: 80\n      targetPort: 3000\n      name: http\n"})}),"\n",(0,a.jsx)(n.h3,{id:"4-kafka-manager-optional",children:"4. Kafka Manager (Optional)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# k8s/kafka/kafka-manager.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-manager\n  namespace: kafka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kafka-manager\n  template:\n    metadata:\n      labels:\n        app: kafka-manager\n    spec:\n      containers:\n        - name: kafka-manager\n          image: hlebalbau/kafka-manager:stable\n          env:\n            - name: ZK_HOSTS\n              value: 'zookeeper-service:2181'\n            - name: APPLICATION_SECRET\n              value: 'random-secret'\n          ports:\n            - containerPort: 9000\n          resources:\n            requests:\n              memory: '256Mi'\n              cpu: '100m'\n            limits:\n              memory: '512Mi'\n              cpu: '200m'\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-manager-service\n  namespace: kafka\nspec:\n  selector:\n    app: kafka-manager\n  ports:\n    - port: 9000\n      targetPort: 9000\n  type: LoadBalancer\n"})}),"\n",(0,a.jsx)(n.h2,{id:"monitoring-and-observability",children:"Monitoring and Observability"}),"\n",(0,a.jsx)(n.h3,{id:"1-kafka-monitoring-with-prometheus",children:"1. Kafka Monitoring with Prometheus"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# k8s/monitoring/kafka-exporter.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-exporter\n  namespace: kafka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kafka-exporter\n  template:\n    metadata:\n      labels:\n        app: kafka-exporter\n    spec:\n      containers:\n        - name: kafka-exporter\n          image: danielqsj/kafka-exporter:v1.6.0\n          args:\n            - --kafka.server=kafka-service:9092\n            - --web.listen-address=:9308\n            - --log.level=info\n          ports:\n            - containerPort: 9308\n              name: metrics\n          resources:\n            requests:\n              memory: '64Mi'\n              cpu: '50m'\n            limits:\n              memory: '128Mi'\n              cpu: '100m'\n          livenessProbe:\n            httpGet:\n              path: /metrics\n              port: 9308\n            initialDelaySeconds: 30\n            periodSeconds: 10\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-exporter-service\n  namespace: kafka\n  labels:\n    app: kafka-exporter\nspec:\n  selector:\n    app: kafka-exporter\n  ports:\n    - port: 9308\n      targetPort: 9308\n      name: metrics\n---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: kafka-exporter\n  namespace: kafka\nspec:\n  selector:\n    matchLabels:\n      app: kafka-exporter\n  endpoints:\n    - port: metrics\n      interval: 30s\n      path: /metrics\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-application-metrics-service",children:"2. Application Metrics Service"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// shared/monitoring/src/kafka-metrics.service.ts\nimport { Injectable } from '@nestjs/common';\nimport { Counter, Histogram, Gauge, register } from 'prom-client';\n\n@Injectable()\nexport class KafkaMetricsService {\n  private readonly messageCounter = new Counter({\n    name: 'kafka_messages_total',\n    help: 'Total number of Kafka messages processed',\n    labelNames: ['topic', 'status', 'service'],\n  });\n\n  private readonly messageProcessingDuration = new Histogram({\n    name: 'kafka_message_processing_duration_seconds',\n    help: 'Duration of message processing in seconds',\n    labelNames: ['topic', 'service'],\n    buckets: [0.01, 0.05, 0.1, 0.5, 1, 5, 10],\n  });\n\n  private readonly messageSize = new Histogram({\n    name: 'kafka_message_size_bytes',\n    help: 'Size of Kafka messages in bytes',\n    labelNames: ['topic', 'service'],\n    buckets: [100, 1000, 10000, 100000, 1000000],\n  });\n\n  private readonly connectionGauge = new Gauge({\n    name: 'kafka_connections_active',\n    help: 'Number of active Kafka connections',\n    labelNames: ['service', 'type'], // type: producer or consumer\n  });\n\n  constructor() {\n    register.registerMetric(this.messageCounter);\n    register.registerMetric(this.messageProcessingDuration);\n    register.registerMetric(this.messageSize);\n    register.registerMetric(this.connectionGauge);\n  }\n\n  recordMessageProcessed(topic: string, service: string, status: 'success' | 'error'): void {\n    this.messageCounter.inc({ topic, status, service });\n  }\n\n  recordProcessingDuration(topic: string, service: string, duration: number): void {\n    this.messageProcessingDuration.observe({ topic, service }, duration);\n  }\n\n  recordMessageSize(topic: string, service: string, size: number): void {\n    this.messageSize.observe({ topic, service }, size);\n  }\n\n  setActiveConnections(service: string, type: 'producer' | 'consumer', count: number): void {\n    this.connectionGauge.set({ service, type }, count);\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3-datadog-integration",children:"3. Datadog Integration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// shared/monitoring/src/datadog-kafka.service.ts\nimport { Injectable } from '@nestjs/common';\nimport { StatsD } from 'node-statsd';\n\n@Injectable()\nexport class DatadogKafkaService {\n  private statsd: StatsD;\n\n  constructor() {\n    this.statsd = new StatsD({\n      host: process.env.DATADOG_AGENT_HOST || 'localhost',\n      port: parseInt(process.env.DATADOG_AGENT_PORT || '8125'),\n      prefix: 'ecommerce.kafka.',\n    });\n  }\n\n  recordMessagePublished(topic: string, service: string): void {\n    this.statsd.increment('message.published', 1, [`topic:${topic}`, `service:${service}`]);\n  }\n\n  recordMessageConsumed(topic: string, service: string, processingTime: number): void {\n    this.statsd.increment('message.consumed', 1, [`topic:${topic}`, `service:${service}`]);\n    this.statsd.timing('message.processing_time', processingTime, [`topic:${topic}`, `service:${service}`]);\n  }\n\n  recordError(topic: string, service: string, errorType: string): void {\n    this.statsd.increment('message.error', 1, [`topic:${topic}`, `service:${service}`, `error_type:${errorType}`]);\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"error-handling-and-retry-logic",children:"Error Handling and Retry Logic"}),"\n",(0,a.jsx)(n.h3,{id:"1-retry-decorator",children:"1. Retry Decorator"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// shared/kafka/src/decorators/retry.decorator.ts\nimport { Logger } from '@nestjs/common';\n\nexport function KafkaRetry(maxRetries: number = 3, backoffMs: number = 1000) {\n  return function (target: any, propertyName: string, descriptor: PropertyDescriptor) {\n    const method = descriptor.value;\n    const logger = new Logger(`${target.constructor.name}:${propertyName}`);\n\n    descriptor.value = async function (...args: any[]) {\n      let lastError: Error;\n\n      for (let attempt = 1; attempt <= maxRetries; attempt++) {\n        try {\n          return await method.apply(this, args);\n        } catch (error) {\n          lastError = error;\n          logger.warn(`Attempt ${attempt}/${maxRetries} failed:`, error.message);\n\n          if (attempt < maxRetries) {\n            const delay = backoffMs * Math.pow(2, attempt - 1); // Exponential backoff\n            await new Promise((resolve) => setTimeout(resolve, delay));\n          }\n        }\n      }\n\n      logger.error(`All ${maxRetries} attempts failed. Last error:`, lastError);\n      throw lastError;\n    };\n  };\n}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-dead-letter-queue-implementation",children:"2. Dead Letter Queue Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// shared/kafka/src/services/dead-letter-queue.service.ts\nimport { Injectable, Logger } from '@nestjs/common';\nimport { ClientKafka } from '@nestjs/microservices';\n\n@Injectable()\nexport class DeadLetterQueueService {\n  private readonly logger = new Logger(DeadLetterQueueService.name);\n\n  constructor(private readonly kafkaClient: ClientKafka) {}\n\n  async sendToDeadLetterQueue(\n    originalTopic: string,\n    message: any,\n    error: Error,\n    retryCount: number = 0,\n  ): Promise<void> {\n    const dlqTopic = `${originalTopic}.dlq`;\n\n    try {\n      await this.kafkaClient.emit(dlqTopic, {\n        ...message,\n        metadata: {\n          ...message.metadata,\n          originalTopic,\n          error: error.message,\n          retryCount,\n          failedAt: new Date().toISOString(),\n        },\n      });\n\n      this.logger.error(`Message sent to DLQ: ${dlqTopic}`, {\n        originalTopic,\n        error: error.message,\n        retryCount,\n      });\n    } catch (dlqError) {\n      this.logger.error(`Failed to send message to DLQ: ${dlqTopic}`, dlqError);\n    }\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3-enhanced-consumer-with-error-handling",children:"3. Enhanced Consumer with Error Handling"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// shared/kafka/src/services/enhanced-kafka-consumer.service.ts\nimport { Logger } from '@nestjs/common';\nimport { KafkaContext } from '@nestjs/microservices';\nimport { KafkaMetricsService } from '@ecommerce/monitoring';\nimport { DeadLetterQueueService } from './dead-letter-queue.service';\n\nexport abstract class EnhancedKafkaConsumerService {\n  protected readonly logger = new Logger(this.constructor.name);\n\n  constructor(\n    protected readonly metricsService: KafkaMetricsService,\n    protected readonly dlqService: DeadLetterQueueService,\n  ) {}\n\n  protected async handleMessageWithRetry<T>(\n    payload: any,\n    context: KafkaContext,\n    processor: (event: T) => Promise<void>,\n    maxRetries: number = 3,\n  ): Promise<void> {\n    const message = context.getMessage();\n    const topic = context.getTopic();\n    const { partition, offset } = message;\n    const startTime = Date.now();\n\n    try {\n      const event = this.parseEvent<T>(payload);\n      await processor(event);\n\n      const processingTime = Date.now() - startTime;\n      this.metricsService.recordMessageProcessed(topic, this.constructor.name, 'success');\n      this.metricsService.recordProcessingDuration(topic, this.constructor.name, processingTime / 1000);\n\n      this.logger.debug(`Successfully processed message from ${topic}, partition ${partition}, offset ${offset}`);\n    } catch (error) {\n      const processingTime = Date.now() - startTime;\n      this.metricsService.recordMessageProcessed(topic, this.constructor.name, 'error');\n      this.metricsService.recordProcessingDuration(topic, this.constructor.name, processingTime / 1000);\n\n      const retryCount = this.getRetryCount(message);\n\n      if (retryCount < maxRetries) {\n        this.logger.warn(`Retrying message processing. Attempt ${retryCount + 1}/${maxRetries}`, error);\n        // Implement retry logic here (could be immediate retry or scheduled)\n        throw error; // Let Kafka handle the retry\n      } else {\n        this.logger.error(`Max retries exceeded. Sending to DLQ`, error);\n        await this.dlqService.sendToDeadLetterQueue(topic, payload, error, retryCount);\n      }\n    }\n  }\n\n  private parseEvent<T>(payload: any): T {\n    try {\n      return typeof payload === 'string' ? JSON.parse(payload) : payload;\n    } catch (error) {\n      throw new Error(`Invalid event format: ${error.message}`);\n    }\n  }\n\n  private getRetryCount(message: any): number {\n    return message.headers?.retryCount ? parseInt(message.headers.retryCount) : 0;\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"testing-strategy",children:"Testing Strategy"}),"\n",(0,a.jsx)(n.h3,{id:"1-unit-tests-for-kafka-services",children:"1. Unit Tests for Kafka Services"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// services/order-service/src/kafka/order-producer.service.spec.ts\nimport { Test, TestingModule } from '@nestjs/testing';\nimport { OrderProducerService } from './order-producer.service';\nimport { ClientKafka } from '@nestjs/microservices';\n\ndescribe('OrderProducerService', () => {\n  let service: OrderProducerService;\n  let kafkaClient: jest.Mocked<ClientKafka>;\n\n  beforeEach(async () => {\n    const mockKafkaClient = {\n      emit: jest.fn(),\n      connect: jest.fn(),\n      close: jest.fn(),\n    };\n\n    const module: TestingModule = await Test.createTestingModule({\n      providers: [\n        OrderProducerService,\n        {\n          provide: 'KAFKA_CLIENT',\n          useValue: mockKafkaClient,\n        },\n      ],\n    }).compile();\n\n    service = module.get<OrderProducerService>(OrderProducerService);\n    kafkaClient = mockKafkaClient as any;\n  });\n\n  it('should publish order created event', async () => {\n    const orderEvent = {\n      orderId: '123',\n      userId: 'user-456',\n      items: [],\n      totalAmount: 100,\n      shippingAddress: {} as any,\n    };\n\n    await service.publishOrderCreated(orderEvent);\n\n    expect(kafkaClient.emit).toHaveBeenCalledWith(\n      'order-events',\n      expect.objectContaining({\n        key: '123',\n        value: expect.stringContaining('ORDER_CREATED'),\n      }),\n    );\n  });\n});\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-integration-tests",children:"2. Integration Tests"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// test/integration/kafka-integration.spec.ts\nimport { Test } from '@nestjs/testing';\nimport { KafkaModule } from '@nestjs/microservices';\nimport { OrderProducerService } from '../../services/order-service/src/kafka/order-producer.service';\nimport { NotificationConsumerController } from '../../services/notification-service/src/kafka/notification-consumer.controller';\n\ndescribe('Kafka Integration', () => {\n  let app: any;\n  let orderProducer: OrderProducerService;\n\n  beforeAll(async () => {\n    const moduleRef = await Test.createTestingModule({\n      imports: [\n        KafkaModule.forRoot({\n          client: {\n            clientId: 'test-client',\n            brokers: ['localhost:9092'],\n          },\n        }),\n      ],\n      providers: [OrderProducerService],\n      controllers: [NotificationConsumerController],\n    }).compile();\n\n    app = moduleRef.createNestApplication();\n    await app.init();\n\n    orderProducer = app.get(OrderProducerService);\n  });\n\n  afterAll(async () => {\n    await app.close();\n  });\n\n  it('should process order created event end-to-end', async () => {\n    const orderEvent = {\n      orderId: 'test-order-123',\n      userId: 'test-user-456',\n      items: [],\n      totalAmount: 100,\n      shippingAddress: {} as any,\n    };\n\n    // Publish event\n    await orderProducer.publishOrderCreated(orderEvent);\n\n    // Wait for processing and verify notification was sent\n    // This would require setting up test infrastructure to verify the notification\n    await new Promise((resolve) => setTimeout(resolve, 2000));\n\n    // Add assertions here to verify the notification was processed\n  });\n});\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3-testcontainers-for-local-testing",children:"3. Testcontainers for Local Testing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// test/setup/kafka-testcontainer.ts\nimport { GenericContainer, StartedTestContainer } from 'testcontainers';\n\nexport class KafkaTestContainer {\n  private zookeeperContainer: StartedTestContainer;\n  private kafkaContainer: StartedTestContainer;\n\n  async start(): Promise<void> {\n    // Start Zookeeper\n    this.zookeeperContainer = await new GenericContainer('confluentinc/cp-zookeeper:7.4.0')\n      .withEnvironment({\n        ZOOKEEPER_CLIENT_PORT: '2181',\n        ZOOKEEPER_TICK_TIME: '2000',\n      })\n      .withExposedPorts(2181)\n      .start();\n\n    const zookeeperHost = this.zookeeperContainer.getHost();\n    const zookeeperPort = this.zookeeperContainer.getMappedPort(2181);\n\n    // Start Kafka\n    this.kafkaContainer = await new GenericContainer('confluentinc/cp-kafka:7.4.0')\n      .withEnvironment({\n        KAFKA_ZOOKEEPER_CONNECT: `${zookeeperHost}:${zookeeperPort}`,\n        KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://localhost:9092',\n        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: '1',\n      })\n      .withExposedPorts(9092)\n      .start();\n  }\n\n  async stop(): Promise<void> {\n    if (this.kafkaContainer) {\n      await this.kafkaContainer.stop();\n    }\n    if (this.zookeeperContainer) {\n      await this.zookeeperContainer.stop();\n    }\n  }\n\n  getKafkaBroker(): string {\n    return `${this.kafkaContainer.getHost()}:${this.kafkaContainer.getMappedPort(9092)}`;\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"deployment-scripts",children:"Deployment Scripts"}),"\n",(0,a.jsx)(n.h3,{id:"1-main-deployment-script",children:"1. Main Deployment Script"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# scripts/deploy-kafka.sh\n\nset -e\n\nPROJECT_ID="your-gcp-project"\nCLUSTER_NAME="ecommerce-cluster"\nREGION="us-central1"\nNAMESPACE_KAFKA="kafka"\nNAMESPACE_ECOMMERCE="ecommerce"\n\necho "\ud83d\ude80 Starting Kafka deployment for e-commerce platform..."\n\n# Ensure we\'re connected to the right cluster\necho "\ud83d\udce1 Connecting to GKE cluster..."\ngcloud container clusters get-credentials $CLUSTER_NAME --region $REGION --project $PROJECT_ID\n\n# Create namespaces\necho "\ud83d\udcc1 Creating namespaces..."\nkubectl create namespace $NAMESPACE_KAFKA --dry-run=client -o yaml | kubectl apply -f -\nkubectl create namespace $NAMESPACE_ECOMMERCE --dry-run=client -o yaml | kubectl apply -f -\n\n# Deploy Kafka infrastructure\necho "\ud83c\udfd7\ufe0f  Deploying Kafka infrastructure..."\n\n# Deploy Zookeeper\nkubectl apply -f k8s/kafka/zookeeper.yaml\necho "\u23f3 Waiting for Zookeeper to be ready..."\nkubectl wait --for=condition=available --timeout=300s deployment/zookeeper -n $NAMESPACE_KAFKA\n\n# Deploy Kafka\nkubectl apply -f k8s/kafka/kafka-broker.yaml\necho "\u23f3 Waiting for Kafka to be ready..."\nkubectl wait --for=condition=available --timeout=300s deployment/kafka-broker -n $NAMESPACE_KAFKA\n\n# Create topics\necho "\ud83d\udccb Creating Kafka topics..."\nkubectl apply -f k8s/kafka/topics-job.yaml\nkubectl wait --for=condition=complete --timeout=300s job/kafka-topics-setup -n $NAMESPACE_KAFKA\n\n# Deploy optional Kafka Manager\necho "\ud83d\udd27 Deploying Kafka Manager..."\nkubectl apply -f k8s/kafka/kafka-manager.yaml\n\n# Deploy monitoring\necho "\ud83d\udcca Deploying monitoring..."\nkubectl apply -f k8s/monitoring/kafka-exporter.yaml\n\n# Deploy services\necho "\ud83c\udf10 Deploying microservices..."\nkubectl apply -f k8s/services/\n\n# Wait for services to be ready\necho "\u23f3 Waiting for services to be ready..."\nkubectl wait --for=condition=available --timeout=300s deployment --all -n $NAMESPACE_ECOMMERCE\n\necho "\u2705 Kafka deployment completed successfully!"\necho ""\necho "\ud83d\udd17 Useful commands:"\necho "  kubectl get pods -n $NAMESPACE_KAFKA"\necho "  kubectl get services -n $NAMESPACE_KAFKA"\necho "  kubectl logs -f deployment/kafka-broker -n $NAMESPACE_KAFKA"\necho ""\necho "\ud83c\udf10 Access Kafka Manager:"\necho "  kubectl port-forward svc/kafka-manager-service 9000:9000 -n $NAMESPACE_KAFKA"\necho "  Then open http://localhost:9000"\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-topic-management-script",children:"2. Topic Management Script"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# scripts/manage-topics.sh\n\nKAFKA_BROKER="kafka-service.kafka.svc.cluster.local:9092"\nKUBECTL_KAFKA_POD=$(kubectl get pod -n kafka -l app=kafka-broker -o jsonpath=\'{.items[0].metadata.name}\')\n\nfunction list_topics() {\n    echo "\ud83d\udccb Listing Kafka topics..."\n    kubectl exec -n kafka $KUBECTL_KAFKA_POD -- kafka-topics --bootstrap-server $KAFKA_BROKER --list\n}\n\nfunction create_topic() {\n    local topic_name=$1\n    local partitions=${2:-3}\n    local replication_factor=${3:-1}\n\n    echo "\u2795 Creating topic: $topic_name (partitions: $partitions, replication: $replication_factor)"\n    kubectl exec -n kafka $KUBECTL_KAFKA_POD -- kafka-topics \\\n        --bootstrap-server $KAFKA_BROKER \\\n        --create \\\n        --topic $topic_name \\\n        --partitions $partitions \\\n        --replication-factor $replication_factor \\\n        --if-not-exists\n}\n\nfunction delete_topic() {\n    local topic_name=$1\n    echo "\ud83d\uddd1\ufe0f  Deleting topic: $topic_name"\n    kubectl exec -n kafka $KUBECTL_KAFKA_POD -- kafka-topics \\\n        --bootstrap-server $KAFKA_BROKER \\\n        --delete \\\n        --topic $topic_name\n}\n\nfunction describe_topic() {\n    local topic_name=$1\n    echo "\ud83d\udd0d Describing topic: $topic_name"\n    kubectl exec -n kafka $KUBECTL_KAFKA_POD -- kafka-topics \\\n        --bootstrap-server $KAFKA_BROKER \\\n        --describe \\\n        --topic $topic_name\n}\n\nfunction consumer_groups() {\n    echo "\ud83d\udc65 Listing consumer groups..."\n    kubectl exec -n kafka $KUBECTL_KAFKA_POD -- kafka-consumer-groups \\\n        --bootstrap-server $KAFKA_BROKER \\\n        --list\n}\n\nfunction group_info() {\n    local group_name=$1\n    echo "\ud83d\udcca Consumer group info: $group_name"\n    kubectl exec -n kafka $KUBECTL_KAFKA_POD -- kafka-consumer-groups \\\n        --bootstrap-server $KAFKA_BROKER \\\n        --describe \\\n        --group $group_name\n}\n\ncase "$1" in\n    list)\n        list_topics\n        ;;\n    create)\n        create_topic $2 $3 $4\n        ;;\n    delete)\n        delete_topic $2\n        ;;\n    describe)\n        describe_topic $2\n        ;;\n    groups)\n        consumer_groups\n        ;;\n    group-info)\n        group_info $2\n        ;;\n    *)\n        echo "Usage: $0 {list|create|delete|describe|groups|group-info}"\n        echo ""\n        echo "Examples:"\n        echo "  $0 list                           # List all topics"\n        echo "  $0 create my-topic 6 1           # Create topic with 6 partitions, replication factor 1"\n        echo "  $0 delete my-topic               # Delete topic"\n        echo "  $0 describe my-topic             # Describe topic"\n        echo "  $0 groups                        # List consumer groups"\n        echo "  $0 group-info my-group           # Show consumer group info"\n        exit 1\n        ;;\nesac\n'})}),"\n",(0,a.jsx)(n.h3,{id:"3-health-check-script",children:"3. Health Check Script"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# scripts/kafka-health-check.sh\n\nNAMESPACE_KAFKA="kafka"\nNAMESPACE_ECOMMERCE="ecommerce"\n\necho "\ud83c\udfe5 Kafka Health Check"\necho "===================="\n\n# Check Kafka pods\necho "\ud83d\udce6 Kafka Pods Status:"\nkubectl get pods -n $NAMESPACE_KAFKA\n\necho ""\necho "\ud83c\udf10 Kafka Services:"\nkubectl get services -n $NAMESPACE_KAFKA\n\necho ""\necho "\ud83d\udcca Kafka Topics:"\nKAFKA_POD=$(kubectl get pod -n $NAMESPACE_KAFKA -l app=kafka-broker -o jsonpath=\'{.items[0].metadata.name}\')\nif [ ! -z "$KAFKA_POD" ]; then\n    kubectl exec -n $NAMESPACE_KAFKA $KAFKA_POD -- kafka-topics --bootstrap-server kafka-service:9092 --list\nelse\n    echo "\u274c No Kafka pods found"\nfi\n\necho ""\necho "\ud83d\udc65 Consumer Groups:"\nif [ ! -z "$KAFKA_POD" ]; then\n    kubectl exec -n $NAMESPACE_KAFKA $KAFKA_POD -- kafka-consumer-groups --bootstrap-server kafka-service:9092 --list\nfi\n\necho ""\necho "\ud83d\ude80 Microservices Status:"\nkubectl get pods -n $NAMESPACE_ECOMMERCE\n\necho ""\necho "\ud83d\udd17 Service Endpoints:"\nkubectl get services -n $NAMESPACE_ECOMMERCE\n\n# Check logs for errors\necho ""\necho "\ud83d\udd0d Recent Kafka Logs (last 10 lines):"\nif [ ! -z "$KAFKA_POD" ]; then\n    kubectl logs --tail=10 -n $NAMESPACE_KAFKA $KAFKA_POD\nfi\n'})}),"\n",(0,a.jsx)(n.h2,{id:"environment-configuration",children:"Environment Configuration"}),"\n",(0,a.jsx)(n.h3,{id:"1-environment-variables",children:"1. Environment Variables"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# .env.production\nNODE_ENV=production\n\n# Kafka Configuration\nKAFKA_BROKER=kafka-service.kafka.svc.cluster.local:9092\nKAFKA_CLIENT_ID=ecommerce-service\nKAFKA_GROUP_ID=ecommerce-group\n\n# Database\nDATABASE_URL=postgresql://user:password@postgres:5432/ecommerce\n\n# Monitoring\nDATADOG_AGENT_HOST=datadog-agent.monitoring.svc.cluster.local\nDATADOG_AGENT_PORT=8125\n\n# Logging\nLOG_LEVEL=info\nLOG_FORMAT=json\n\n# Security\nJWT_SECRET=your-jwt-secret\nAPI_KEY=your-api-key\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-configmap-for-kubernetes",children:"2. ConfigMap for Kubernetes"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# k8s/config/kafka-config.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kafka-config\n  namespace: ecommerce\ndata:\n  KAFKA_BROKER: 'kafka-service.kafka.svc.cluster.local:9092'\n  KAFKA_CLIENT_ID: 'ecommerce-service'\n  LOG_LEVEL: 'info'\n  LOG_FORMAT: 'json'\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: kafka-secrets\n  namespace: ecommerce\ntype: Opaque\nstringData:\n  DATABASE_URL: 'postgresql://user:password@postgres:5432/ecommerce'\n  JWT_SECRET: 'your-jwt-secret'\n  API_KEY: 'your-api-key'\n"})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices-and-recommendations",children:"Best Practices and Recommendations"}),"\n",(0,a.jsx)(n.h3,{id:"1-message-design",children:"1. Message Design"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Use versioned events for backward compatibility"}),"\n",(0,a.jsx)(n.li,{children:"Include correlation IDs for tracing"}),"\n",(0,a.jsx)(n.li,{children:"Keep messages small and focused"}),"\n",(0,a.jsx)(n.li,{children:"Use proper partitioning strategies"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-consumer-design",children:"2. Consumer Design"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Implement idempotent consumers"}),"\n",(0,a.jsx)(n.li,{children:"Use appropriate consumer group configurations"}),"\n",(0,a.jsx)(n.li,{children:"Handle message ordering requirements"}),"\n",(0,a.jsx)(n.li,{children:"Implement proper error handling and retries"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-producer-design",children:"3. Producer Design"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Use async producers for better performance"}),"\n",(0,a.jsx)(n.li,{children:"Implement proper serialization"}),"\n",(0,a.jsx)(n.li,{children:"Handle acknowledgment strategies appropriately"}),"\n",(0,a.jsx)(n.li,{children:"Monitor producer metrics"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"4-operations",children:"4. Operations"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Monitor lag and throughput"}),"\n",(0,a.jsx)(n.li,{children:"Set up proper alerting"}),"\n",(0,a.jsx)(n.li,{children:"Implement log retention policies"}),"\n",(0,a.jsx)(n.li,{children:"Regular backup and disaster recovery testing"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"5-security",children:"5. Security"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Enable SASL/SSL for production"}),"\n",(0,a.jsx)(n.li,{children:"Implement proper access controls"}),"\n",(0,a.jsx)(n.li,{children:"Use secrets management for credentials"}),"\n",(0,a.jsx)(n.li,{children:"Regular security audits"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"performance-optimization-guidelines",children:"Performance Optimization Guidelines"}),"\n",(0,a.jsx)(n.h3,{id:"1-producer-optimization",children:"1. Producer Optimization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// Producer configuration for high throughput\nconst producerConfig = {\n  // Batching settings\n  batchSize: 16384, // 16KB\n  linger: 5, // Wait up to 5ms to batch messages\n\n  // Compression\n  compression: 'snappy', // or 'gzip', 'lz4'\n\n  // Memory management\n  bufferMemory: 33554432, // 32MB\n\n  // Acknowledgment settings\n  acks: 1, // Leader acknowledgment (balance between performance and durability)\n\n  // Retry settings\n  retries: 2147483647, // Max int value\n  retryBackoffMs: 100,\n\n  // Timeout settings\n  requestTimeoutMs: 30000,\n  deliveryTimeoutMs: 120000,\n};\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-consumer-optimization",children:"2. Consumer Optimization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// Consumer configuration for optimal performance\nconst consumerConfig = {\n  // Fetch settings\n  fetchMinBytes: 1024, // 1KB minimum\n  fetchMaxWait: 500, // Max wait time in ms\n  fetchMaxBytes: 52428800, // 50MB max\n\n  // Processing settings\n  maxPollRecords: 500, // Records per poll\n  maxPollInterval: 300000, // 5 minutes\n\n  // Session settings\n  sessionTimeout: 30000, // 30 seconds\n  heartbeatInterval: 3000, // 3 seconds\n\n  // Auto-commit settings\n  enableAutoCommit: false, // Manual commit for better control\n  autoCommitInterval: 5000,\n};\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3-partitioning-strategy",children:"3. Partitioning Strategy"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// Custom partitioner for optimal distribution\nexport class EcommercePartitioner {\n  partition(topic: string, key: string, keyBytes: Buffer, value: any): number {\n    const partitions = this.getPartitionCount(topic);\n\n    switch (topic) {\n      case 'order-events':\n        // Partition by user ID for order locality\n        return this.hashUserId(value.userId) % partitions;\n\n      case 'product-events':\n        // Partition by product category\n        return this.hashCategory(value.category) % partitions;\n\n      case 'user-events':\n        // Even distribution for user events\n        return this.hash(key) % partitions;\n\n      default:\n        return this.hash(key) % partitions;\n    }\n  }\n\n  private hashUserId(userId: string): number {\n    // Consistent hashing implementation\n    return this.murmurHash(userId);\n  }\n\n  private hashCategory(category: string): number {\n    return this.murmurHash(category);\n  }\n\n  private hash(key: string): number {\n    return this.murmurHash(key);\n  }\n\n  private murmurHash(key: string): number {\n    // MurmurHash3 implementation for consistent partitioning\n    // Implementation details...\n    return 0;\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"security-implementation",children:"Security Implementation"}),"\n",(0,a.jsx)(n.h3,{id:"1-saslssl-configuration",children:"1. SASL/SSL Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# k8s/kafka/kafka-security.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: kafka-ssl-secrets\n  namespace: kafka\ntype: Opaque\ndata:\n  keystore.jks: <base64-encoded-keystore>\n  truststore.jks: <base64-encoded-truststore>\n  keystore-password: <base64-encoded-password>\n  truststore-password: <base64-encoded-password>\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-broker-secure\n  namespace: kafka\nspec:\n  template:\n    spec:\n      containers:\n        - name: kafka-broker\n          image: confluentinc/cp-kafka:7.4.0\n          env:\n            # SSL Configuration\n            - name: KAFKA_SSL_KEYSTORE_LOCATION\n              value: '/etc/kafka/secrets/keystore.jks'\n            - name: KAFKA_SSL_KEYSTORE_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: kafka-ssl-secrets\n                  key: keystore-password\n            - name: KAFKA_SSL_TRUSTSTORE_LOCATION\n              value: '/etc/kafka/secrets/truststore.jks'\n            - name: KAFKA_SSL_TRUSTSTORE_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: kafka-ssl-secrets\n                  key: truststore-password\n\n            # SASL Configuration\n            - name: KAFKA_SASL_ENABLED_MECHANISMS\n              value: 'PLAIN,SCRAM-SHA-256'\n            - name: KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL\n              value: 'SCRAM-SHA-256'\n\n            # Listener Configuration\n            - name: KAFKA_LISTENERS\n              value: 'SASL_SSL://0.0.0.0:9092'\n            - name: KAFKA_ADVERTISED_LISTENERS\n              value: 'SASL_SSL://kafka-service:9092'\n            - name: KAFKA_SECURITY_INTER_BROKER_PROTOCOL\n              value: 'SASL_SSL'\n\n          volumeMounts:\n            - name: kafka-ssl-secrets\n              mountPath: /etc/kafka/secrets\n              readOnly: true\n      volumes:\n        - name: kafka-ssl-secrets\n          secret:\n            secretName: kafka-ssl-secrets\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-access-control-lists-acls",children:"2. Access Control Lists (ACLs)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"#!/bin/bash\n# scripts/setup-kafka-acls.sh\n\nKAFKA_BROKER=\"kafka-service:9092\"\nKAFKA_POD=$(kubectl get pod -n kafka -l app=kafka-broker -o jsonpath='{.items[0].metadata.name}')\n\n# Create service users\nkubectl exec -n kafka $KAFKA_POD -- kafka-configs \\\n  --bootstrap-server $KAFKA_BROKER \\\n  --alter \\\n  --add-config 'SCRAM-SHA-256=[password=orderservice123]' \\\n  --entity-type users \\\n  --entity-name order-service\n\nkubectl exec -n kafka $KAFKA_POD -- kafka-configs \\\n  --bootstrap-server $KAFKA_BROKER \\\n  --alter \\\n  --add-config 'SCRAM-SHA-256=[password=paymentservice123]' \\\n  --entity-type users \\\n  --entity-name payment-service\n\n# Set up ACLs for order service\nkubectl exec -n kafka $KAFKA_POD -- kafka-acls \\\n  --bootstrap-server $KAFKA_BROKER \\\n  --add \\\n  --allow-principal User:order-service \\\n  --operation Write \\\n  --topic order-events\n\nkubectl exec -n kafka $KAFKA_POD -- kafka-acls \\\n  --bootstrap-server $KAFKA_BROKER \\\n  --add \\\n  --allow-principal User:order-service \\\n  --operation Read \\\n  --topic payment-events \\\n  --group order-service-group\n\n# Set up ACLs for payment service\nkubectl exec -n kafka $KAFKA_POD -- kafka-acls \\\n  --bootstrap-server $KAFKA_BROKER \\\n  --add \\\n  --allow-principal User:payment-service \\\n  --operation Write \\\n  --topic payment-events\n\nkubectl exec -n kafka $KAFKA_POD -- kafka-acls \\\n  --bootstrap-server $KAFKA_BROKER \\\n  --add \\\n  --allow-principal User:payment-service \\\n  --operation Read \\\n  --topic order-events \\\n  --group payment-service-group\n"})}),"\n",(0,a.jsx)(n.h2,{id:"data-governance-and-compliance",children:"Data Governance and Compliance"}),"\n",(0,a.jsx)(n.h3,{id:"1-schema-registry-implementation",children:"1. Schema Registry Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// shared/schema/src/schema-registry.service.ts\nimport { Injectable } from '@nestjs/common';\nimport { SchemaRegistry } from '@kafkajs/confluent-schema-registry';\n\n@Injectable()\nexport class SchemaRegistryService {\n  private registry: SchemaRegistry;\n\n  constructor() {\n    this.registry = new SchemaRegistry({\n      host: process.env.SCHEMA_REGISTRY_URL || 'http://schema-registry:8081',\n    });\n  }\n\n  async registerSchema(subject: string, schema: any): Promise<number> {\n    return await this.registry.register(subject, schema);\n  }\n\n  async getSchema(id: number): Promise<any> {\n    return await this.registry.getSchema(id);\n  }\n\n  async getLatestSchema(subject: string): Promise<any> {\n    return await this.registry.getLatestSchema(subject);\n  }\n\n  async encode(id: number, payload: any): Promise<Buffer> {\n    return await this.registry.encode(id, payload);\n  }\n\n  async decode(buffer: Buffer): Promise<any> {\n    return await this.registry.decode(buffer);\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-gdpr-compliance-implementation",children:"2. GDPR Compliance Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// shared/compliance/src/gdpr-handler.service.ts\nimport { Injectable } from '@nestjs/common';\nimport { KafkaProducerService } from '@ecommerce/kafka';\n\n@Injectable()\nexport class GDPRHandlerService extends KafkaProducerService {\n  async handleDataDeletionRequest(userId: string): Promise<void> {\n    // Publish data deletion event\n    await this.publishEvent('user-events', {\n      id: crypto.randomUUID(),\n      type: 'USER_DATA_DELETION_REQUESTED',\n      aggregateId: userId,\n      version: 1,\n      timestamp: new Date(),\n      userId,\n      reason: 'GDPR_COMPLIANCE',\n    });\n  }\n\n  async handleDataExportRequest(userId: string): Promise<void> {\n    // Publish data export event\n    await this.publishEvent('user-events', {\n      id: crypto.randomUUID(),\n      type: 'USER_DATA_EXPORT_REQUESTED',\n      aggregateId: userId,\n      version: 1,\n      timestamp: new Date(),\n      userId,\n      reason: 'GDPR_COMPLIANCE',\n    });\n  }\n\n  async anonymizeUserData(userId: string): Promise<void> {\n    // Publish anonymization event\n    await this.publishEvent('user-events', {\n      id: crypto.randomUUID(),\n      type: 'USER_DATA_ANONYMIZED',\n      aggregateId: userId,\n      version: 1,\n      timestamp: new Date(),\n      userId,\n      anonymizedAt: new Date(),\n    });\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"disaster-recovery-and-business-continuity",children:"Disaster Recovery and Business Continuity"}),"\n",(0,a.jsx)(n.h3,{id:"1-cross-region-replication",children:"1. Cross-Region Replication"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# k8s/kafka/kafka-mirror-maker.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-mirror-maker\n  namespace: kafka\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: kafka-mirror-maker\n  template:\n    metadata:\n      labels:\n        app: kafka-mirror-maker\n    spec:\n      containers:\n        - name: mirror-maker\n          image: confluentinc/cp-kafka:7.4.0\n          command:\n            - kafka-mirror-maker\n            - --consumer.config=/etc/kafka/consumer.properties\n            - --producer.config=/etc/kafka/producer.properties\n            - --whitelist=user-events,order-events,payment-events,product-events\n          volumeMounts:\n            - name: mirror-maker-config\n              mountPath: /etc/kafka\n          resources:\n            requests:\n              memory: '512Mi'\n              cpu: '250m'\n            limits:\n              memory: '1Gi'\n              cpu: '500m'\n      volumes:\n        - name: mirror-maker-config\n          configMap:\n            name: mirror-maker-config\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mirror-maker-config\n  namespace: kafka\ndata:\n  consumer.properties: |\n    bootstrap.servers=kafka-source:9092\n    group.id=mirror-maker-consumer\n    auto.offset.reset=earliest\n    enable.auto.commit=false\n  producer.properties: |\n    bootstrap.servers=kafka-destination:9092\n    acks=all\n    retries=2147483647\n    batch.size=16384\n    linger.ms=5\n    buffer.memory=33554432\n    compression.type=snappy\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-backup-and-recovery-scripts",children:"2. Backup and Recovery Scripts"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# scripts/backup-kafka-topics.sh\n\nKAFKA_BROKER="kafka-service:9092"\nBACKUP_DIR="/tmp/kafka-backup-$(date +%Y%m%d-%H%M%S)"\nKAFKA_POD=$(kubectl get pod -n kafka -l app=kafka-broker -o jsonpath=\'{.items[0].metadata.name}\')\n\nmkdir -p $BACKUP_DIR\n\necho "\ud83d\udce6 Starting Kafka backup process..."\n\n# Get list of topics\nTOPICS=$(kubectl exec -n kafka $KAFKA_POD -- kafka-topics --bootstrap-server $KAFKA_BROKER --list)\n\nfor topic in $TOPICS; do\n    echo "\ud83d\udcbe Backing up topic: $topic"\n\n    # Export topic data\n    kubectl exec -n kafka $KAFKA_POD -- kafka-console-consumer \\\n        --bootstrap-server $KAFKA_BROKER \\\n        --topic $topic \\\n        --from-beginning \\\n        --timeout-ms 10000 > "$BACKUP_DIR/$topic.json" 2>/dev/null\n\n    # Export topic configuration\n    kubectl exec -n kafka $KAFKA_POD -- kafka-configs \\\n        --bootstrap-server $KAFKA_BROKER \\\n        --describe \\\n        --entity-type topics \\\n        --entity-name $topic > "$BACKUP_DIR/$topic.config"\n\n    echo "\u2705 Backed up topic: $topic"\ndone\n\n# Create archive\ntar -czf "kafka-backup-$(date +%Y%m%d-%H%M%S).tar.gz" -C $(dirname $BACKUP_DIR) $(basename $BACKUP_DIR)\n\necho "\ud83c\udf89 Backup completed: kafka-backup-$(date +%Y%m%d-%H%M%S).tar.gz"\n'})}),"\n",(0,a.jsx)(n.h2,{id:"cicd-integration-with-github-actions",children:"CI/CD Integration with GitHub Actions"}),"\n",(0,a.jsx)(n.h3,{id:"1-kafka-testing-pipeline",children:"1. Kafka Testing Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# .github/workflows/kafka-tests.yml\nname: Kafka Integration Tests\n\non:\n  push:\n    branches: [main, develop]\n  pull_request:\n    branches: [main]\n\njobs:\n  kafka-integration-tests:\n    runs-on: ubuntu-latest\n\n    services:\n      zookeeper:\n        image: confluentinc/cp-zookeeper:7.4.0\n        env:\n          ZOOKEEPER_CLIENT_PORT: 2181\n          ZOOKEEPER_TICK_TIME: 2000\n        ports:\n          - 2181:2181\n\n      kafka:\n        image: confluentinc/cp-kafka:7.4.0\n        env:\n          KAFKA_ZOOKEEPER_CONNECT: localhost:2181\n          KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n        ports:\n          - 9092:9092\n        options: >-\n          --health-cmd "kafka-topics --bootstrap-server localhost:9092 --list"\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v3\n        with:\n          node-version: \'18\'\n          cache: \'npm\'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Wait for Kafka\n        run: |\n          for i in {1..30}; do\n            if docker exec $(docker ps -q --filter "ancestor=confluentinc/cp-kafka:7.4.0") kafka-topics --bootstrap-server localhost:9092 --list; then\n              break\n            fi\n            sleep 2\n          done\n\n      - name: Create test topics\n        run: |\n          docker exec $(docker ps -q --filter "ancestor=confluentinc/cp-kafka:7.4.0") kafka-topics --bootstrap-server localhost:9092 --create --topic test-events --partitions 3 --replication-factor 1\n\n      - name: Run Kafka integration tests\n        run: npm run test:kafka\n        env:\n          KAFKA_BROKER: localhost:9092\n          NODE_ENV: test\n\n      - name: Upload test results\n        uses: actions/upload-artifact@v3\n        if: always()\n        with:\n          name: kafka-test-results\n          path: test-results/\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-argocd-deployment-configuration",children:"2. ArgoCD Deployment Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# argocd/kafka-application.yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: kafka-infrastructure\n  namespace: argocd\nspec:\n  project: ecommerce\n  source:\n    repoURL: https://github.com/your-org/ecommerce-infrastructure\n    targetRevision: main\n    path: k8s/kafka\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: kafka\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\n    retry:\n      limit: 5\n      backoff:\n        duration: 5s\n        factor: 2\n        maxDuration: 3m\n---\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: ecommerce-services\n  namespace: argocd\nspec:\n  project: ecommerce\n  source:\n    repoURL: https://github.com/your-org/ecommerce-services\n    targetRevision: main\n    path: k8s/services\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: ecommerce\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\n  dependsOn:\n    - name: kafka-infrastructure\n"})}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting-guide",children:"Troubleshooting Guide"}),"\n",(0,a.jsx)(n.h3,{id:"1-common-issues-and-solutions",children:"1. Common Issues and Solutions"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// shared/diagnostics/src/kafka-diagnostics.service.ts\nimport { Injectable, Logger } from '@nestjs/common';\nimport { KafkaMetricsService } from '@ecommerce/monitoring';\n\n@Injectable()\nexport class KafkaDiagnosticsService {\n  private readonly logger = new Logger(KafkaDiagnosticsService.name);\n\n  constructor(private readonly metricsService: KafkaMetricsService) {}\n\n  async diagnoseProducerIssues(): Promise<DiagnosticReport> {\n    const report: DiagnosticReport = {\n      timestamp: new Date(),\n      status: 'healthy',\n      issues: [],\n      recommendations: [],\n    };\n\n    // Check producer metrics\n    const errorRate = await this.getProducerErrorRate();\n    if (errorRate > 0.05) {\n      // 5% error rate threshold\n      report.issues.push({\n        severity: 'high',\n        description: `Producer error rate is ${(errorRate * 100).toFixed(2)}%`,\n        possibleCauses: [\n          'Network connectivity issues',\n          'Kafka broker unavailable',\n          'Authentication failures',\n          'Topic configuration issues',\n        ],\n        solutions: [\n          'Check network connectivity to Kafka brokers',\n          'Verify Kafka broker health',\n          'Check authentication credentials',\n          'Review topic configurations and ACLs',\n        ],\n      });\n    }\n\n    // Check message throughput\n    const throughput = await this.getMessageThroughput();\n    if (throughput < 100) {\n      // Messages per second threshold\n      report.issues.push({\n        severity: 'medium',\n        description: `Low message throughput: ${throughput} msgs/sec`,\n        possibleCauses: [\n          'Small batch sizes',\n          'High linger.ms setting',\n          'Insufficient producer instances',\n          'Broker performance issues',\n        ],\n        solutions: [\n          'Increase batch.size configuration',\n          'Optimize linger.ms setting',\n          'Scale producer instances',\n          'Monitor broker performance',\n        ],\n      });\n    }\n\n    return report;\n  }\n\n  async diagnoseConsumerIssues(): Promise<DiagnosticReport> {\n    const report: DiagnosticReport = {\n      timestamp: new Date(),\n      status: 'healthy',\n      issues: [],\n      recommendations: [],\n    };\n\n    // Check consumer lag\n    const consumerLag = await this.getConsumerLag();\n    if (consumerLag > 10000) {\n      // 10k messages lag threshold\n      report.issues.push({\n        severity: 'high',\n        description: `High consumer lag: ${consumerLag} messages`,\n        possibleCauses: [\n          'Slow message processing',\n          'Insufficient consumer instances',\n          'Consumer rebalancing issues',\n          'Processing errors causing retries',\n        ],\n        solutions: [\n          'Optimize message processing logic',\n          'Scale consumer instances',\n          'Review consumer group configuration',\n          'Implement circuit breakers for external calls',\n        ],\n      });\n    }\n\n    return report;\n  }\n\n  private async getProducerErrorRate(): Promise<number> {\n    // Implementation to get producer error rate from metrics\n    return 0.02; // Example: 2% error rate\n  }\n\n  private async getMessageThroughput(): Promise<number> {\n    // Implementation to get message throughput\n    return 500; // Example: 500 messages per second\n  }\n\n  private async getConsumerLag(): Promise<number> {\n    // Implementation to get consumer lag\n    return 5000; // Example: 5k messages lag\n  }\n}\n\ninterface DiagnosticReport {\n  timestamp: Date;\n  status: 'healthy' | 'warning' | 'critical';\n  issues: DiagnosticIssue[];\n  recommendations: string[];\n}\n\ninterface DiagnosticIssue {\n  severity: 'low' | 'medium' | 'high';\n  description: string;\n  possibleCauses: string[];\n  solutions: string[];\n}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-debug-commands-and-tools",children:"2. Debug Commands and Tools"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# scripts/kafka-debug.sh\n\nKAFKA_BROKER="kafka-service.kafka.svc.cluster.local:9092"\nKAFKA_POD=$(kubectl get pod -n kafka -l app=kafka-broker -o jsonpath=\'{.items[0].metadata.name}\')\n\nfunction debug_consumer_groups() {\n    echo "\ud83d\udd0d Consumer Groups Debug Information"\n    echo "=================================="\n\n    # List all consumer groups\n    echo "\ud83d\udccb Consumer Groups:"\n    kubectl exec -n kafka $KAFKA_POD -- kafka-consumer-groups \\\n        --bootstrap-server $KAFKA_BROKER --list\n\n    echo ""\n\n    # Show detailed info for each group\n    GROUPS=$(kubectl exec -n kafka $KAFKA_POD -- kafka-consumer-groups \\\n        --bootstrap-server $KAFKA_BROKER --list)\n\n    for group in $GROUPS; do\n        echo "\ud83d\udc65 Group: $group"\n        kubectl exec -n kafka $KAFKA_POD -- kafka-consumer-groups \\\n            --bootstrap-server $KAFKA_BROKER \\\n            --describe --group $group\n        echo ""\n    done\n}\n\nfunction debug_topic_details() {\n    local topic=$1\n    echo "\ud83d\udd0d Topic Debug Information: $topic"\n    echo "=================================="\n\n    # Topic configuration\n    echo "\u2699\ufe0f  Topic Configuration:"\n    kubectl exec -n kafka $KAFKA_POD -- kafka-configs \\\n        --bootstrap-server $KAFKA_BROKER \\\n        --describe --entity-type topics --entity-name $topic\n\n    echo ""\n\n    # Topic details\n    echo "\ud83d\udcca Topic Details:"\n    kubectl exec -n kafka $KAFKA_POD -- kafka-topics \\\n        --bootstrap-server $KAFKA_BROKER \\\n        --describe --topic $topic\n\n    echo ""\n\n    # Log segments\n    echo "\ud83d\udcc1 Log Segments:"\n    kubectl exec -n kafka $KAFKA_POD -- kafka-log-dirs \\\n        --bootstrap-server $KAFKA_BROKER \\\n        --describe --json | jq ".brokers[].logDirs[].partitions | to_entries[] | select(.key | contains(\\"$topic\\"))"\n}\n\nfunction debug_broker_health() {\n    echo "\ud83c\udfe5 Broker Health Check"\n    echo "====================="\n\n    # Broker metadata\n    echo "\ud83d\udda5\ufe0f  Broker Metadata:"\n    kubectl exec -n kafka $KAFKA_POD -- kafka-broker-api-versions \\\n        --bootstrap-server $KAFKA_BROKER\n\n    echo ""\n\n    # Cluster metadata\n    echo "\ud83c\udf10 Cluster Metadata:"\n    kubectl exec -n kafka $KAFKA_POD -- kafka-metadata-shell \\\n        --snapshot /var/lib/kafka/data/__cluster_metadata-0/00000000000000000000.log\n}\n\nfunction monitor_messages() {\n    local topic=$1\n    local max_messages=${2:-10}\n\n    echo "\ud83d\udce8 Monitoring messages on topic: $topic"\n    echo "======================================="\n\n    kubectl exec -n kafka $KAFKA_POD -- kafka-console-consumer \\\n        --bootstrap-server $KAFKA_BROKER \\\n        --topic $topic \\\n        --from-beginning \\\n        --max-messages $max_messages \\\n        --property print.timestamp=true \\\n        --property print.key=true \\\n        --property print.headers=true\n}\n\n# Main script logic\ncase "$1" in\n    consumer-groups)\n        debug_consumer_groups\n        ;;\n    topic)\n        debug_topic_details $2\n        ;;\n    broker-health)\n        debug_broker_health\n        ;;\n    monitor)\n        monitor_messages $2 $3\n        ;;\n    *)\n        echo "Usage: $0 {consumer-groups|topic|broker-health|monitor}"\n        echo ""\n        echo "Examples:"\n        echo "  $0 consumer-groups                    # Debug all consumer groups"\n        echo "  $0 topic order-events                # Debug specific topic"\n        echo "  $0 broker-health                     # Check broker health"\n        echo "  $0 monitor order-events 20           # Monitor 20 messages from topic"\n        exit 1\n        ;;\nesac\n'})}),"\n",(0,a.jsx)(n.p,{children:"This comprehensive guide provides everything you need to implement Kafka in your e-commerce microservices architecture. The implementation is production-ready and follows best practices for scalability, reliability, and maintainability."})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}}}]);