"use strict";(self.webpackChunkdocs_site=self.webpackChunkdocs_site||[]).push([[3660],{2038:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>i,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"rag-recommendations/generation-engine","title":"Generation and Personalization Engine","description":"This section covers the LLM-powered generation component that creates personalized product recommendations, explanations, and content. The generation engine transforms retrieved products into compelling, contextual recommendations tailored to each user.","source":"@site/docs/rag-recommendations/generation-engine.md","sourceDirName":"rag-recommendations","slug":"/rag-recommendations/generation-engine","permalink":"/fullstack-dev/docs/rag-recommendations/generation-engine","draft":false,"unlisted":false,"editUrl":"https://github.com/tamnk74/fullstack-dev/tree/main/docs-site/docs/rag-recommendations/generation-engine.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Retrieval System Implementation","permalink":"/fullstack-dev/docs/rag-recommendations/retrieval-system"},"next":{"title":"Build a web application","permalink":"/fullstack-dev/docs/backend/"}}');var o=t(5813),s=t(5741);const i={},a="Generation and Personalization Engine",c={},l=[{value:"Generation Architecture",id:"generation-architecture",level:2},{value:"LLM Integration Framework",id:"llm-integration-framework",level:2},{value:"Multi-Provider LLM Client",id:"multi-provider-llm-client",level:3},{value:"Prompt Engineering Framework",id:"prompt-engineering-framework",level:3},{value:"Complete Vertex AI Setup Example",id:"complete-vertex-ai-setup-example",level:2},{value:"Environment Setup",id:"environment-setup",level:3},{value:"Complete Implementation Example",id:"complete-implementation-example",level:3},{value:"Production Deployment Configuration",id:"production-deployment-configuration",level:3},{value:"Cost Optimization and Monitoring",id:"cost-optimization-and-monitoring",level:3},{value:"Content Quality Control",id:"content-quality-control",level:3},{value:"API Integration",id:"api-integration",level:3},{value:"Integration with Complete RAG System",id:"integration-with-complete-rag-system",level:2},{value:"End-to-End Pipeline",id:"end-to-end-pipeline",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"generation-and-personalization-engine",children:"Generation and Personalization Engine"})}),"\n",(0,o.jsx)(n.p,{children:"This section covers the LLM-powered generation component that creates personalized product recommendations, explanations, and content. The generation engine transforms retrieved products into compelling, contextual recommendations tailored to each user."}),"\n",(0,o.jsx)(n.h2,{id:"generation-architecture",children:"Generation Architecture"}),"\n",(0,o.jsx)(n.p,{children:"The generation system combines retrieval results with user context to create personalized content:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Retrieved     \u2502    \u2502   Generation    \u2502    \u2502   Personalized  \u2502\n\u2502   Products      \u2502\u2500\u2500\u2500\u25b6\u2502   Engine        \u2502\u2500\u2500\u2500\u25b6\u2502   Content       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502                      \u2502                      \u2502                 \u2502\n\u251c\u2500 Product Data        \u251c\u2500 Context Fusion     \u251c\u2500 Recommendations \u2502\n\u251c\u2500 User Context        \u251c\u2500 LLM Generation     \u251c\u2500 Explanations    \u2502\n\u251c\u2500 Search Query        \u251c\u2500 Template Engine    \u251c\u2500 Product Summaries\u2502\n\u251c\u2500 Session History     \u251c\u2500 Content Filtering  \u251c\u2500 Comparison Text \u2502\n\u2514\u2500 Behavioral Data     \u2514\u2500 Quality Control    \u2514\u2500 Purchase Advice \u2502\n"})}),"\n",(0,o.jsx)(n.h2,{id:"llm-integration-framework",children:"LLM Integration Framework"}),"\n",(0,o.jsx)(n.h3,{id:"multi-provider-llm-client",children:"Multi-Provider LLM Client"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# generation/llm_client.py\nimport openai\nimport anthropic\nfrom typing import Dict, Any, List, Optional, Union\nimport logging\nimport asyncio\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass GenerationRequest:\n    """Request structure for content generation"""\n    prompt: str\n    max_tokens: int = 1000\n    temperature: float = 0.7\n    user_context: Optional[Dict[str, Any]] = None\n    format_type: str = "text"  # text, json, markdown\n\n@dataclass\nclass GenerationResponse:\n    """Response structure for generated content"""\n    content: str\n    model_used: str\n    tokens_used: int\n    generation_time: float\n    quality_score: Optional[float] = None\n\nclass BaseLLMProvider(ABC):\n    """Abstract base class for LLM providers"""\n    \n    @abstractmethod\n    async def generate(self, request: GenerationRequest) -> GenerationResponse:\n        pass\n    \n    @abstractmethod\n    def is_available(self) -> bool:\n        pass\n\nclass OpenAIProvider(BaseLLMProvider):\n    """OpenAI GPT provider"""\n    \n    def __init__(self, api_key: str, model: str = "gpt-4"):\n        self.client = openai.AsyncOpenAI(api_key=api_key)\n        self.model = model\n        self.available = True\n    \n    async def generate(self, request: GenerationRequest) -> GenerationResponse:\n        """Generate content using OpenAI API"""\n        try:\n            import time\n            start_time = time.time()\n            \n            messages = [\n                {"role": "system", "content": self._create_system_prompt(request)},\n                {"role": "user", "content": request.prompt}\n            ]\n            \n            response = await self.client.chat.completions.create(\n                model=self.model,\n                messages=messages,\n                max_tokens=request.max_tokens,\n                temperature=request.temperature\n            )\n            \n            generation_time = time.time() - start_time\n            \n            return GenerationResponse(\n                content=response.choices[0].message.content,\n                model_used=self.model,\n                tokens_used=response.usage.total_tokens,\n                generation_time=generation_time\n            )\n            \n        except Exception as e:\n            logger.error(f"OpenAI generation failed: {e}")\n            self.available = False\n            raise\n    \n    def is_available(self) -> bool:\n        return self.available\n    \n    def _create_system_prompt(self, request: GenerationRequest) -> str:\n        """Create system prompt based on context"""\n        base_prompt = """You are an expert e-commerce recommendation assistant. Your task is to create personalized, helpful product recommendations and explanations."""\n        \n        if request.user_context:\n            context_info = []\n            if request.user_context.get(\'preferred_categories\'):\n                context_info.append(f"User prefers: {\', \'.join(request.user_context[\'preferred_categories\'])}")\n            if request.user_context.get(\'budget_range\'):\n                context_info.append(f"Budget: ${request.user_context[\'budget_range\']}")\n            \n            if context_info:\n                base_prompt += f"\\n\\nUser Context: {\'; \'.join(context_info)}"\n        \n        if request.format_type == "json":\n            base_prompt += "\\n\\nPlease respond in valid JSON format."\n        elif request.format_type == "markdown":\n            base_prompt += "\\n\\nPlease format your response using Markdown."\n        \n        return base_prompt\n\nclass AnthropicProvider(BaseLLMProvider):\n    """Anthropic Claude provider"""\n    \n    def __init__(self, api_key: str, model: str = "claude-3-sonnet-20240229"):\n        self.client = anthropic.AsyncAnthropic(api_key=api_key)\n        self.model = model\n        self.available = True\n    \n    async def generate(self, request: GenerationRequest) -> GenerationResponse:\n        """Generate content using Anthropic API"""\n        try:\n            import time\n            start_time = time.time()\n            \n            system_prompt = self._create_system_prompt(request)\n            \n            response = await self.client.messages.create(\n                model=self.model,\n                max_tokens=request.max_tokens,\n                temperature=request.temperature,\n                system=system_prompt,\n                messages=[{"role": "user", "content": request.prompt}]\n            )\n            \n            generation_time = time.time() - start_time\n            \n            return GenerationResponse(\n                content=response.content[0].text,\n                model_used=self.model,\n                tokens_used=response.usage.input_tokens + response.usage.output_tokens,\n                generation_time=generation_time\n            )\n            \n        except Exception as e:\n            logger.error(f"Anthropic generation failed: {e}")\n            self.available = False\n            raise\n    \n    def is_available(self) -> bool:\n        return self.available\n    \n    def _create_system_prompt(self, request: GenerationRequest) -> str:\n        """Create system prompt for Claude"""\n        return """You are Claude, an AI assistant specialized in e-commerce recommendations. \n        You provide thoughtful, accurate product recommendations with clear explanations. \n        Always be helpful, honest, and consider the user\'s specific needs and preferences."""\n\nclass VertexAIProvider(BaseLLMProvider):\n    """Google Vertex AI Gemini provider"""\n    \n    def __init__(self, project_id: str, location: str = "us-central1", model: str = "gemini-1.5-pro"):\n        try:\n            import vertexai\n            from vertexai.generative_models import GenerativeModel, SafetySetting, HarmCategory\n            \n            # Initialize Vertex AI\n            vertexai.init(project=project_id, location=location)\n            \n            # Safety settings for production use\n            self.safety_settings = [\n                SafetySetting(\n                    category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n                    threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                ),\n                SafetySetting(\n                    category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n                    threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                ),\n                SafetySetting(\n                    category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n                    threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                ),\n                SafetySetting(\n                    category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n                    threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                ),\n            ]\n            \n            # Generation config for consistent outputs\n            self.generation_config = {\n                "max_output_tokens": 2048,\n                "temperature": 0.7,\n                "top_p": 0.95,\n                "top_k": 40,\n            }\n            \n            self.model = GenerativeModel(model)\n            self.project_id = project_id\n            self.location = location\n            self.model_name = model\n            self.available = True\n            \n            logger.info(f"Initialized Vertex AI Gemini model: {model}")\n            \n        except ImportError:\n            logger.error("Vertex AI SDK not installed. Install with: pip install google-cloud-aiplatform")\n            self.available = False\n            raise\n        except Exception as e:\n            logger.error(f"Failed to initialize Vertex AI: {e}")\n            self.available = False\n            raise\n    \n    async def generate(self, request: GenerationRequest) -> GenerationResponse:\n        """Generate content using Vertex AI Gemini"""\n        try:\n            import time\n            start_time = time.time()\n            \n            # Create system instruction\n            system_instruction = self._create_system_instruction(request)\n            \n            # Update generation config with request parameters\n            generation_config = self.generation_config.copy()\n            generation_config.update({\n                "max_output_tokens": min(request.max_tokens, 2048),\n                "temperature": request.temperature,\n            })\n            \n            # Create model instance with system instruction\n            model_with_system = GenerativeModel(\n                self.model_name,\n                system_instruction=[system_instruction],\n                generation_config=generation_config,\n                safety_settings=self.safety_settings\n            )\n            \n            # Generate content\n            response = model_with_system.generate_content(\n                request.prompt,\n                stream=False\n            )\n            \n            generation_time = time.time() - start_time\n            \n            # Extract text from response\n            if response.text:\n                content = response.text\n            else:\n                # Handle safety filter or other issues\n                content = "I apologize, but I couldn\'t generate a response for that request. Please try rephrasing your query."\n                logger.warning("Gemini response was filtered or empty")\n            \n            # Estimate token usage (Gemini doesn\'t provide exact counts)\n            estimated_tokens = len(request.prompt.split()) + len(content.split())\n            \n            return GenerationResponse(\n                content=content,\n                model_used=self.model_name,\n                tokens_used=estimated_tokens,\n                generation_time=generation_time\n            )\n            \n        except Exception as e:\n            logger.error(f"Vertex AI Gemini generation failed: {e}")\n            self.available = False\n            raise\n    \n    def is_available(self) -> bool:\n        return self.available\n    \n    def _create_system_instruction(self, request: GenerationRequest) -> str:\n        """Create system instruction for Gemini"""\n        base_instruction = """You are an expert e-commerce recommendation assistant powered by Google\'s Gemini AI. \n        Your role is to provide personalized, helpful, and accurate product recommendations with clear explanations.\n        \n        Key guidelines:\n        - Always be helpful and user-focused\n        - Provide specific, actionable recommendations\n        - Explain your reasoning clearly\n        - Consider user context and preferences\n        - Be honest about product limitations\n        - Use a friendly but professional tone"""\n        \n        if request.user_context:\n            context_info = []\n            if request.user_context.get(\'preferred_categories\'):\n                context_info.append(f"User typically shops for: {\', \'.join(request.user_context[\'preferred_categories\'])}")\n            if request.user_context.get(\'budget_range\'):\n                budget = request.user_context[\'budget_range\']\n                context_info.append(f"User\'s budget range: ${budget.get(\'min\', 0)} - ${budget.get(\'max\', \'unlimited\')}")\n            if request.user_context.get(\'experience_level\'):\n                context_info.append(f"User\'s experience level: {request.user_context[\'experience_level\']}")\n            \n            if context_info:\n                base_instruction += f"\\n\\nUser Context:\\n{chr(10).join(context_info)}"\n        \n        if request.format_type == "json":\n            base_instruction += "\\n\\nIMPORTANT: Format your response as valid JSON only. Do not include any text outside the JSON structure."\n        elif request.format_type == "markdown":\n            base_instruction += "\\n\\nFormat your response using proper Markdown syntax with headers, lists, and emphasis where appropriate."\n        \n        return base_instruction\n\nclass LLMClient:\n    """Multi-provider LLM client with fallback support"""\n    \n    def __init__(self):\n        self.providers: List[BaseLLMProvider] = []\n        self.primary_provider: Optional[BaseLLMProvider] = None\n    \n    def add_provider(self, provider: BaseLLMProvider, is_primary: bool = False):\n        """Add an LLM provider"""\n        self.providers.append(provider)\n        if is_primary or self.primary_provider is None:\n            self.primary_provider = provider\n    \n    async def generate(self, request: GenerationRequest) -> GenerationResponse:\n        """Generate content with fallback support"""\n        # Try primary provider first\n        if self.primary_provider and self.primary_provider.is_available():\n            try:\n                return await self.primary_provider.generate(request)\n            except Exception as e:\n                logger.warning(f"Primary provider failed: {e}")\n        \n        # Try fallback providers\n        for provider in self.providers:\n            if provider != self.primary_provider and provider.is_available():\n                try:\n                    logger.info(f"Using fallback provider: {provider.__class__.__name__}")\n                    return await provider.generate(request)\n                except Exception as e:\n                    logger.warning(f"Fallback provider failed: {e}")\n                    continue\n        \n        raise Exception("All LLM providers are unavailable")\n'})}),"\n",(0,o.jsx)(n.h3,{id:"prompt-engineering-framework",children:"Prompt Engineering Framework"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# generation/prompt_templates.py\nfrom typing import Dict, Any, List\nfrom dataclasses import dataclass\nimport json\n\n@dataclass\nclass PromptTemplate:\n    """Template for generating prompts"""\n    template: str\n    required_vars: List[str]\n    optional_vars: List[str] = None\n    format_type: str = "text"\n\nclass PromptManager:\n    """Manages prompt templates for different recommendation scenarios"""\n    \n    def __init__(self):\n        self.templates = {\n            \'product_recommendation\': PromptTemplate(\n                template="""Based on the user\'s search query "{query}" and the following product options, create personalized recommendations.\n\nUser Context:\n- Preferred categories: {preferred_categories}\n- Budget range: {budget_range}\n- Previous purchases: {purchase_history}\n- Search intent: {search_intent}\n\nAvailable Products:\n{product_list}\n\nPlease provide:\n1. Top 3 product recommendations with explanations\n2. Why each product matches the user\'s needs\n3. Comparison between the recommended products\n4. Any additional considerations or alternatives\n\nFormat as a helpful, conversational response.""",\n                required_vars=[\'query\', \'product_list\'],\n                optional_vars=[\'preferred_categories\', \'budget_range\', \'purchase_history\', \'search_intent\']\n            ),\n            \n            \'product_explanation\': PromptTemplate(\n                template="""Explain why the following product is a good match for the user\'s query "{query}".\n\nProduct Details:\n{product_details}\n\nUser Context:\n{user_context}\n\nProvide a clear, compelling explanation that highlights:\n1. How the product meets the user\'s specific needs\n2. Key features and benefits relevant to the query\n3. Value proposition compared to alternatives\n4. Any potential considerations or limitations\n\nKeep the explanation concise but informative (2-3 paragraphs).""",\n                required_vars=[\'query\', \'product_details\'],\n                optional_vars=[\'user_context\']\n            ),\n            \n            \'comparison_analysis\': PromptTemplate(\n                template="""Compare the following products for a user searching for "{query}".\n\nProducts to Compare:\n{products_data}\n\nUser Requirements:\n{user_requirements}\n\nProvide a detailed comparison including:\n1. Feature comparison table\n2. Pros and cons for each product\n3. Best use cases for each option\n4. Clear recommendation based on user needs\n5. Price-value analysis\n\nFormat the response in clear sections with bullet points where appropriate.""",\n                required_vars=[\'query\', \'products_data\'],\n                optional_vars=[\'user_requirements\'],\n                format_type=\'markdown\'\n            ),\n            \n            \'purchase_guide\': PromptTemplate(\n                template="""Create a comprehensive buying guide for "{product_category}" based on the user\'s search "{query}".\n\nAvailable Products:\n{product_options}\n\nUser Profile:\n{user_profile}\n\nInclude:\n1. Key factors to consider when choosing\n2. Budget recommendations for different needs\n3. Must-have vs nice-to-have features\n4. Common mistakes to avoid\n5. Specific product recommendations from the available options\n\nStructure as an easy-to-follow guide with clear sections.""",\n                required_vars=[\'query\', \'product_category\', \'product_options\'],\n                optional_vars=[\'user_profile\'],\n                format_type=\'markdown\'\n            ),\n            \n            \'personalized_summary\': PromptTemplate(\n                template="""Create a personalized product summary for this user.\n\nProduct: {product_name}\nProduct Details: {product_details}\n\nUser Profile:\n- Interests: {user_interests}\n- Experience level: {experience_level}\n- Usage scenario: {usage_scenario}\n- Budget considerations: {budget_info}\n\nWrite a compelling 2-3 sentence summary that:\n1. Highlights the most relevant features for this user\n2. Connects to their specific interests and needs\n3. Uses language appropriate for their experience level\n4. Mentions value in their budget context\n\nKeep it engaging and personal.""",\n                required_vars=[\'product_name\', \'product_details\'],\n                optional_vars=[\'user_interests\', \'experience_level\', \'usage_scenario\', \'budget_info\']\n            )\n        }\n    \n    def generate_prompt(self, template_name: str, variables: Dict[str, Any]) -> str:\n        """Generate a prompt from template and variables"""\n        if template_name not in self.templates:\n            raise ValueError(f"Template \'{template_name}\' not found")\n        \n        template = self.templates[template_name]\n        \n        # Check required variables\n        missing_vars = [var for var in template.required_vars if var not in variables]\n        if missing_vars:\n            raise ValueError(f"Missing required variables: {missing_vars}")\n        \n        # Set default values for optional variables\n        prompt_vars = variables.copy()\n        if template.optional_vars:\n            for var in template.optional_vars:\n                if var not in prompt_vars:\n                    prompt_vars[var] = "Not specified"\n        \n        # Generate prompt\n        try:\n            prompt = template.template.format(**prompt_vars)\n            return prompt\n        except KeyError as e:\n            raise ValueError(f"Variable substitution failed: {e}")\n    \n    def get_template_info(self, template_name: str) -> Dict[str, Any]:\n        """Get information about a template"""\n        if template_name not in self.templates:\n            raise ValueError(f"Template \'{template_name}\' not found")\n        \n        template = self.templates[template_name]\n        return {\n            \'required_vars\': template.required_vars,\n            \'optional_vars\': template.optional_vars or [],\n            \'format_type\': template.format_type\n        }\n'})}),"\n",(0,o.jsx)(n.h2,{id:"complete-vertex-ai-setup-example",children:"Complete Vertex AI Setup Example"}),"\n",(0,o.jsx)(n.h3,{id:"environment-setup",children:"Environment Setup"}),"\n",(0,o.jsx)(n.p,{children:"First, install the required dependencies and set up authentication:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'# Install Vertex AI SDK\npip install google-cloud-aiplatform google-auth google-auth-oauthlib\n\n# Set up authentication (choose one method)\n# Method 1: Service Account Key\nexport GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account-key.json"\n\n# Method 2: Application Default Credentials (for local development)\ngcloud auth application-default login\n\n# Method 3: Workload Identity (for GKE/Cloud Run)\n# Configure workload identity in your cluster\n'})}),"\n",(0,o.jsx)(n.h3,{id:"complete-implementation-example",children:"Complete Implementation Example"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# vertex_ai_example.py\nimport asyncio\nimport os\nimport json\nfrom typing import Dict, Any, List, Optional\nimport logging\nfrom dataclasses import dataclass\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ProductRecommendation:\n    """Product recommendation with Vertex AI generated explanation"""\n    product_id: str\n    title: str\n    price: float\n    category: str\n    description: str\n    ai_explanation: str\n    confidence_score: float\n    recommendation_rank: int\n\nclass VertexAIRecommendationEngine:\n    """Complete recommendation engine using Vertex AI Gemini"""\n    \n    def __init__(self, project_id: str, location: str = "us-central1"):\n        self.project_id = project_id\n        self.location = location\n        self.model = None\n        self.initialize_vertex_ai()\n    \n    def initialize_vertex_ai(self):\n        """Initialize Vertex AI with proper configuration"""\n        try:\n            import vertexai\n            from vertexai.generative_models import GenerativeModel, SafetySetting, HarmCategory\n            \n            # Initialize Vertex AI\n            vertexai.init(project=self.project_id, location=self.location)\n            \n            # Configure safety settings for e-commerce content\n            safety_settings = [\n                SafetySetting(\n                    category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n                    threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                ),\n                SafetySetting(\n                    category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n                    threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                ),\n                SafetySetting(\n                    category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n                    threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                ),\n                SafetySetting(\n                    category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n                    threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                ),\n            ]\n            \n            # Generation configuration optimized for recommendations\n            generation_config = {\n                "max_output_tokens": 1024,\n                "temperature": 0.7,\n                "top_p": 0.95,\n                "top_k": 40,\n            }\n            \n            # System instruction for e-commerce recommendations\n            system_instruction = """You are an expert e-commerce recommendation assistant.\n            Your role is to help customers find products that best match their needs and preferences.\n            \n            Guidelines:\n            - Provide personalized recommendations based on user context\n            - Explain why each product is suitable for the customer\n            - Be honest about product limitations\n            - Consider budget, preferences, and use cases\n            - Use a helpful, friendly tone\n            - Focus on value and relevance to the customer\'s needs"""\n            \n            self.model = GenerativeModel(\n                "gemini-1.5-pro",\n                system_instruction=[system_instruction],\n                generation_config=generation_config,\n                safety_settings=safety_settings\n            )\n            \n            logger.info("Vertex AI Gemini model initialized successfully")\n            \n        except ImportError:\n            logger.error("Vertex AI SDK not installed. Run: pip install google-cloud-aiplatform")\n            raise\n        except Exception as e:\n            logger.error(f"Failed to initialize Vertex AI: {e}")\n            raise\n    \n    async def generate_recommendations(self, \n                                     user_query: str,\n                                     products: List[Dict[str, Any]],\n                                     user_context: Optional[Dict[str, Any]] = None,\n                                     max_recommendations: int = 5) -> List[ProductRecommendation]:\n        """Generate personalized product recommendations using Vertex AI"""\n        \n        try:\n            # Prepare the prompt with user context and products\n            prompt = self._create_recommendation_prompt(user_query, products, user_context)\n            \n            # Generate recommendations using Gemini\n            response = self.model.generate_content(prompt)\n            \n            # Parse the response\n            recommendations = self._parse_gemini_response(response.text, products)\n            \n            # Limit to requested number of recommendations\n            return recommendations[:max_recommendations]\n            \n        except Exception as e:\n            logger.error(f"Failed to generate recommendations: {e}")\n            raise\n    \n    def _create_recommendation_prompt(self, \n                                    user_query: str, \n                                    products: List[Dict[str, Any]], \n                                    user_context: Optional[Dict[str, Any]]) -> str:\n        """Create a comprehensive prompt for Gemini"""\n        \n        # Format user context\n        context_str = ""\n        if user_context:\n            context_parts = []\n            if user_context.get(\'budget_range\'):\n                context_parts.append(f"Budget: ${user_context[\'budget_range\'].get(\'min\', 0)} - ${user_context[\'budget_range\'].get(\'max\', \'unlimited\')}")\n            if user_context.get(\'preferred_categories\'):\n                context_parts.append(f"Preferred categories: {\', \'.join(user_context[\'preferred_categories\'])}")\n            if user_context.get(\'previous_purchases\'):\n                context_parts.append(f"Previous purchases: {\', \'.join(user_context[\'previous_purchases\'][:3])}")\n            if user_context.get(\'experience_level\'):\n                context_parts.append(f"Experience level: {user_context[\'experience_level\']}")\n            \n            if context_parts:\n                context_str = f"User Context:\\n{chr(10).join(context_parts)}\\n\\n"\n        \n        # Format products\n        products_str = "Available Products:\\n"\n        for i, product in enumerate(products, 1):\n            products_str += f"""\nProduct {i}:\n- ID: {product.get(\'id\', \'unknown\')}\n- Title: {product.get(\'title\', \'No title\')}\n- Price: ${product.get(\'price\', 0):.2f}\n- Category: {product.get(\'category\', \'Uncategorized\')}\n- Brand: {product.get(\'brand\', \'Unknown\')}\n- Rating: {product.get(\'rating\', 0):.1f}/5.0 ({product.get(\'review_count\', 0)} reviews)\n- Description: {product.get(\'description\', \'No description\')[:200]}{\'...\' if len(product.get(\'description\', \'\')) > 200 else \'\'}\n- Key Features: {\', \'.join(product.get(\'features\', [])[:5])}\n"""\n        \n        prompt = f"""User Query: "{user_query}"\n\n{context_str}{products_str}\n\nTask: Analyze the user\'s query and context, then recommend the top 3-5 products that best match their needs.\n\nFor each recommendation, provide:\n1. Product ID and title\n2. Why this product matches the user\'s needs (2-3 sentences)\n3. Key benefits specific to their use case\n4. Any considerations or limitations\n5. Confidence score (0.0-1.0) for how well it matches\n\nFormat your response as JSON with this structure:\n{{\n  "recommendations": [\n    {{\n      "product_id": "product_id_here",\n      "explanation": "Detailed explanation here...",\n      "key_benefits": ["benefit1", "benefit2", "benefit3"],\n      "considerations": ["consideration1", "consideration2"],\n      "confidence_score": 0.95,\n      "rank": 1\n    }}\n  ],\n  "summary": "Overall recommendation summary...",\n  "reasoning": "Why these products were selected..."\n}}\n\nImportant: Respond only with valid JSON. Do not include any text outside the JSON structure."""\n        \n        return prompt\n    \n    def _parse_gemini_response(self, \n                             response_text: str, \n                             original_products: List[Dict[str, Any]]) -> List[ProductRecommendation]:\n        """Parse Gemini\'s JSON response into ProductRecommendation objects"""\n        \n        try:\n            # Clean the response text\n            response_text = response_text.strip()\n            \n            # Remove markdown code blocks if present\n            if response_text.startswith(\'```json\'):\n                response_text = response_text[7:]\n            if response_text.endswith(\'```\'):\n                response_text = response_text[:-3]\n            \n            # Parse JSON\n            data = json.loads(response_text)\n            \n            recommendations = []\n            product_lookup = {p.get(\'id\'): p for p in original_products}\n            \n            for rec_data in data.get(\'recommendations\', []):\n                product_id = rec_data.get(\'product_id\')\n                product = product_lookup.get(product_id, {})\n                \n                recommendation = ProductRecommendation(\n                    product_id=product_id,\n                    title=product.get(\'title\', \'Unknown Product\'),\n                    price=product.get(\'price\', 0.0),\n                    category=product.get(\'category\', \'Unknown\'),\n                    description=product.get(\'description\', \'\'),\n                    ai_explanation=rec_data.get(\'explanation\', \'\'),\n                    confidence_score=rec_data.get(\'confidence_score\', 0.0),\n                    recommendation_rank=rec_data.get(\'rank\', 0)\n                )\n                recommendations.append(recommendation)\n            \n            # Sort by rank\n            recommendations.sort(key=lambda x: x.recommendation_rank)\n            \n            logger.info(f"Successfully parsed {len(recommendations)} recommendations")\n            return recommendations\n            \n        except json.JSONDecodeError as e:\n            logger.error(f"Failed to parse JSON response: {e}")\n            logger.error(f"Response text: {response_text}")\n            \n            # Fallback: create basic recommendations from original products\n            return self._create_fallback_recommendations(original_products)\n        \n        except Exception as e:\n            logger.error(f"Error parsing response: {e}")\n            return self._create_fallback_recommendations(original_products)\n    \n    def _create_fallback_recommendations(self, products: List[Dict[str, Any]]) -> List[ProductRecommendation]:\n        """Create basic recommendations when AI parsing fails"""\n        recommendations = []\n        \n        for i, product in enumerate(products[:3]):\n            recommendation = ProductRecommendation(\n                product_id=product.get(\'id\', f\'fallback_{i}\'),\n                title=product.get(\'title\', \'Product\'),\n                price=product.get(\'price\', 0.0),\n                category=product.get(\'category\', \'Unknown\'),\n                description=product.get(\'description\', \'\'),\n                ai_explanation="This product matches your search criteria and is available in our catalog.",\n                confidence_score=0.7,\n                recommendation_rank=i + 1\n            )\n            recommendations.append(recommendation)\n        \n        return recommendations\n\nclass RecommendationService:\n    """Service class for handling recommendation requests"""\n    \n    def __init__(self, project_id: str, location: str = "us-central1"):\n        self.engine = VertexAIRecommendationEngine(project_id, location)\n        self.product_database = self._initialize_sample_products()\n    \n    def _initialize_sample_products(self) -> List[Dict[str, Any]]:\n        """Initialize sample product database"""\n        return [\n            {\n                "id": "laptop_001",\n                "title": "MacBook Air M2 13-inch",\n                "price": 1199.00,\n                "category": "laptops",\n                "brand": "Apple",\n                "rating": 4.7,\n                "review_count": 2845,\n                "description": "Apple MacBook Air 13-inch with M2 chip, 8GB RAM, 256GB SSD. Perfect for everyday computing, creative work, and portability.",\n                "features": ["M2 Chip", "13.6-inch Liquid Retina Display", "8GB RAM", "256GB SSD", "All-day battery life"]\n            },\n            {\n                "id": "laptop_002", \n                "title": "Dell XPS 13 Plus",\n                "price": 1299.00,\n                "category": "laptops",\n                "brand": "Dell",\n                "rating": 4.5,\n                "review_count": 1234,\n                "description": "Dell XPS 13 Plus with Intel 12th Gen processors, 16GB RAM, 512GB SSD. Premium ultrabook for professionals.",\n                "features": ["Intel 12th Gen i7", "13.4-inch OLED Display", "16GB RAM", "512GB SSD", "Premium build quality"]\n            },\n            {\n                "id": "laptop_003",\n                "title": "Lenovo ThinkPad X1 Carbon",\n                "price": 1499.00,\n                "category": "laptops", \n                "brand": "Lenovo",\n                "rating": 4.6,\n                "review_count": 987,\n                "description": "Business-grade laptop with Intel processors, 16GB RAM, 1TB SSD. Built for productivity and durability.",\n                "features": ["Intel 12th Gen i7", "14-inch 2.8K Display", "16GB RAM", "1TB SSD", "Military-grade durability"]\n            },\n            {\n                "id": "phone_001",\n                "title": "iPhone 15 Pro",\n                "price": 999.00,\n                "category": "smartphones",\n                "brand": "Apple", \n                "rating": 4.8,\n                "review_count": 5421,\n                "description": "Latest iPhone with A17 Pro chip, advanced camera system, and titanium design.",\n                "features": ["A17 Pro chip", "48MP Main Camera", "6.1-inch Super Retina XDR", "Titanium Design", "Action Button"]\n            },\n            {\n                "id": "headphones_001",\n                "title": "Sony WH-1000XM5",\n                "price": 399.00,\n                "category": "headphones",\n                "brand": "Sony",\n                "rating": 4.6,\n                "review_count": 3210,\n                "description": "Premium noise-canceling headphones with exceptional sound quality and comfort.",\n                "features": ["Industry-leading noise canceling", "30-hour battery", "Quick Charge", "Multipoint connection", "LDAC support"]\n            }\n        ]\n    \n    async def get_recommendations(self, \n                                user_query: str,\n                                user_context: Optional[Dict[str, Any]] = None,\n                                max_results: int = 5) -> Dict[str, Any]:\n        """Get personalized recommendations for a user query"""\n        \n        try:\n            # Filter products based on query (simple keyword matching for demo)\n            relevant_products = self._filter_products(user_query)\n            \n            if not relevant_products:\n                return {\n                    "success": False,\n                    "message": "No products found matching your query",\n                    "recommendations": []\n                }\n            \n            # Generate AI-powered recommendations\n            recommendations = await self.engine.generate_recommendations(\n                user_query=user_query,\n                products=relevant_products,\n                user_context=user_context,\n                max_recommendations=max_results\n            )\n            \n            return {\n                "success": True,\n                "query": user_query,\n                "user_context": user_context,\n                "recommendations": [\n                    {\n                        "product_id": rec.product_id,\n                        "title": rec.title,\n                        "price": rec.price,\n                        "category": rec.category,\n                        "ai_explanation": rec.ai_explanation,\n                        "confidence_score": rec.confidence_score,\n                        "rank": rec.recommendation_rank\n                    }\n                    for rec in recommendations\n                ],\n                "total_recommendations": len(recommendations)\n            }\n            \n        except Exception as e:\n            logger.error(f"Recommendation service error: {e}")\n            return {\n                "success": False,\n                "error": str(e),\n                "recommendations": []\n            }\n    \n    def _filter_products(self, query: str) -> List[Dict[str, Any]]:\n        """Simple product filtering based on query keywords"""\n        query_lower = query.lower()\n        relevant_products = []\n        \n        for product in self.product_database:\n            # Check if query matches product title, category, or features\n            if (query_lower in product[\'title\'].lower() or\n                query_lower in product[\'category\'].lower() or\n                any(query_lower in feature.lower() for feature in product.get(\'features\', [])) or\n                query_lower in product[\'description\'].lower()):\n                relevant_products.append(product)\n        \n        return relevant_products\n\n# Usage Example\nasync def main():\n    """Example usage of the Vertex AI recommendation system"""\n    \n    # Configuration\n    PROJECT_ID = "your-gcp-project-id"  # Replace with your GCP project ID\n    LOCATION = "us-central1"\n    \n    try:\n        # Initialize the recommendation service\n        service = RecommendationService(PROJECT_ID, LOCATION)\n        \n        # Example 1: Laptop recommendation for a student\n        print("=== Example 1: Student Laptop Recommendation ===")\n        student_context = {\n            "budget_range": {"min": 800, "max": 1500},\n            "preferred_categories": ["laptops", "computers"],\n            "experience_level": "intermediate",\n            "use_case": "studying and light gaming"\n        }\n        \n        result1 = await service.get_recommendations(\n            user_query="best laptop for college student programming",\n            user_context=student_context,\n            max_results=3\n        )\n        \n        print(f"Query: {result1[\'query\']}")\n        print(f"Success: {result1[\'success\']}")\n        for i, rec in enumerate(result1[\'recommendations\'], 1):\n            print(f"\\n{i}. {rec[\'title\']} (${rec[\'price\']})")\n            print(f"   Confidence: {rec[\'confidence_score\']:.2f}")\n            print(f"   AI Explanation: {rec[\'ai_explanation\']}")\n        \n        # Example 2: Premium headphones recommendation\n        print("\\n\\n=== Example 2: Premium Headphones Recommendation ===")\n        audiophile_context = {\n            "budget_range": {"min": 300, "max": 500},\n            "preferred_categories": ["headphones", "audio"],\n            "experience_level": "expert",\n            "previous_purchases": ["Sennheiser HD600", "Audio-Technica ATH-M50x"]\n        }\n        \n        result2 = await service.get_recommendations(\n            user_query="noise canceling headphones for travel",\n            user_context=audiophile_context,\n            max_results=2\n        )\n        \n        print(f"Query: {result2[\'query\']}")\n        print(f"Success: {result2[\'success\']}")\n        for i, rec in enumerate(result2[\'recommendations\'], 1):\n            print(f"\\n{i}. {rec[\'title\']} (${rec[\'price\']})")\n            print(f"   Confidence: {rec[\'confidence_score\']:.2f}")\n            print(f"   AI Explanation: {rec[\'ai_explanation\']}")\n        \n        # Example 3: Simple query without user context\n        print("\\n\\n=== Example 3: Simple Query ===")\n        result3 = await service.get_recommendations(\n            user_query="smartphone",\n            max_results=1\n        )\n        \n        print(f"Query: {result3[\'query\']}")\n        for rec in result3[\'recommendations\']:\n            print(f"\\nRecommendation: {rec[\'title\']} (${rec[\'price\']})")\n            print(f"AI Explanation: {rec[\'ai_explanation\']}")\n        \n    except Exception as e:\n        logger.error(f"Example execution failed: {e}")\n        print(f"Error: {e}")\n\nif __name__ == "__main__":\n    # Run the example\n    asyncio.run(main())\n'})}),"\n",(0,o.jsx)(n.h3,{id:"production-deployment-configuration",children:"Production Deployment Configuration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'# vertex-ai-config.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vertex-ai-config\ndata:\n  PROJECT_ID: "your-gcp-project-id"\n  LOCATION: "us-central1"\n  MODEL_NAME: "gemini-1.5-pro"\n  MAX_TOKENS: "1024"\n  TEMPERATURE: "0.7"\n  \n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rag-recommendation-service\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: rag-recommendation\n  template:\n    metadata:\n      labels:\n        app: rag-recommendation\n    spec:\n      serviceAccountName: vertex-ai-service-account\n      containers:\n      - name: recommendation-api\n        image: gcr.io/your-project/rag-recommendation:latest\n        ports:\n        - containerPort: 8080\n        env:\n        - name: GOOGLE_CLOUD_PROJECT\n          valueFrom:\n            configMapKeyRef:\n              name: vertex-ai-config\n              key: PROJECT_ID\n        - name: VERTEX_AI_LOCATION\n          valueFrom:\n            configMapKeyRef:\n              name: vertex-ai-config\n              key: LOCATION\n        resources:\n          requests:\n            memory: "512Mi"\n            cpu: "250m"\n          limits:\n            memory: "1Gi" \n            cpu: "500m"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n'})}),"\n",(0,o.jsx)(n.h3,{id:"cost-optimization-and-monitoring",children:"Cost Optimization and Monitoring"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# monitoring/vertex_ai_monitor.py\nimport time\nfrom typing import Dict, Any\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\n\n@dataclass\nclass UsageMetrics:\n    """Track Vertex AI usage metrics"""\n    total_requests: int = 0\n    total_tokens: int = 0\n    total_cost: float = 0.0\n    average_response_time: float = 0.0\n    error_count: int = 0\n    \nclass VertexAIMonitor:\n    """Monitor Vertex AI usage and costs"""\n    \n    def __init__(self):\n        self.metrics = UsageMetrics()\n        self.request_history = []\n        \n        # Pricing (as of 2025 - check latest pricing)\n        self.pricing = {\n            "gemini-1.5-pro": {\n                "input_tokens_per_1k": 0.0035,\n                "output_tokens_per_1k": 0.0105\n            }\n        }\n    \n    def track_request(self, \n                     model_name: str,\n                     input_tokens: int,\n                     output_tokens: int,\n                     response_time: float,\n                     success: bool):\n        """Track individual request metrics"""\n        \n        # Calculate cost\n        pricing = self.pricing.get(model_name, self.pricing["gemini-1.5-pro"])\n        input_cost = (input_tokens / 1000) * pricing["input_tokens_per_1k"]\n        output_cost = (output_tokens / 1000) * pricing["output_tokens_per_1k"]\n        total_cost = input_cost + output_cost\n        \n        # Update metrics\n        self.metrics.total_requests += 1\n        self.metrics.total_tokens += input_tokens + output_tokens\n        self.metrics.total_cost += total_cost\n        \n        # Update average response time\n        total_time = self.metrics.average_response_time * (self.metrics.total_requests - 1)\n        self.metrics.average_response_time = (total_time + response_time) / self.metrics.total_requests\n        \n        if not success:\n            self.metrics.error_count += 1\n        \n        # Store request history\n        self.request_history.append({\n            "timestamp": datetime.now(),\n            "model": model_name,\n            "input_tokens": input_tokens,\n            "output_tokens": output_tokens,\n            "cost": total_cost,\n            "response_time": response_time,\n            "success": success\n        })\n        \n        logging.info(f"Request tracked - Cost: ${total_cost:.4f}, Time: {response_time:.2f}s")\n    \n    def get_daily_summary(self) -> Dict[str, Any]:\n        """Get daily usage summary"""\n        today = datetime.now().date()\n        today_requests = [\n            req for req in self.request_history \n            if req["timestamp"].date() == today\n        ]\n        \n        if not today_requests:\n            return {"date": today, "no_requests": True}\n        \n        total_cost = sum(req["cost"] for req in today_requests)\n        total_requests = len(today_requests)\n        avg_response_time = sum(req["response_time"] for req in today_requests) / total_requests\n        error_rate = sum(1 for req in today_requests if not req["success"]) / total_requests\n        \n        return {\n            "date": today,\n            "total_requests": total_requests,\n            "total_cost": total_cost,\n            "average_response_time": avg_response_time,\n            "error_rate": error_rate,\n            "cost_per_request": total_cost / total_requests if total_requests > 0 else 0\n        }\n'})}),"\n",(0,o.jsx)(n.p,{children:"This comprehensive Vertex AI example provides:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Complete Implementation"})," - Full working code with error handling"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Production Configuration"})," - Kubernetes deployment and monitoring"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cost Tracking"})," - Usage monitoring and optimization"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real Examples"})," - Multiple use cases with different user contexts"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safety Configuration"})," - Proper content filtering for production use"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Fallback Handling"})," - Graceful degradation when AI responses fail"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"The example demonstrates how to integrate Vertex AI Gemini into your RAG recommendation system with proper production practices."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# generation/recommendation_engine.py\nimport json\nfrom typing import Dict, Any, List, Optional\nimport logging\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RecommendationItem:\n    \"\"\"Individual recommendation item\"\"\"\n    product_id: str\n    title: str\n    explanation: str\n    confidence_score: float\n    ranking_factors: Dict[str, float]\n    personalization_factors: Dict[str, Any]\n\n@dataclass\nclass RecommendationSet:\n    \"\"\"Complete set of recommendations\"\"\"\n    recommendations: List[RecommendationItem]\n    summary: str\n    comparison: Optional[str] = None\n    buying_guide: Optional[str] = None\n    metadata: Dict[str, Any] = None\n\nclass RecommendationGenerator:\n    \"\"\"Generates personalized recommendations using LLM\"\"\"\n    \n    def __init__(self, llm_client: LLMClient, prompt_manager: PromptManager):\n        self.llm_client = llm_client\n        self.prompt_manager = prompt_manager\n        \n        # Generation settings\n        self.default_settings = {\n            'max_tokens': 1500,\n            'temperature': 0.7,\n            'top_recommendations': 5\n        }\n    \n    async def generate_recommendations(self,\n                                     query: str,\n                                     retrieved_products: List[Dict[str, Any]],\n                                     user_context: Optional[Dict[str, Any]] = None,\n                                     recommendation_type: str = 'standard') -> RecommendationSet:\n        \"\"\"Generate complete recommendation set\"\"\"\n        logger.info(f\"Generating recommendations for query: {query}\")\n        \n        try:\n            # Prepare product data\n            product_list = self._format_products_for_prompt(retrieved_products)\n            \n            # Generate main recommendations\n            recommendations = await self._generate_main_recommendations(\n                query, product_list, user_context\n            )\n            \n            # Generate summary\n            summary = await self._generate_summary(query, recommendations, user_context)\n            \n            # Generate comparison if multiple products\n            comparison = None\n            if len(recommendations) > 1:\n                comparison = await self._generate_comparison(query, recommendations, user_context)\n            \n            # Generate buying guide if requested\n            buying_guide = None\n            if recommendation_type == 'detailed':\n                buying_guide = await self._generate_buying_guide(query, recommendations, user_context)\n            \n            return RecommendationSet(\n                recommendations=recommendations,\n                summary=summary,\n                comparison=comparison,\n                buying_guide=buying_guide,\n                metadata={\n                    'query': query,\n                    'recommendation_type': recommendation_type,\n                    'product_count': len(retrieved_products),\n                    'user_context_available': user_context is not None\n                }\n            )\n            \n        except Exception as e:\n            logger.error(f\"Recommendation generation failed: {e}\")\n            raise\n    \n    async def _generate_main_recommendations(self,\n                                           query: str,\n                                           product_list: str,\n                                           user_context: Optional[Dict[str, Any]]) -> List[RecommendationItem]:\n        \"\"\"Generate main product recommendations\"\"\"\n        \n        # Prepare variables for prompt\n        variables = {\n            'query': query,\n            'product_list': product_list,\n            'preferred_categories': self._format_user_preferences(user_context, 'categories'),\n            'budget_range': self._format_user_preferences(user_context, 'budget'),\n            'purchase_history': self._format_user_preferences(user_context, 'history'),\n            'search_intent': self._infer_search_intent(query)\n        }\n        \n        # Generate prompt\n        prompt = self.prompt_manager.generate_prompt('product_recommendation', variables)\n        \n        # Create generation request\n        request = GenerationRequest(\n            prompt=prompt,\n            max_tokens=self.default_settings['max_tokens'],\n            temperature=self.default_settings['temperature'],\n            user_context=user_context,\n            format_type='json'\n        )\n        \n        # Generate response\n        response = await self.llm_client.generate(request)\n        \n        # Parse and structure recommendations\n        return self._parse_recommendations(response.content, query)\n    \n    async def _generate_summary(self,\n                              query: str,\n                              recommendations: List[RecommendationItem],\n                              user_context: Optional[Dict[str, Any]]) -> str:\n        \"\"\"Generate executive summary of recommendations\"\"\"\n        \n        # Create summary prompt\n        rec_summaries = [f\"- {rec.title}: {rec.explanation[:100]}...\" for rec in recommendations]\n        \n        prompt = f\"\"\"Based on the search query \"{query}\" and the following recommendations, create a brief executive summary:\n\nRecommendations:\n{chr(10).join(rec_summaries)}\n\nUser Context: {self._format_user_context_summary(user_context)}\n\nProvide a 2-3 sentence summary that:\n1. Addresses the user's search intent\n2. Highlights the best overall recommendation\n3. Mentions key decision factors\n\nKeep it concise and actionable.\"\"\"\n        \n        request = GenerationRequest(\n            prompt=prompt,\n            max_tokens=200,\n            temperature=0.6,\n            user_context=user_context\n        )\n        \n        response = await self.llm_client.generate(request)\n        return response.content.strip()\n    \n    async def _generate_comparison(self,\n                                 query: str,\n                                 recommendations: List[RecommendationItem],\n                                 user_context: Optional[Dict[str, Any]]) -> str:\n        \"\"\"Generate detailed comparison between recommendations\"\"\"\n        \n        # Prepare comparison data\n        products_data = []\n        for rec in recommendations:\n            products_data.append(f\"Product: {rec.title}\\nFeatures: {rec.explanation}\")\n        \n        variables = {\n            'query': query,\n            'products_data': '\\n\\n'.join(products_data),\n            'user_requirements': self._format_user_requirements(user_context)\n        }\n        \n        prompt = self.prompt_manager.generate_prompt('comparison_analysis', variables)\n        \n        request = GenerationRequest(\n            prompt=prompt,\n            max_tokens=1000,\n            temperature=0.6,\n            user_context=user_context,\n            format_type='markdown'\n        )\n        \n        response = await self.llm_client.generate(request)\n        return response.content\n    \n    async def _generate_buying_guide(self,\n                                   query: str,\n                                   recommendations: List[RecommendationItem],\n                                   user_context: Optional[Dict[str, Any]]) -> str:\n        \"\"\"Generate comprehensive buying guide\"\"\"\n        \n        # Infer product category from query and recommendations\n        product_category = self._infer_product_category(query, recommendations)\n        \n        # Format product options\n        product_options = [f\"- {rec.title}: {rec.explanation}\" for rec in recommendations]\n        \n        variables = {\n            'query': query,\n            'product_category': product_category,\n            'product_options': '\\n'.join(product_options),\n            'user_profile': self._format_user_profile(user_context)\n        }\n        \n        prompt = self.prompt_manager.generate_prompt('purchase_guide', variables)\n        \n        request = GenerationRequest(\n            prompt=prompt,\n            max_tokens=1500,\n            temperature=0.7,\n            user_context=user_context,\n            format_type='markdown'\n        )\n        \n        response = await self.llm_client.generate(request)\n        return response.content\n    \n    def _format_products_for_prompt(self, products: List[Dict[str, Any]]) -> str:\n        \"\"\"Format product data for LLM prompt\"\"\"\n        formatted_products = []\n        \n        for i, product in enumerate(products[:self.default_settings['top_recommendations']], 1):\n            product_text = f\"\"\"Product {i}: {product.get('title', 'Unknown Product')}\n- Category: {product.get('category', 'N/A')}\n- Brand: {product.get('brand', 'N/A')}\n- Price: ${product.get('price', 0):.2f}\n- Rating: {product.get('rating', 0):.1f}/5.0 ({product.get('review_count', 0)} reviews)\n- Description: {product.get('description', 'No description available')[:200]}...\n- Key Features: {', '.join(product.get('keywords', [])[:5])}\"\"\"\n            \n            formatted_products.append(product_text)\n        \n        return '\\n\\n'.join(formatted_products)\n    \n    def _format_user_preferences(self, user_context: Optional[Dict[str, Any]], pref_type: str) -> str:\n        \"\"\"Format user preferences for prompts\"\"\"\n        if not user_context:\n            return \"Not specified\"\n        \n        if pref_type == 'categories':\n            return ', '.join(user_context.get('preferred_categories', []))\n        elif pref_type == 'budget':\n            budget = user_context.get('budget_range', {})\n            if budget:\n                return f\"${budget.get('min', 0)} - ${budget.get('max', 'unlimited')}\"\n        elif pref_type == 'history':\n            history = user_context.get('purchase_history', [])\n            return ', '.join(history[:3]) if history else \"No previous purchases\"\n        \n        return \"Not specified\"\n    \n    def _infer_search_intent(self, query: str) -> str:\n        \"\"\"Infer user's search intent from query\"\"\"\n        query_lower = query.lower()\n        \n        if any(word in query_lower for word in ['buy', 'purchase', 'order']):\n            return \"Purchase intent\"\n        elif any(word in query_lower for word in ['compare', 'vs', 'difference']):\n            return \"Comparison intent\"\n        elif any(word in query_lower for word in ['best', 'top', 'recommend']):\n            return \"Recommendation seeking\"\n        elif any(word in query_lower for word in ['cheap', 'budget', 'affordable']):\n            return \"Budget-conscious\"\n        elif any(word in query_lower for word in ['premium', 'luxury', 'high-end']):\n            return \"Quality-focused\"\n        else:\n            return \"General exploration\"\n    \n    def _parse_recommendations(self, llm_response: str, query: str) -> List[RecommendationItem]:\n        \"\"\"Parse LLM response into structured recommendations\"\"\"\n        recommendations = []\n        \n        try:\n            # Try to parse as JSON first\n            if llm_response.strip().startswith('{') or llm_response.strip().startswith('['):\n                data = json.loads(llm_response)\n                if isinstance(data, dict) and 'recommendations' in data:\n                    data = data['recommendations']\n                \n                for item in data:\n                    rec = RecommendationItem(\n                        product_id=item.get('product_id', ''),\n                        title=item.get('title', ''),\n                        explanation=item.get('explanation', ''),\n                        confidence_score=item.get('confidence_score', 0.8),\n                        ranking_factors=item.get('ranking_factors', {}),\n                        personalization_factors=item.get('personalization_factors', {})\n                    )\n                    recommendations.append(rec)\n            \n        except json.JSONDecodeError:\n            # Fallback: parse text format\n            logger.warning(\"Failed to parse JSON, falling back to text parsing\")\n            recommendations = self._parse_text_recommendations(llm_response, query)\n        \n        return recommendations\n    \n    def _parse_text_recommendations(self, text: str, query: str) -> List[RecommendationItem]:\n        \"\"\"Parse text-based recommendations\"\"\"\n        recommendations = []\n        \n        # Simple parsing logic - would be enhanced in production\n        sections = text.split('\\n\\n')\n        \n        for i, section in enumerate(sections):\n            if any(keyword in section.lower() for keyword in ['recommend', 'product', 'option']):\n                rec = RecommendationItem(\n                    product_id=f\"parsed_{i}\",\n                    title=f\"Recommendation {i+1}\",\n                    explanation=section.strip(),\n                    confidence_score=0.7,\n                    ranking_factors={'text_parsing': 1.0},\n                    personalization_factors={}\n                )\n                recommendations.append(rec)\n        \n        return recommendations[:3]  # Limit to top 3\n"})}),"\n",(0,o.jsx)(n.h3,{id:"content-quality-control",children:"Content Quality Control"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# generation/quality_control.py\nimport re\nfrom typing import Dict, Any, List\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass ContentQualityController:\n    \"\"\"Ensures generated content meets quality standards\"\"\"\n    \n    def __init__(self):\n        # Quality thresholds\n        self.min_explanation_length = 50\n        self.max_explanation_length = 500\n        self.min_summary_length = 20\n        self.max_summary_length = 200\n        \n        # Content filters\n        self.prohibited_phrases = [\n            'i cannot', 'i am unable', 'i don\\'t know',\n            'as an ai', 'i\\'m sorry', 'i apologize'\n        ]\n        \n        # Required elements for recommendations\n        self.required_elements = [\n            'product name', 'price', 'key features', 'explanation'\n        ]\n    \n    def validate_recommendation_set(self, recommendation_set: RecommendationSet) -> Dict[str, Any]:\n        \"\"\"Validate complete recommendation set quality\"\"\"\n        validation_results = {\n            'overall_score': 0.0,\n            'issues': [],\n            'recommendations': True,\n            'detailed_scores': {}\n        }\n        \n        try:\n            # Validate individual recommendations\n            rec_scores = []\n            for i, rec in enumerate(recommendation_set.recommendations):\n                rec_validation = self._validate_recommendation(rec)\n                rec_scores.append(rec_validation['score'])\n                validation_results['detailed_scores'][f'recommendation_{i}'] = rec_validation\n                \n                if rec_validation['score'] < 0.6:\n                    validation_results['issues'].append(\n                        f\"Recommendation {i+1} quality below threshold: {rec_validation['issues']}\"\n                    )\n            \n            # Validate summary\n            if recommendation_set.summary:\n                summary_validation = self._validate_summary(recommendation_set.summary)\n                validation_results['detailed_scores']['summary'] = summary_validation\n                \n                if summary_validation['score'] < 0.7:\n                    validation_results['issues'].append(f\"Summary quality issues: {summary_validation['issues']}\")\n            \n            # Validate comparison (if exists)\n            if recommendation_set.comparison:\n                comparison_validation = self._validate_comparison(recommendation_set.comparison)\n                validation_results['detailed_scores']['comparison'] = comparison_validation\n            \n            # Calculate overall score\n            all_scores = rec_scores\n            if recommendation_set.summary:\n                all_scores.append(summary_validation['score'])\n            \n            validation_results['overall_score'] = sum(all_scores) / len(all_scores) if all_scores else 0\n            \n            # Determine if recommendations pass quality check\n            validation_results['recommendations'] = (\n                validation_results['overall_score'] >= 0.7 and\n                len(validation_results['issues']) == 0\n            )\n            \n        except Exception as e:\n            logger.error(f\"Validation failed: {e}\")\n            validation_results['recommendations'] = False\n            validation_results['issues'].append(f\"Validation error: {str(e)}\")\n        \n        return validation_results\n    \n    def _validate_recommendation(self, recommendation: RecommendationItem) -> Dict[str, Any]:\n        \"\"\"Validate individual recommendation quality\"\"\"\n        issues = []\n        score_components = {}\n        \n        # Check explanation length\n        explanation_length = len(recommendation.explanation)\n        if explanation_length < self.min_explanation_length:\n            issues.append(f\"Explanation too short ({explanation_length} chars)\")\n            score_components['length'] = 0.3\n        elif explanation_length > self.max_explanation_length:\n            issues.append(f\"Explanation too long ({explanation_length} chars)\")\n            score_components['length'] = 0.7\n        else:\n            score_components['length'] = 1.0\n        \n        # Check for prohibited phrases\n        explanation_lower = recommendation.explanation.lower()\n        prohibited_found = [phrase for phrase in self.prohibited_phrases if phrase in explanation_lower]\n        if prohibited_found:\n            issues.append(f\"Contains prohibited phrases: {prohibited_found}\")\n            score_components['content'] = 0.2\n        else:\n            score_components['content'] = 1.0\n        \n        # Check for product-specific information\n        has_product_info = any(keyword in explanation_lower for keyword in [\n            'feature', 'price', 'quality', 'performance', 'benefit'\n        ])\n        score_components['relevance'] = 1.0 if has_product_info else 0.5\n        \n        # Check confidence score validity\n        if 0 <= recommendation.confidence_score <= 1:\n            score_components['confidence'] = 1.0\n        else:\n            issues.append(f\"Invalid confidence score: {recommendation.confidence_score}\")\n            score_components['confidence'] = 0.0\n        \n        # Calculate overall score\n        overall_score = sum(score_components.values()) / len(score_components)\n        \n        return {\n            'score': overall_score,\n            'issues': issues,\n            'score_components': score_components\n        }\n    \n    def _validate_summary(self, summary: str) -> Dict[str, Any]:\n        \"\"\"Validate summary quality\"\"\"\n        issues = []\n        score_components = {}\n        \n        # Check length\n        summary_length = len(summary)\n        if summary_length < self.min_summary_length:\n            issues.append(f\"Summary too short ({summary_length} chars)\")\n            score_components['length'] = 0.3\n        elif summary_length > self.max_summary_length:\n            issues.append(f\"Summary too long ({summary_length} chars)\")\n            score_components['length'] = 0.7\n        else:\n            score_components['length'] = 1.0\n        \n        # Check for actionable content\n        has_action = any(keyword in summary.lower() for keyword in [\n            'recommend', 'suggest', 'best', 'choose', 'consider'\n        ])\n        score_components['actionable'] = 1.0 if has_action else 0.6\n        \n        # Check coherence (simple heuristic)\n        sentences = summary.split('.')\n        score_components['coherence'] = 1.0 if len(sentences) >= 2 else 0.7\n        \n        overall_score = sum(score_components.values()) / len(score_components)\n        \n        return {\n            'score': overall_score,\n            'issues': issues,\n            'score_components': score_components\n        }\n    \n    def _validate_comparison(self, comparison: str) -> Dict[str, Any]:\n        \"\"\"Validate comparison content quality\"\"\"\n        issues = []\n        score_components = {}\n        \n        # Check for comparison keywords\n        comparison_keywords = ['vs', 'versus', 'compared', 'difference', 'better', 'advantage']\n        has_comparison = any(keyword in comparison.lower() for keyword in comparison_keywords)\n        score_components['comparison_content'] = 1.0 if has_comparison else 0.5\n        \n        # Check structure (looking for organized content)\n        has_structure = any(marker in comparison for marker in ['1.', '2.', '-', '*', '##'])\n        score_components['structure'] = 1.0 if has_structure else 0.7\n        \n        # Check length appropriateness\n        comparison_length = len(comparison)\n        if comparison_length < 100:\n            score_components['depth'] = 0.5\n        elif comparison_length > 2000:\n            score_components['depth'] = 0.7\n        else:\n            score_components['depth'] = 1.0\n        \n        overall_score = sum(score_components.values()) / len(score_components)\n        \n        return {\n            'score': overall_score,\n            'issues': issues,\n            'score_components': score_components\n        }\n    \n    def improve_content(self, content: str, content_type: str) -> str:\n        \"\"\"Attempt to improve content quality automatically\"\"\"\n        improved_content = content\n        \n        # Remove prohibited phrases\n        for phrase in self.prohibited_phrases:\n            improved_content = improved_content.replace(phrase, '')\n        \n        # Clean up formatting\n        improved_content = re.sub(r'\\n\\s*\\n', '\\n\\n', improved_content)  # Normalize line breaks\n        improved_content = re.sub(r'\\s+', ' ', improved_content)  # Normalize spaces\n        improved_content = improved_content.strip()\n        \n        # Add structure if missing (for comparisons)\n        if content_type == 'comparison' and not any(marker in improved_content for marker in ['1.', '-', '*']):\n            # Simple structure addition\n            sentences = improved_content.split('. ')\n            if len(sentences) > 2:\n                structured_sentences = [f\"\u2022 {sentence.strip()}\" for sentence in sentences if sentence.strip()]\n                improved_content = '\\n'.join(structured_sentences)\n        \n        return improved_content\n"})}),"\n",(0,o.jsx)(n.h3,{id:"api-integration",children:"API Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# api/recommendation_api.py\nfrom fastapi import FastAPI, HTTPException, Depends\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Any, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\napp = FastAPI(title="RAG Recommendation Generation API")\n\nclass RecommendationRequest(BaseModel):\n    query: str = Field(..., description="User search query")\n    retrieved_products: List[Dict[str, Any]] = Field(..., description="Products from retrieval system")\n    user_context: Optional[Dict[str, Any]] = Field(default=None, description="User context for personalization")\n    recommendation_type: str = Field(default="standard", description="Type of recommendation (standard, detailed)")\n    include_comparison: bool = Field(default=True, description="Include product comparison")\n    include_buying_guide: bool = Field(default=False, description="Include buying guide")\n\nclass RecommendationResponse(BaseModel):\n    recommendations: List[Dict[str, Any]]\n    summary: str\n    comparison: Optional[str] = None\n    buying_guide: Optional[str] = None\n    quality_score: float\n    generation_metadata: Dict[str, Any]\n\n# Global recommendation generator\nrecommendation_generator: Optional[RecommendationGenerator] = None\n\ndef get_recommendation_generator() -> RecommendationGenerator:\n    """Dependency to get recommendation generator"""\n    if recommendation_generator is None:\n        raise HTTPException(status_code=500, detail="Recommendation generator not initialized")\n    return recommendation_generator\n\n@app.post("/generate-recommendations", response_model=RecommendationResponse)\nasync def generate_recommendations(\n    request: RecommendationRequest,\n    generator: RecommendationGenerator = Depends(get_recommendation_generator)\n) -> RecommendationResponse:\n    """Generate personalized product recommendations"""\n    try:\n        import time\n        start_time = time.time()\n        \n        # Generate recommendations\n        recommendation_set = await generator.generate_recommendations(\n            query=request.query,\n            retrieved_products=request.retrieved_products,\n            user_context=request.user_context,\n            recommendation_type=request.recommendation_type\n        )\n        \n        # Validate quality\n        quality_controller = ContentQualityController()\n        quality_results = quality_controller.validate_recommendation_set(recommendation_set)\n        \n        generation_time = time.time() - start_time\n        \n        # Format response\n        formatted_recommendations = []\n        for rec in recommendation_set.recommendations:\n            formatted_rec = {\n                \'product_id\': rec.product_id,\n                \'title\': rec.title,\n                \'explanation\': rec.explanation,\n                \'confidence_score\': rec.confidence_score,\n                \'ranking_factors\': rec.ranking_factors,\n                \'personalization_factors\': rec.personalization_factors\n            }\n            formatted_recommendations.append(formatted_rec)\n        \n        return RecommendationResponse(\n            recommendations=formatted_recommendations,\n            summary=recommendation_set.summary,\n            comparison=recommendation_set.comparison if request.include_comparison else None,\n            buying_guide=recommendation_set.buying_guide if request.include_buying_guide else None,\n            quality_score=quality_results[\'overall_score\'],\n            generation_metadata={\n                \'generation_time\': generation_time,\n                \'quality_validation\': quality_results,\n                \'recommendation_count\': len(formatted_recommendations),\n                \'user_personalization\': request.user_context is not None\n            }\n        )\n        \n    except Exception as e:\n        logger.error(f"Recommendation generation error: {e}")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post("/explain-recommendation")\nasync def explain_recommendation(\n    product_id: str,\n    query: str,\n    product_details: Dict[str, Any],\n    user_context: Optional[Dict[str, Any]] = None,\n    generator: RecommendationGenerator = Depends(get_recommendation_generator)\n):\n    """Generate explanation for a specific product recommendation"""\n    try:\n        # Use explanation template\n        variables = {\n            \'query\': query,\n            \'product_details\': json.dumps(product_details, indent=2),\n            \'user_context\': json.dumps(user_context, indent=2) if user_context else "Not provided"\n        }\n        \n        prompt = generator.prompt_manager.generate_prompt(\'product_explanation\', variables)\n        \n        request = GenerationRequest(\n            prompt=prompt,\n            max_tokens=400,\n            temperature=0.7,\n            user_context=user_context\n        )\n        \n        response = await generator.llm_client.generate(request)\n        \n        return {\n            \'product_id\': product_id,\n            \'explanation\': response.content,\n            \'confidence_score\': 0.8,  # Default confidence\n            \'generation_metadata\': {\n                \'model_used\': response.model_used,\n                \'tokens_used\': response.tokens_used,\n                \'generation_time\': response.generation_time\n            }\n        }\n        \n    except Exception as e:\n        logger.error(f"Explanation generation error: {e}")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get("/health")\nasync def health_check():\n    """Health check endpoint"""\n    return {"status": "healthy", "service": "rag-recommendation-generation"}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-complete-rag-system",children:"Integration with Complete RAG System"}),"\n",(0,o.jsx)(n.h3,{id:"end-to-end-pipeline",children:"End-to-End Pipeline"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# main/rag_system.py\nimport asyncio\nfrom typing import Dict, Any, List, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass RAGRecommendationSystem:\n    \"\"\"Complete RAG-based recommendation system\"\"\"\n    \n    def __init__(self,\n                 retrieval_engine: RetrievalEngine,\n                 recommendation_generator: RecommendationGenerator,\n                 cache_manager: Optional[RetrievalCache] = None):\n        \n        self.retrieval_engine = retrieval_engine\n        self.recommendation_generator = recommendation_generator\n        self.cache_manager = cache_manager\n    \n    async def get_recommendations(self,\n                                query: str,\n                                user_id: Optional[str] = None,\n                                limit: int = 10,\n                                include_explanations: bool = True) -> Dict[str, Any]:\n        \"\"\"Complete end-to-end recommendation pipeline\"\"\"\n        \n        try:\n            logger.info(f\"Processing recommendation request: {query}\")\n            \n            # Get user context\n            user_context = await self._get_user_context(user_id) if user_id else None\n            \n            # Check cache first\n            cached_result = None\n            if self.cache_manager:\n                cached_result = await self.cache_manager.get_cached_results(\n                    query, {}, user_id\n                )\n            \n            if cached_result:\n                logger.info(\"Returning cached results\")\n                return cached_result\n            \n            # Step 1: Retrieve relevant products\n            retrieval_results = await self.retrieval_engine.search(\n                query=query,\n                user_context=user_context,\n                limit=limit * 2  # Retrieve more for better generation\n            )\n            \n            if not retrieval_results:\n                return {\n                    'recommendations': [],\n                    'message': 'No products found matching your query',\n                    'suggestions': await self._get_query_suggestions(query)\n                }\n            \n            # Step 2: Convert retrieval results to product data\n            product_data = [result.payload for result in retrieval_results]\n            \n            # Step 3: Generate personalized recommendations\n            if include_explanations:\n                recommendation_set = await self.recommendation_generator.generate_recommendations(\n                    query=query,\n                    retrieved_products=product_data,\n                    user_context=user_context,\n                    recommendation_type='detailed'\n                )\n                \n                result = {\n                    'query': query,\n                    'recommendations': [\n                        {\n                            'product_id': rec.product_id,\n                            'title': rec.title,\n                            'explanation': rec.explanation,\n                            'confidence_score': rec.confidence_score,\n                            'product_data': next(\n                                (p for p in product_data if p.get('id') == rec.product_id), \n                                {}\n                            )\n                        }\n                        for rec in recommendation_set.recommendations\n                    ],\n                    'summary': recommendation_set.summary,\n                    'comparison': recommendation_set.comparison,\n                    'buying_guide': recommendation_set.buying_guide\n                }\n            else:\n                # Simple recommendations without explanations\n                result = {\n                    'query': query,\n                    'recommendations': [\n                        {\n                            'product_id': result.product_id,\n                            'score': result.score,\n                            'product_data': result.payload\n                        }\n                        for result in retrieval_results[:limit]\n                    ]\n                }\n            \n            # Cache the result\n            if self.cache_manager:\n                await self.cache_manager.cache_results(query, {}, result, user_id)\n            \n            logger.info(f\"Generated {len(result['recommendations'])} recommendations\")\n            return result\n            \n        except Exception as e:\n            logger.error(f\"Recommendation pipeline failed: {e}\")\n            return {\n                'error': str(e),\n                'recommendations': [],\n                'message': 'Sorry, we encountered an error processing your request'\n            }\n    \n    async def _get_user_context(self, user_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get user context for personalization\"\"\"\n        # This would typically fetch from user service or database\n        # Placeholder implementation\n        return {\n            'preferred_categories': ['electronics', 'books'],\n            'preferred_brands': ['apple', 'samsung'],\n            'budget_range': {'min': 0, 'max': 1000},\n            'recent_searches': ['laptop', 'phone case'],\n            'purchase_history': ['MacBook Pro', 'iPhone 13']\n        }\n    \n    async def _get_query_suggestions(self, query: str) -> List[str]:\n        \"\"\"Get alternative query suggestions\"\"\"\n        # Simple suggestion logic - would be enhanced in production\n        suggestions = [\n            f\"Try searching for '{query}' with specific brands\",\n            f\"Consider broader terms related to '{query}'\",\n            \"Browse our popular categories\",\n            \"Check out trending products\"\n        ]\n        return suggestions\n"})}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"LLM Integration"}),": Use multiple LLM providers with fallback support for reliability"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Prompt Engineering"}),": Design specialized prompts for different recommendation scenarios"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Quality Control"}),": Implement automated content quality validation and improvement"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Personalization"}),": Leverage user context to create truly personalized recommendations"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Performance"}),": Cache generated content and optimize LLM calls for production use"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Flexibility"}),": Support different recommendation types and output formats"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This completes the comprehensive RAG-based recommendation system for e-commerce applications, providing intelligent, personalized product recommendations with clear explanations and buying guidance."})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}},5741:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>a});var r=t(9729);const o={},s=r.createContext(o);function i(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);