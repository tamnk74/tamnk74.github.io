"use strict";(self.webpackChunkdocs_site=self.webpackChunkdocs_site||[]).push([[2880],{4118:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>d,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"rag-recommendations/data-pipeline","title":"Data Pipeline and Vector Embeddings","description":"This section covers building robust data pipelines and generating high-quality vector embeddings that capture the semantic essence of products and user behavior. Vector embeddings are the foundation that enables our recommendation system to understand relationships between products, users, and their preferences at a semantic level.","source":"@site/docs/rag-recommendations/data-pipeline.md","sourceDirName":"rag-recommendations","slug":"/rag-recommendations/data-pipeline","permalink":"/fullstack-dev/docs/rag-recommendations/data-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/tamnk74/fullstack-dev/tree/main/docs-site/docs/rag-recommendations/data-pipeline.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Introduction and Architecture Overview","permalink":"/fullstack-dev/docs/rag-recommendations/introduction"},"next":{"title":"Retrieval System Implementation","permalink":"/fullstack-dev/docs/rag-recommendations/retrieval-system"}}');var i=t(5813),s=t(7814);const a={},d="Data Pipeline and Vector Embeddings",o={},c=[{value:"Data Pipeline Architecture",id:"data-pipeline-architecture",level:2},{value:"Setting Up the Environment",id:"setting-up-the-environment",level:2},{value:"Dependencies",id:"dependencies",level:3},{value:"Product Data Schema",id:"product-data-schema",level:2},{value:"Product Model Definition",id:"product-model-definition",level:3},{value:"Product Data Processing",id:"product-data-processing",level:3},{value:"User Behavior Models",id:"user-behavior-models",level:2},{value:"User Data Schema",id:"user-data-schema",level:3},{value:"User Behavior Processing",id:"user-behavior-processing",level:3},{value:"Vector Embedding Generation",id:"vector-embedding-generation",level:2},{value:"Embedding Service Implementation",id:"embedding-service-implementation",level:3},{value:"Product Embedding Generation",id:"product-embedding-generation",level:3},{value:"Embedding Quality Validation",id:"embedding-quality-validation",level:2},{value:"Validation Framework",id:"validation-framework",level:3},{value:"Database Integration",id:"database-integration",level:2},{value:"Vector Storage Setup",id:"vector-storage-setup",level:3},{value:"Batch Processing Pipeline",id:"batch-processing-pipeline",level:2},{value:"Pipeline Orchestration",id:"pipeline-orchestration",level:3},{value:"Performance Monitoring",id:"performance-monitoring",level:2},{value:"Metrics Collection",id:"metrics-collection",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"data-pipeline-and-vector-embeddings",children:"Data Pipeline and Vector Embeddings"})}),"\n",(0,i.jsx)(n.p,{children:"This section covers building robust data pipelines and generating high-quality vector embeddings that capture the semantic essence of products and user behavior. Vector embeddings are the foundation that enables our recommendation system to understand relationships between products, users, and their preferences at a semantic level."}),"\n",(0,i.jsx)(n.h2,{id:"data-pipeline-architecture",children:"Data Pipeline Architecture"}),"\n",(0,i.jsx)(n.p,{children:"Our data pipeline follows the Extract, Transform, Load (ETL) pattern, specifically designed for vector embedding generation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Raw Data      \u2502    \u2502  Processing     \u2502    \u2502   Vector        \u2502\n\u2502   Sources       \u2502\u2500\u2500\u2500\u25b6\u2502  Pipeline       \u2502\u2500\u2500\u2500\u25b6\u2502   Storage       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502                      \u2502                      \u2502                 \u2502\n\u251c\u2500 Product Catalog     \u251c\u2500 Data Cleaning       \u251c\u2500 Product Vectors\u2502\n\u251c\u2500 User Profiles       \u251c\u2500 Feature Extraction  \u251c\u2500 User Vectors   \u2502\n\u251c\u2500 Interaction Logs    \u251c\u2500 Embedding Generation\u251c\u2500 Query Vectors  \u2502\n\u251c\u2500 Reviews/Ratings     \u251c\u2500 Quality Validation  \u251c\u2500 Metadata Index \u2502\n\u2514\u2500 Search Queries      \u2514\u2500 Batch Processing    \u2514\u2500 Search Index   \u2502\n"})}),"\n",(0,i.jsx)(n.h2,{id:"setting-up-the-environment",children:"Setting Up the Environment"}),"\n",(0,i.jsx)(n.h3,{id:"dependencies",children:"Dependencies"}),"\n",(0,i.jsx)(n.p,{children:"First, set up the development environment with necessary dependencies:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create virtual environment\npython -m venv rag-ecommerce\nsource rag-ecommerce/bin/activate  # On Windows: rag-ecommerce\\Scripts\\activate\n\n# Install core dependencies\npip install -r requirements.txt\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# requirements.txt\n# Data processing\npandas==2.1.0\nnumpy==1.24.3\nsqlalchemy==2.0.20\npsycopg2-binary==2.9.7\n\n# Machine learning and embeddings\nsentence-transformers==2.2.2\ntransformers==4.33.2\ntorch==2.0.1\nscikit-learn==1.3.0\n\n# Vector databases\nqdrant-client==1.4.0\npinecone-client==2.2.4\nweaviate-client==3.23.1\n\n# API and LLM integration\nopenai==0.28.0\nanthropic==0.3.11\nlangchain==0.0.292\n\n# Data validation and monitoring\npydantic==2.3.0\ngreat-expectations==0.17.15\nmlflow==2.6.0\n\n# Utilities\npython-dotenv==1.0.0\ntqdm==4.66.1\nredis==4.6.0\ncelery==5.3.1\n"})}),"\n",(0,i.jsx)(n.h2,{id:"product-data-schema",children:"Product Data Schema"}),"\n",(0,i.jsx)(n.h3,{id:"product-model-definition",children:"Product Model Definition"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# models/product.py\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nfrom enum import Enum\n\nclass ProductCategory(str, Enum):\n    ELECTRONICS = "electronics"\n    CLOTHING = "clothing"\n    HOME_GARDEN = "home_garden"\n    BOOKS = "books"\n    SPORTS = "sports"\n    BEAUTY = "beauty"\n    AUTOMOTIVE = "automotive"\n\nclass Product(BaseModel):\n    id: str = Field(..., description="Unique product identifier")\n    title: str = Field(..., description="Product title")\n    description: str = Field(..., description="Detailed product description")\n    category: ProductCategory = Field(..., description="Primary product category")\n    subcategory: Optional[str] = Field(None, description="Product subcategory")\n    brand: Optional[str] = Field(None, description="Product brand")\n    price: float = Field(..., gt=0, description="Product price")\n    currency: str = Field(default="USD", description="Price currency")\n    \n    # Product attributes\n    attributes: Dict[str, Any] = Field(default_factory=dict, description="Product attributes")\n    tags: List[str] = Field(default_factory=list, description="Product tags")\n    images: List[str] = Field(default_factory=list, description="Product image URLs")\n    \n    # Metrics\n    rating: Optional[float] = Field(None, ge=0, le=5, description="Average rating")\n    review_count: int = Field(default=0, ge=0, description="Number of reviews")\n    popularity_score: float = Field(default=0.0, description="Popularity metric")\n    \n    # Inventory and availability\n    stock_quantity: int = Field(default=0, ge=0, description="Available stock")\n    is_available: bool = Field(default=True, description="Product availability")\n    \n    # Temporal data\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n    updated_at: datetime = Field(default_factory=datetime.utcnow)\n    \n    # SEO and discoverability\n    keywords: List[str] = Field(default_factory=list, description="SEO keywords")\n    search_terms: List[str] = Field(default_factory=list, description="Common search terms")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"product-data-processing",children:"Product Data Processing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# processors/product_processor.py\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any\nimport re\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass ProductDataProcessor:\n    def __init__(self):\n        self.stop_words = self._load_stop_words()\n        self.category_keywords = self._load_category_keywords()\n    \n    def clean_product_data(self, products_df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Clean and standardize product data\"\"\"\n        logger.info(f\"Cleaning {len(products_df)} products\")\n        \n        # Remove duplicates\n        products_df = products_df.drop_duplicates(subset=['id'])\n        \n        # Standardize text fields\n        products_df['title'] = products_df['title'].apply(self._clean_text)\n        products_df['description'] = products_df['description'].apply(self._clean_text)\n        \n        # Normalize prices\n        products_df['price'] = pd.to_numeric(products_df['price'], errors='coerce')\n        products_df = products_df.dropna(subset=['price'])\n        \n        # Fill missing values\n        products_df['description'] = products_df['description'].fillna('')\n        products_df['brand'] = products_df['brand'].fillna('Unknown')\n        products_df['rating'] = products_df['rating'].fillna(0.0)\n        products_df['review_count'] = products_df['review_count'].fillna(0)\n        \n        # Calculate derived features\n        products_df['title_length'] = products_df['title'].str.len()\n        products_df['description_length'] = products_df['description'].str.len()\n        products_df['price_category'] = pd.cut(\n            products_df['price'], \n            bins=[0, 25, 100, 500, np.inf], \n            labels=['budget', 'mid-range', 'premium', 'luxury']\n        )\n        \n        logger.info(f\"Cleaned data: {len(products_df)} products remaining\")\n        return products_df\n    \n    def extract_product_features(self, product: Product) -> Dict[str, Any]:\n        \"\"\"Extract meaningful features from product data\"\"\"\n        features = {\n            # Basic features\n            'title': product.title,\n            'description': product.description,\n            'category': product.category.value,\n            'subcategory': product.subcategory or '',\n            'brand': product.brand or '',\n            \n            # Numeric features\n            'price': product.price,\n            'rating': product.rating or 0.0,\n            'review_count': product.review_count,\n            'popularity_score': product.popularity_score,\n            \n            # Text processing\n            'combined_text': self._create_combined_text(product),\n            'keywords': product.keywords,\n            'search_terms': product.search_terms,\n            \n            # Categorical features\n            'price_range': self._categorize_price(product.price),\n            'rating_tier': self._categorize_rating(product.rating or 0.0),\n            'popularity_tier': self._categorize_popularity(product.popularity_score),\n            \n            # Availability features\n            'is_available': product.is_available,\n            'stock_level': self._categorize_stock(product.stock_quantity),\n        }\n        \n        # Add attribute-based features\n        features.update(self._process_attributes(product.attributes))\n        \n        return features\n    \n    def _clean_text(self, text: str) -> str:\n        \"\"\"Clean and normalize text data\"\"\"\n        if pd.isna(text) or not isinstance(text, str):\n            return \"\"\n        \n        # Convert to lowercase\n        text = text.lower()\n        \n        # Remove special characters but keep alphanumeric and spaces\n        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n        \n        # Remove extra whitespaces\n        text = re.sub(r'\\s+', ' ', text).strip()\n        \n        return text\n    \n    def _create_combined_text(self, product: Product) -> str:\n        \"\"\"Create combined text for embedding generation\"\"\"\n        components = [\n            product.title,\n            product.description,\n            product.brand or '',\n            product.category.value,\n            product.subcategory or '',\n            ' '.join(product.keywords),\n            ' '.join(product.search_terms)\n        ]\n        \n        # Filter out empty components\n        components = [comp for comp in components if comp and comp.strip()]\n        \n        return ' '.join(components)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"user-behavior-models",children:"User Behavior Models"}),"\n",(0,i.jsx)(n.h3,{id:"user-data-schema",children:"User Data Schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# models/user.py\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nfrom enum import Enum\n\nclass UserDemographic(BaseModel):\n    age_range: Optional[str] = Field(None, description="Age range (e.g., \'25-34\')")\n    gender: Optional[str] = Field(None, description="User gender")\n    location: Optional[str] = Field(None, description="User location")\n    income_range: Optional[str] = Field(None, description="Income bracket")\n\nclass UserPreferences(BaseModel):\n    preferred_categories: List[str] = Field(default_factory=list)\n    preferred_brands: List[str] = Field(default_factory=list)\n    price_sensitivity: float = Field(default=0.5, ge=0, le=1)\n    quality_preference: float = Field(default=0.5, ge=0, le=1)\n    style_preferences: List[str] = Field(default_factory=list)\n\nclass InteractionType(str, Enum):\n    VIEW = "view"\n    CLICK = "click"\n    ADD_TO_CART = "add_to_cart"\n    PURCHASE = "purchase"\n    REVIEW = "review"\n    SEARCH = "search"\n    WISHLIST = "wishlist"\n\nclass UserInteraction(BaseModel):\n    id: str\n    user_id: str\n    product_id: Optional[str] = None\n    interaction_type: InteractionType\n    timestamp: datetime = Field(default_factory=datetime.utcnow)\n    session_id: str\n    \n    # Context data\n    page_url: Optional[str] = None\n    referrer: Optional[str] = None\n    device_type: Optional[str] = None\n    search_query: Optional[str] = None\n    \n    # Interaction specifics\n    duration: Optional[float] = None  # Time spent (seconds)\n    scroll_depth: Optional[float] = None  # Page scroll percentage\n    quantity: Optional[int] = None\n    price_paid: Optional[float] = None\n'})}),"\n",(0,i.jsx)(n.h3,{id:"user-behavior-processing",children:"User Behavior Processing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# processors/user_processor.py\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict\nfrom datetime import datetime\nfrom collections import defaultdict\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass UserBehaviorProcessor:\n    def __init__(self):\n        self.interaction_weights = {\n            InteractionType.VIEW: 1.0,\n            InteractionType.CLICK: 2.0,\n            InteractionType.ADD_TO_CART: 5.0,\n            InteractionType.PURCHASE: 10.0,\n            InteractionType.REVIEW: 8.0,\n            InteractionType.SEARCH: 3.0,\n            InteractionType.WISHLIST: 4.0,\n        }\n    \n    def process_user_interactions(self, interactions_df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Process and enrich user interaction data\"\"\"\n        logger.info(f\"Processing {len(interactions_df)} user interactions\")\n        \n        # Convert timestamp to datetime\n        interactions_df['timestamp'] = pd.to_datetime(interactions_df['timestamp'])\n        \n        # Add temporal features\n        interactions_df['hour'] = interactions_df['timestamp'].dt.hour\n        interactions_df['day_of_week'] = interactions_df['timestamp'].dt.dayofweek\n        interactions_df['is_weekend'] = interactions_df['day_of_week'].isin([5, 6])\n        \n        # Calculate interaction weights\n        interactions_df['interaction_weight'] = interactions_df['interaction_type'].map(\n            lambda x: self.interaction_weights.get(InteractionType(x), 1.0)\n        )\n        \n        # Add session-based features\n        interactions_df = self._add_session_features(interactions_df)\n        \n        return interactions_df\n    \n    def create_user_profiles(self, users_df: pd.DataFrame, interactions_df: pd.DataFrame) -> Dict[str, Dict]:\n        \"\"\"Create comprehensive user profiles from interaction data\"\"\"\n        user_profiles = {}\n        \n        for user_id in users_df['id'].unique():\n            user_interactions = interactions_df[interactions_df['user_id'] == user_id]\n            \n            profile = {\n                'user_id': user_id,\n                'behavioral_features': self._extract_behavioral_features(user_interactions),\n                'preference_features': self._extract_preference_features(user_interactions),\n                'temporal_features': self._extract_temporal_features(user_interactions),\n                'engagement_features': self._extract_engagement_features(user_interactions),\n            }\n            \n            user_profiles[user_id] = profile\n        \n        logger.info(f\"Created profiles for {len(user_profiles)} users\")\n        return user_profiles\n"})}),"\n",(0,i.jsx)(n.h2,{id:"vector-embedding-generation",children:"Vector Embedding Generation"}),"\n",(0,i.jsx)(n.h3,{id:"embedding-service-implementation",children:"Embedding Service Implementation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# embeddings/embedding_generator.py\nimport numpy as np\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom typing import List, Dict, Any, Optional\nimport logging\nfrom abc import ABC, abstractmethod\nimport asyncio\nimport openai\n\nlogger = logging.getLogger(__name__)\n\nclass BaseEmbeddingGenerator(ABC):\n    """Abstract base class for embedding generators"""\n    \n    @abstractmethod\n    async def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n        pass\n    \n    @abstractmethod\n    def get_embedding_dimension(self) -> int:\n        pass\n\nclass SentenceTransformerEmbedding(BaseEmbeddingGenerator):\n    """Local sentence transformer embedding generator"""\n    \n    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):\n        self.model_name = model_name\n        self.model = SentenceTransformer(model_name)\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        self.model.to(self.device)\n        \n        logger.info(f"Loaded SentenceTransformer model: {model_name}")\n    \n    async def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n        """Generate embeddings for a list of texts"""\n        if not texts:\n            return np.array([])\n        \n        # Process in batches to manage memory\n        batch_size = 32\n        all_embeddings = []\n        \n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i + batch_size]\n            embeddings = self.model.encode(\n                batch,\n                convert_to_numpy=True,\n                show_progress_bar=False,\n                batch_size=batch_size\n            )\n            all_embeddings.append(embeddings)\n        \n        return np.vstack(all_embeddings)\n    \n    def get_embedding_dimension(self) -> int:\n        return self.model.get_sentence_embedding_dimension()\n\nclass OpenAIEmbedding(BaseEmbeddingGenerator):\n    """OpenAI API embedding generator"""\n    \n    def __init__(self, model_name: str = "text-embedding-ada-002", api_key: Optional[str] = None):\n        self.model_name = model_name\n        openai.api_key = api_key or os.getenv("OPENAI_API_KEY")\n        self.dimensions = {\n            "text-embedding-ada-002": 1536,\n            "text-embedding-3-small": 1536,\n            "text-embedding-3-large": 3072,\n        }\n        \n        logger.info(f"Initialized OpenAI embedding model: {model_name}")\n    \n    async def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n        """Generate embeddings using OpenAI API"""\n        if not texts:\n            return np.array([])\n        \n        batch_size = 100\n        all_embeddings = []\n        \n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i + batch_size]\n            \n            try:\n                response = await openai.Embedding.acreate(\n                    model=self.model_name,\n                    input=batch\n                )\n                \n                batch_embeddings = [data.embedding for data in response.data]\n                all_embeddings.extend(batch_embeddings)\n                \n                await asyncio.sleep(0.1)  # Rate limiting\n                \n            except Exception as e:\n                logger.error(f"Error generating embeddings: {e}")\n                zero_embedding = [0.0] * self.get_embedding_dimension()\n                all_embeddings.extend([zero_embedding] * len(batch))\n        \n        return np.array(all_embeddings)\n    \n    def get_embedding_dimension(self) -> int:\n        return self.dimensions.get(self.model_name, 1536)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"product-embedding-generation",children:"Product Embedding Generation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class ProductEmbeddingGenerator:\n    \"\"\"Specialized embedding generator for products\"\"\"\n    \n    def __init__(self, embedding_generator: BaseEmbeddingGenerator):\n        self.embedding_generator = embedding_generator\n        self.dimension = embedding_generator.get_embedding_dimension()\n    \n    async def generate_product_embeddings(self, products: List[Dict[str, Any]]) -> Dict[str, np.ndarray]:\n        \"\"\"Generate embeddings for products\"\"\"\n        logger.info(f\"Generating embeddings for {len(products)} products\")\n        \n        # Prepare texts for embedding\n        product_texts = []\n        product_ids = []\n        \n        for product in products:\n            text = self._create_product_text(product)\n            product_texts.append(text)\n            product_ids.append(product['id'])\n        \n        # Generate embeddings\n        embeddings = await self.embedding_generator.generate_embeddings(product_texts)\n        \n        # Create mapping\n        product_embeddings = {}\n        for product_id, embedding in zip(product_ids, embeddings):\n            product_embeddings[product_id] = embedding\n        \n        return product_embeddings\n    \n    def _create_product_text(self, product: Dict[str, Any]) -> str:\n        \"\"\"Create text representation of product for embedding\"\"\"\n        components = []\n        \n        # Title (highest weight)\n        if product.get('title'):\n            components.append(f\"Title: {product['title']}\")\n        \n        # Category and subcategory\n        if product.get('category'):\n            components.append(f\"Category: {product['category']}\")\n        if product.get('subcategory'):\n            components.append(f\"Subcategory: {product['subcategory']}\")\n        \n        # Brand\n        if product.get('brand'):\n            components.append(f\"Brand: {product['brand']}\")\n        \n        # Description\n        if product.get('description'):\n            description = product['description'][:500]  # Truncate long descriptions\n            components.append(f\"Description: {description}\")\n        \n        # Price range\n        if product.get('price_range'):\n            components.append(f\"Price range: {product['price_range']}\")\n        \n        # Keywords and tags\n        if product.get('keywords'):\n            components.append(f\"Keywords: {', '.join(product['keywords'])}\")\n        \n        return ' '.join(components)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"embedding-quality-validation",children:"Embedding Quality Validation"}),"\n",(0,i.jsx)(n.h3,{id:"validation-framework",children:"Validation Framework"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# validation/embedding_validator.py\nimport numpy as np\nfrom typing import List, Dict, Any\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import PCA\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass EmbeddingQualityValidator:\n    \"\"\"Validate the quality of generated embeddings\"\"\"\n    \n    def __init__(self):\n        self.similarity_threshold = 0.7\n        self.diversity_threshold = 0.3\n    \n    def validate_product_embeddings(\n        self, \n        product_embeddings: Dict[str, np.ndarray],\n        products: List[Dict[str, Any]]\n    ) -> Dict[str, Any]:\n        \"\"\"Comprehensive validation of product embeddings\"\"\"\n        logger.info(\"Validating product embeddings...\")\n        \n        results = {\n            'total_products': len(product_embeddings),\n            'embedding_dimension': len(next(iter(product_embeddings.values()))),\n            'quality_metrics': {},\n            'recommendations': []\n        }\n        \n        # Create product lookup\n        product_lookup = {p['id']: p for p in products}\n        \n        # Test semantic similarity\n        similarity_results = self._test_semantic_similarity(product_embeddings, product_lookup)\n        results['quality_metrics']['semantic_similarity'] = similarity_results\n        \n        # Test category clustering\n        clustering_results = self._test_category_clustering(product_embeddings, product_lookup)\n        results['quality_metrics']['category_clustering'] = clustering_results\n        \n        # Test embedding diversity\n        diversity_results = self._test_embedding_diversity(product_embeddings)\n        results['quality_metrics']['diversity'] = diversity_results\n        \n        # Generate recommendations\n        results['recommendations'] = self._generate_quality_recommendations(results['quality_metrics'])\n        \n        overall_score = self._calculate_overall_score(results['quality_metrics'])\n        logger.info(f\"Validation complete. Overall quality score: {overall_score:.3f}\")\n        \n        return results\n    \n    def _test_semantic_similarity(\n        self, \n        embeddings: Dict[str, np.ndarray], \n        products: Dict[str, Dict[str, Any]]\n    ) -> Dict[str, float]:\n        \"\"\"Test if semantically similar products have similar embeddings\"\"\"\n        # Sample products for testing\n        sample_size = min(100, len(embeddings))\n        sampled_ids = list(embeddings.keys())[:sample_size]\n        \n        similarity_scores = []\n        \n        for i, product_id_1 in enumerate(sampled_ids):\n            for product_id_2 in sampled_ids[i+1:]:\n                # Calculate embedding similarity\n                emb_sim = cosine_similarity(\n                    embeddings[product_id_1].reshape(1, -1),\n                    embeddings[product_id_2].reshape(1, -1)\n                )[0, 0]\n                \n                # Calculate semantic similarity\n                semantic_sim = self._calculate_semantic_similarity(\n                    products[product_id_1], \n                    products[product_id_2]\n                )\n                \n                similarity_scores.append({\n                    'embedding_similarity': emb_sim,\n                    'semantic_similarity': semantic_sim\n                })\n        \n        # Calculate correlation\n        emb_sims = [s['embedding_similarity'] for s in similarity_scores]\n        sem_sims = [s['semantic_similarity'] for s in similarity_scores]\n        \n        correlation = np.corrcoef(emb_sims, sem_sims)[0, 1] if len(emb_sims) > 1 else 0\n        \n        return {\n            'correlation': correlation,\n            'avg_embedding_similarity': np.mean(emb_sims),\n            'avg_semantic_similarity': np.mean(sem_sims),\n            'sample_size': len(similarity_scores)\n        }\n    \n    def _calculate_semantic_similarity(self, product1: Dict[str, Any], product2: Dict[str, Any]) -> float:\n        \"\"\"Calculate semantic similarity between two products\"\"\"\n        similarity = 0.0\n        \n        # Category similarity (highest weight)\n        if product1.get('category') == product2.get('category'):\n            similarity += 0.4\n            \n            # Subcategory similarity\n            if product1.get('subcategory') == product2.get('subcategory'):\n                similarity += 0.2\n        \n        # Brand similarity\n        if product1.get('brand') == product2.get('brand'):\n            similarity += 0.2\n        \n        # Price range similarity\n        if product1.get('price_range') == product2.get('price_range'):\n            similarity += 0.1\n        \n        # Title similarity (simple word overlap)\n        title1_words = set(product1.get('title', '').lower().split())\n        title2_words = set(product2.get('title', '').lower().split())\n        if title1_words and title2_words:\n            word_overlap = len(title1_words & title2_words) / len(title1_words | title2_words)\n            similarity += 0.1 * word_overlap\n        \n        return similarity\n"})}),"\n",(0,i.jsx)(n.h2,{id:"database-integration",children:"Database Integration"}),"\n",(0,i.jsx)(n.h3,{id:"vector-storage-setup",children:"Vector Storage Setup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# storage/vector_storage.py\nimport numpy as np\nfrom typing import Dict, List, Any\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass VectorStorage:\n    """Handles storage and retrieval of vector embeddings"""\n    \n    def __init__(self, vector_db_client):\n        self.client = vector_db_client\n    \n    async def store_product_embeddings(\n        self, \n        collection_name: str,\n        product_embeddings: Dict[str, np.ndarray],\n        product_metadata: Dict[str, Dict[str, Any]]\n    ) -> bool:\n        """Store product embeddings with metadata"""\n        try:\n            vectors_data = []\n            \n            for product_id, embedding in product_embeddings.items():\n                vector_data = {\n                    \'id\': product_id,\n                    \'vector\': embedding,\n                    \'payload\': product_metadata.get(product_id, {})\n                }\n                vectors_data.append(vector_data)\n            \n            # Batch insert\n            success = await self.client.upsert_vectors(collection_name, vectors_data)\n            \n            if success:\n                logger.info(f"Successfully stored {len(vectors_data)} product embeddings")\n            else:\n                logger.error("Failed to store product embeddings")\n            \n            return success\n            \n        except Exception as e:\n            logger.error(f"Error storing embeddings: {e}")\n            return False\n    \n    async def store_user_embeddings(\n        self,\n        collection_name: str, \n        user_embeddings: Dict[str, np.ndarray],\n        user_metadata: Dict[str, Dict[str, Any]]\n    ) -> bool:\n        """Store user embeddings with metadata"""\n        try:\n            vectors_data = []\n            \n            for user_id, embedding in user_embeddings.items():\n                vector_data = {\n                    \'id\': user_id,\n                    \'vector\': embedding,\n                    \'payload\': user_metadata.get(user_id, {})\n                }\n                vectors_data.append(vector_data)\n            \n            success = await self.client.upsert_vectors(collection_name, vectors_data)\n            \n            if success:\n                logger.info(f"Successfully stored {len(vectors_data)} user embeddings")\n            \n            return success\n            \n        except Exception as e:\n            logger.error(f"Error storing user embeddings: {e}")\n            return False\n'})}),"\n",(0,i.jsx)(n.h2,{id:"batch-processing-pipeline",children:"Batch Processing Pipeline"}),"\n",(0,i.jsx)(n.h3,{id:"pipeline-orchestration",children:"Pipeline Orchestration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# pipeline/embedding_pipeline.py\nimport asyncio\nfrom typing import List, Dict, Any\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nclass EmbeddingPipeline:\n    """Orchestrates the embedding generation pipeline"""\n    \n    def __init__(self, \n                 product_processor: ProductDataProcessor,\n                 user_processor: UserBehaviorProcessor,\n                 embedding_generator: BaseEmbeddingGenerator,\n                 vector_storage: VectorStorage):\n        \n        self.product_processor = product_processor\n        self.user_processor = user_processor\n        self.embedding_generator = embedding_generator\n        self.vector_storage = vector_storage\n    \n    async def run_product_pipeline(self, products_data: List[Dict[str, Any]]) -> bool:\n        """Run complete product embedding pipeline"""\n        try:\n            logger.info("Starting product embedding pipeline")\n            \n            # Step 1: Clean and process product data\n            logger.info("Processing product data...")\n            processed_products = []\n            for product_data in products_data:\n                features = self.product_processor.extract_product_features(product_data)\n                processed_products.append(features)\n            \n            # Step 2: Generate embeddings\n            logger.info("Generating product embeddings...")\n            product_embeddings = await self._generate_product_embeddings(processed_products)\n            \n            # Step 3: Validate embeddings\n            logger.info("Validating embeddings...")\n            validation_results = self._validate_embeddings(product_embeddings, processed_products)\n            \n            # Step 4: Store embeddings\n            logger.info("Storing embeddings...")\n            metadata = {p[\'id\']: p for p in processed_products}\n            success = await self.vector_storage.store_product_embeddings(\n                \'products\', product_embeddings, metadata\n            )\n            \n            if success:\n                logger.info("Product pipeline completed successfully")\n            \n            return success\n            \n        except Exception as e:\n            logger.error(f"Product pipeline failed: {e}")\n            return False\n    \n    async def run_user_pipeline(self, user_profiles: Dict[str, Dict[str, Any]]) -> bool:\n        """Run complete user embedding pipeline"""\n        try:\n            logger.info("Starting user embedding pipeline")\n            \n            # Generate user embeddings\n            user_embeddings = await self._generate_user_embeddings(user_profiles)\n            \n            # Store embeddings\n            success = await self.vector_storage.store_user_embeddings(\n                \'users\', user_embeddings, user_profiles\n            )\n            \n            return success\n            \n        except Exception as e:\n            logger.error(f"User pipeline failed: {e}")\n            return False\n    \n    async def _generate_product_embeddings(self, products: List[Dict[str, Any]]) -> Dict[str, np.ndarray]:\n        """Generate embeddings for products"""\n        product_texts = []\n        product_ids = []\n        \n        for product in products:\n            text = self._create_product_text(product)\n            product_texts.append(text)\n            product_ids.append(product[\'id\'])\n        \n        embeddings = await self.embedding_generator.generate_embeddings(product_texts)\n        \n        return {\n            product_id: embedding \n            for product_id, embedding in zip(product_ids, embeddings)\n        }\n'})}),"\n",(0,i.jsx)(n.h2,{id:"performance-monitoring",children:"Performance Monitoring"}),"\n",(0,i.jsx)(n.h3,{id:"metrics-collection",children:"Metrics Collection"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# monitoring/pipeline_metrics.py\nimport time\nfrom typing import Dict, Any\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass PipelineMetrics:\n    """Collect and track pipeline performance metrics"""\n    \n    def __init__(self):\n        self.metrics = {\n            \'processing_times\': {},\n            \'embedding_quality\': {},\n            \'throughput\': {},\n            \'error_rates\': {}\n        }\n    \n    def track_processing_time(self, stage: str, duration: float):\n        """Track processing time for pipeline stages"""\n        if stage not in self.metrics[\'processing_times\']:\n            self.metrics[\'processing_times\'][stage] = []\n        \n        self.metrics[\'processing_times\'][stage].append(duration)\n        \n        logger.info(f"{stage} completed in {duration:.2f}s")\n    \n    def track_embedding_quality(self, quality_metrics: Dict[str, Any]):\n        """Track embedding quality metrics"""\n        self.metrics[\'embedding_quality\'] = quality_metrics\n        \n        logger.info(f"Embedding quality: {quality_metrics}")\n    \n    def get_summary(self) -> Dict[str, Any]:\n        """Get summary of all metrics"""\n        summary = {}\n        \n        # Average processing times\n        for stage, times in self.metrics[\'processing_times\'].items():\n            summary[f\'{stage}_avg_time\'] = sum(times) / len(times) if times else 0\n        \n        # Latest quality metrics\n        summary[\'quality_metrics\'] = self.metrics[\'embedding_quality\']\n        \n        return summary\n'})}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(n.p,{children:["Continue to the ",(0,i.jsx)(n.a,{href:"./retrieval-system",children:"Retrieval System Implementation"})," section to build sophisticated search capabilities that leverage these embeddings."]}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Quality"}),": Clean, well-structured data is essential for high-quality embeddings"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Text Processing"}),": Proper text cleaning and feature extraction significantly impact embedding quality"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validation"}),": Always validate embedding quality using semantic similarity tests"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Batch Processing"}),": Use efficient batch processing for large-scale embedding generation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitoring"}),": Track pipeline performance and embedding quality metrics continuously"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},7814:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>d});var r=t(9729);const i={},s=r.createContext(i);function a(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);